{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6f8f0e",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718a9ce",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Agente simple</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2dca78",
   "metadata": {},
   "source": [
    "<center>Introducción</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e234f-f8e0-4b42-85c0-fb179612a115",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/cara-alambre.jpg\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: [Pixabay](https://pixabay.com/images/search/robot/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3dc5d-306b-40db-b995-17afb239e1d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c3a93c-580a-4595-8f58-492a5251fe79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Coordinador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c933af6-345c-432f-954f-a1f9bebcbefb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Campo Elías Pardo, PhD, cepardot@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be919ce-77cb-4d9b-affc-a70d8e349c26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Conferencistas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a400b58-9c86-435a-bd55-4684b0738dc4",
   "metadata": {},
   "source": [
    "- Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "- Daniel  Montenegro, Msc, dextronomo@gmail.com \n",
    "- Oleg Jarma, Estadístico, ojarmam@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ac01bd-02da-45be-91a7-b80a0df77828",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc866c0-2aa0-405d-9d3c-c58b5bbb9a4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab17bc2-74c3-41f4-8ba3-d6e00f0c8912",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6150ff3-6b86-4884-bdf2-3fcd5472459e",
   "metadata": {},
   "source": [
    "- Nayibe Yesenia Arias, naariasc@unal.edu.co\n",
    "- Venus Celeste Puertas, vpuertasg@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783d283a-33ed-46e2-aa3a-e6d5c4cc44e9",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04627aa2-3a72-430f-9fe1-869b75063757",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Richard S. Sutton, Andrew G. Barto, Reinforcement learning: an introduction, 2nd edition, 2020](http://library.lol/main/6502B74CE247C4CD4D4FB54747AD7C7E)\n",
    "1. [Praveen Palanisamy - Hands-On Intelligent Agents with OpenAI Gym_ Your Guide to Developing AI Agents Using Deep Reinforcement Learning, 2020](http://library.lol/main/E4FD128CF9B93E0F7A542B053330517A)\n",
    "1. [Turing Paper 1936](http://www.thocp.net/biographies/papers/turing_oncomputablenumbers_1936.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd63d24",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ee4c4",
   "metadata": {},
   "source": [
    "*  [Introducción](#Introducción)\n",
    "* [Anatomía de un agente](#Anatomía-de-un-agente)\n",
    "* [Clase Environment](#Clase-Environment)\n",
    "* [Clase Agent](#Clase-Agent)\n",
    "* [Clase Action](#Clase-Action)\n",
    "* [Ejecución de un episodio](#Ejecución-de-un-episodio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be027523",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc08e0",
   "metadata": {},
   "source": [
    "En esta lección exploramos una implementación en Python de un agente en un situación muy simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eb2d39",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Anatomía de un agente</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068d77f3",
   "metadata": {},
   "source": [
    " El ambiente entregará una recompensa aleatoria al agente durante un número limitado de pasos. El ejemplo es extremadamente simple, pero muestra la interacción entre el agente y el ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc2c48",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Clase Environment</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2aa9acd6-b376-4d77-9e9b-74753d923ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Environment:\n",
    "    \"\"\"Define una clase Enviroment básica. \n",
    "    se adimitiran 10 recompensas aleatorias\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.steps_left = 10\n",
    "        self.current_state = np.array([0.0, 0.0, 0.0])\n",
    "    \n",
    "    # regresa una lista de floats (type annotations)\n",
    "    # usualmente esta es una función del estado actual del Environment\n",
    "    def next_observation(self, action):\n",
    "        if action == 1:\n",
    "            self.current_state += 1.0\n",
    "        else:\n",
    "            self.current_state -= 1.0\n",
    "        \n",
    "        return self.current_state \n",
    "       \n",
    "    def reward(self,  action):\n",
    "        return random.random()\n",
    "        \n",
    "    # determina si un episodio ha terminado o no\n",
    "    def is_done(self):\n",
    "        return self.steps_left == 0\n",
    "    \n",
    "    def reset(self):\n",
    "         self.steps_left = 10\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.steps_left -= 1\n",
    "        observation = self.next_observation(action)\n",
    "        reward = self.reward(action)\n",
    "        is_done = self.is_done()\n",
    "        return observation, reward, is_done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14865f0a-2363-4b45-8041-9e73296d0c1c",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Clase Action</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3dd585b4-40ab-46d7-8afc-cbaf45615803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action:\n",
    "    \"\"\"\n",
    "    Clase acción. Define una acción aleatoria\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.probs = [0.4, 0.6]\n",
    "        self.actions = [0, 1]\n",
    "    \n",
    "    def __call__(self):\n",
    "        a = np.random.choice(len(self.actions), p=self.probs)\n",
    "        return self.actions[a]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585e295c",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Clase Agent</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "151a1bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Define una clase Agent básica. \n",
    "    lleva la cuenta de la recompensa acumulada\"\"\"\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "        self.current_observation = None\n",
    "        self.actions = Action()\n",
    "        self.env = Environment()\n",
    "     \n",
    "    # acepta la instancia de Environment y hace las siguientes tareas:\n",
    "    # 1. Observa el ambiente\n",
    "    # 2. Toma un decisión sobre que acción tomar basado en las observaciones\n",
    "    # 3. somete la accion al ambiente\n",
    "    # 4. recibe la recompensa del paso actual y la acumula\n",
    "    def take_action(self):\n",
    "        action = self.actions()\n",
    "        observation, reward, is_done = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "        self.current_observation = observation\n",
    "        return is_done\n",
    "    \n",
    "    def get_total_reward(self):\n",
    "        return self.total_reward\n",
    "    \n",
    "    def get_current_observation(self):\n",
    "        return self.current_observation\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7cc9eb",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Ejecución de un episodio</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e64a2d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa total recibida: 4.3690\n",
      "Posición final:  [2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = Agent()\n",
    "    is_done = agent.take_action()\n",
    "    \n",
    "    while not is_done:\n",
    "        is_done = agent.take_action()\n",
    "    \n",
    "    print(\"Recompensa total recibida: %.4f\" % agent.get_total_reward())\n",
    "    print(\"Posición final: \", agent.get_current_observation())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
