{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6f8f0e",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718a9ce",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Agente simple</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2dca78",
   "metadata": {},
   "source": [
    "<center>Introducción</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e234f-f8e0-4b42-85c0-fb179612a115",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/cara-alambre.jpg\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: [Pixabay](https://pixabay.com/images/search/robot/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3dc5d-306b-40db-b995-17afb239e1d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a400b58-9c86-435a-bd55-4684b0738dc4",
   "metadata": {},
   "source": [
    "- Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "- Daniel  Montenegro, Msc, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ac01bd-02da-45be-91a7-b80a0df77828",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc866c0-2aa0-405d-9d3c-c58b5bbb9a4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783d283a-33ed-46e2-aa3a-e6d5c4cc44e9",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04627aa2-3a72-430f-9fe1-869b75063757",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Richard S. Sutton, Andrew G. Barto, Reinforcement learning: an introduction, 2nd edition, 2020](http://library.lol/main/6502B74CE247C4CD4D4FB54747AD7C7E)\n",
    "1. [Praveen Palanisamy - Hands-On Intelligent Agents with OpenAI Gym_ Your Guide to Developing AI Agents Using Deep Reinforcement Learning, 2020](http://library.lol/main/E4FD128CF9B93E0F7A542B053330517A)\n",
    "1. [Turing Paper 1936](http://www.thocp.net/biographies/papers/turing_oncomputablenumbers_1936.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd63d24",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ee4c4",
   "metadata": {},
   "source": [
    "*  [Introducción](#Introducción)\n",
    "* [Anatomía de un agente](#Anatomía-de-un-agente)\n",
    "    * [Clase Environment](#Clase-Environment)\n",
    "    * [Clase Agent](#Clase-Agent)\n",
    "    * [Clase Action](#Clase-Action)\n",
    "    * [Clase Trainer](#Clase-Trainer)\n",
    "    * [Ejecución de un episodio](#Ejecución-de-un-episodio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be027523",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc08e0",
   "metadata": {},
   "source": [
    "En esta lección exploramos una implementación en Python de un agente en un situación muy simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eb2d39",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Anatomía de un agente</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068d77f3",
   "metadata": {},
   "source": [
    "El ambiente entregará una recompensa aleatoria al agente durante un número limitado de pasos. El ejemplo es extremadamente simple, pero muestra la interacción entre el agente y el ambiente.\n",
    "\n",
    "Implementamos las siguientes  clases:\n",
    "\n",
    "* Environment\n",
    "* Action\n",
    "* Agent\n",
    "* Trainer\n",
    "\n",
    "Al final se tiene el código para ejecutar un entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfed08d-07cd-446a-a07f-8fa1e51ac579",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Clase Environment</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d153740-61ec-44cf-a52d-a0fe8deb4b1c",
   "metadata": {},
   "source": [
    "`Enviroment` es la clase encargada del manejo de ambiente. El constructor `__init__()` recibe el número de pasos que recorrerá un agente. El agente se moverá en tres dimensiones en la diagonal definida por los puntos $(x,x,x)$, en donde $x$ será un número entero. Prevemos que el agente dará un paso en el sentido positivo con probabilidad 0.6 y en sentido contrario con probabilidad 0.4. En cada paso el agente recibe una recompensa positiva, si avanza en el sentido positivo y negativa en caso contrario. El método is `is_done()` es implementado para que el ambiente indique que un episodio ha sido completado.\n",
    "\n",
    "El método `step()` implementa la interfaz (API) para el agente. Es mediante este método que el agente se comunicará con el ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2aa9acd6-b376-4d77-9e9b-74753d923ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "class Environment:\n",
    "    \"\"\"Define una clase Enviroment básica. \n",
    "    se adimitiran 10 recompensas aleatorias\"\"\"\n",
    "    \n",
    "    def __init__(self, steps=10):\n",
    "        self.steps = steps\n",
    "        self.steps_left = steps\n",
    "        self.current_state = np.array([0.0, 0.0, 0.0])\n",
    "    \n",
    "    # regresa una lista de floats (type annotations)\n",
    "    # usualmente esta es una función del estado actual del Environment\n",
    "    def next_observation(self, action):\n",
    "        if action == 1:\n",
    "            self.current_state += 1.0\n",
    "        else:\n",
    "            self.current_state -= 1.0\n",
    "        \n",
    "        return self.current_state \n",
    "       \n",
    "    def reward(self,  action):\n",
    "        r = np.abs(random.random())\n",
    "        if action == 0:\n",
    "            r = -r\n",
    "        return r\n",
    "        \n",
    "    # determina si un episodio ha terminado o no\n",
    "    def is_done(self):\n",
    "        return self.steps_left == 0\n",
    "    \n",
    "    def reset(self, steps=None):\n",
    "        self.steps = self.steps if steps is None else steps\n",
    "        self.steps_left = self.steps if steps is None else steps\n",
    "        self.current_state = np.array([0.0, 0.0, 0.0])   \n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.steps_left == 0:\n",
    "            raise Exception('El episodio ya fué completado!!')\n",
    "        self.steps_left -= 1\n",
    "        observation = self.next_observation(action)\n",
    "        reward = self.reward(action)\n",
    "        is_done = self.is_done()\n",
    "        return observation, reward, is_done, _\n",
    "    \n",
    "    def get_steps(self):\n",
    "        return self.steps\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39561b2a-f345-4c97-b6b0-9250a15e8114",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Clase Action</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d0c0b-a270-44e9-ad2e-9c759d9d5f9d",
   "metadata": {},
   "source": [
    "Esta clase implementa el generador de acciones. En este ejemplo, cada acción 0 o 1 respectivamente es generada siguiendo la distribución de probabilidad definida por el vector $[p_1, p_2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd585b4-40ab-46d7-8afc-cbaf45615803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action:\n",
    "    \"\"\"\n",
    "    Clase acción. Define una acción aleatoria\n",
    "    \"\"\"\n",
    "    def __init__(self, probs =[0.4, 0.6], actions=[0, 1]):\n",
    "        self.probs = probs\n",
    "        self.actions = actions\n",
    "    \n",
    "    def __call__(self):\n",
    "        a = np.random.choice(len(self.actions), p=self.probs)\n",
    "        return self.actions[a]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2001e10-f4a7-4e3e-918c-f5d3270c1600",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Clase Agent</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2ead6-74b9-4782-b11d-6fd081ae7510",
   "metadata": {},
   "source": [
    "Esta clase implementa un agente muy simple. El agente recibe una acción generada por su motor de acciones y se la entrega al ambiente mediante la interfaz `step` del ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "151a1bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Define una clase Agent básica. \n",
    "    lleva la cuenta de la recompensa acumulada\"\"\"\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "        self.current_observation = [0, 0, 0]\n",
    "        self.action = None\n",
    "        self.env = None\n",
    "    \n",
    "    def reset(self):\n",
    "        self.total_reward = 0.0\n",
    "        self.current_observation = [0, 0, 0]\n",
    "        \n",
    "    # acepta la instancia de Environment y hace las siguientes tareas:\n",
    "    # 1. Observa el ambiente\n",
    "    # 2. Toma un decisión sobre que acción tomar basado en las observaciones\n",
    "    # 3. somete la accion al ambiente\n",
    "    # 4. recibe la recompensa del paso actual y la acumula\n",
    "    \n",
    "    def set_config(self, action, env):\n",
    "        self.action = action\n",
    "        self.env = env\n",
    "    \n",
    "    def take_action(self):\n",
    "        # selecciona una acción\n",
    "        action = self.action()\n",
    "        # informa al ambiente de la acción y recibe retroalimentación\n",
    "        observation, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "        self.current_observation = observation\n",
    "        return observation, reward, is_done, _\n",
    "    \n",
    "    def get_total_reward(self):\n",
    "        return self.total_reward\n",
    "    \n",
    "    def get_current_observation(self):\n",
    "        return self.current_observation\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498898a3-3302-447d-8b9b-d3babd6d36ce",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Clase Trainer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cce830-9e1b-40eb-9ca8-d44be97afcbc",
   "metadata": {},
   "source": [
    "Esta clase entrenará los agentes para logren recompensas. *Trainer* instancia un motor de acciones, un ambiente y un agente- El método `episode()` ejecuta un episodio. El método `reset()` reconfigura el ambiente y el agente. Para el ambiente es posible cambiar el numero de pasos en el episodio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b83e77c7-9b9e-41be-a2f1-3f6b1cff1173",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self,  agent, action, env):\n",
    "        \"\"\"\n",
    "        Instancia objetos para el entrenamiento\n",
    "        \"\"\"\n",
    "        self.action = action\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        # configura al agente\n",
    "        self.agent.set_config(action, env)\n",
    "\n",
    "    def reset(self, steps=None):\n",
    "        self.env.reset(steps)\n",
    "        self.agent.reset()\n",
    "        \n",
    "    def episode(self):\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "            observation, reward, is_done, aux_inf = self.agent.take_action()\n",
    "        print(\"Episodio completado\")\n",
    "        print(\"Recompensa total recibida: %.4f\" % self.agent.get_total_reward())\n",
    "        print(\"Posición final: \", self.agent.get_current_observation())\n",
    "        print('Número de pasos', env.get_steps())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7cc9eb",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejecuta un episodio de entrenamiento</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e64a2d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio completado\n",
      "Recompensa total recibida: 1.0727\n",
      "Posición final:  [2. 2. 2.]\n",
      "Número de pasos 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# configura el Trainer\n",
    "steps = 10\n",
    "action = Action()\n",
    "env = Environment(steps)\n",
    "agent = Agent()\n",
    "trainer = Trainer(agent, action, env)\n",
    "\n",
    "# corre episodio\n",
    "trainer.episode()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1893bb44-eb2c-4f42-aeb1-e156ea91499e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio completado\n",
      "Recompensa total recibida: 1.0015\n",
      "Posición final:  [3. 3. 3.]\n",
      "Número de pasos 15\n"
     ]
    }
   ],
   "source": [
    "trainer.reset(15)\n",
    "trainer.episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf7c4a3d-789c-4582-a4c0-a200755155c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "El episodio ya fué completado!!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mTrainer.episode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m is_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_done:\n\u001b[0;32m---> 19\u001b[0m     observation, reward, is_done, aux_inf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisodio completado\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecompensa total recibida: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mget_total_reward())\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mAgent.take_action\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake_action\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     27\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction()\n\u001b[0;32m---> 28\u001b[0m     observation, reward, is_done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_observation \u001b[38;5;241m=\u001b[39m observation\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 40\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEl episodio ya fué completado!!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_left \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     42\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_observation(action)\n",
      "\u001b[0;31mException\u001b[0m: El episodio ya fué completado!!"
     ]
    }
   ],
   "source": [
    "trainer.episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e5f00-4cbd-4063-b149-50aac3b8a2df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
