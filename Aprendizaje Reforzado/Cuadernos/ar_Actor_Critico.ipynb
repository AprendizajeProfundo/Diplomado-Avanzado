{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Método Actor-Crítico</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Atlantis_seen_over_the_Mediterranean_Sea.jpeg\" width=\"800\" height=\"400\" align=\"center\"/>\n",
    "<figcaption>Templo de Confucio</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "<a href=\"https://commons.wikimedia.org/wiki/File:Atlantis_seen_over_the_Mediterranean_Sea,_near_the_Algerian_coast.jpeg\">NASA</a>, Public domain, via Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Adaptado de Rowel Atienza, Advance Deep Learning with Tensorflow 2 and Keras,Pack,2020](https://www.amazon.com/-/es/Rowel-Atienza-ebook/dp/B0851D5YQQ).\n",
    "1. [Sutton, R. S., & Barto, A. G. (2018).Reinforcement learning: An introductio, MIT Press, 2018](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)\n",
    "1. [Ejecutar en Colab](https://colab.research.google.com/drive/1ExE__T9e2dMDKbxrJfgp8jP0So8umC-A#sandboxMode=true&scrollTo=2XelFhSJGWGX)\n",
    "1. [Human-level control through deep reinforcement\n",
    "learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Refuerzo con línea base](#Refuerzo-con-línea-base)\n",
    "* [Método Actor-crítico ](#Método-Actor-crítico )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta lección exploramos el `método actor-crítico` que es una variante de los métodos básicos de [gradiente de política](ar_Gradientes_Politica.ipynb) estudiados antes. Este método mejora de forma casi mágica la estabilidad y velocidad de convergencia.\n",
    "\n",
    "Recordemos primero el método `Reinforce` con línea base introducido en la lección de gradiente de política."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Refuerzo con línea base </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Ya mencionamos antes que nuestra aproximación del gradiente de la función objetivo es dada por $\\nabla J \\approx \\mathbb{E}[Q(s,a)\\nabla \\pi(s|a)]$ es proporcional a la recompensa descontada desde un estado dado. Esta recompensa depende por supuesto del ambiente e introduce una fuente importante de variación. Una solución a este inconveniente es substraer de la recompensa una constante. Posibles soluciones son restar de la recompensa algún valor como por ejemplo:\n",
    " \n",
    "+ La media de las recompensas descontadas.\n",
    "+ La media móvil de las recompensas descontadas.\n",
    "+ El valor del estado V(s).\n",
    "\n",
    "La última solución es introducida en esta lección actor-crítico. Recuerde que el valor del estado $V(s)$ es definido como el retorno esperado partiendo del estado $s$.\n",
    "\n",
    "Por otro lado para recortar los episodios y no hacer cálculos innecesarios se puede determinar a partir de que momento el factor de descuento es tan pequeño que más pasos en el episodio no aportan mucho al cálculo de la recompensa descontada. Por ejemplo, $0.9^{50}= 0.005$. Entonces si el factor de descuento es $\\gamma=0.9$, quizás sea suficiente parar después de 50 pasos en un episodio.\n",
    "\n",
    "\n",
    "La introducción de un constante que reste a la recompensa descontada se basa en el siguiente hecho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede verificarse que\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\theta}\\left[\\left( \\nabla \\log \\pi_{\\theta}(a_t|s_t) \\right) \\right]=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En efecto, bajo el supuesto que la integral y el gradiente pueden intercambiarse, se tiene que\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\theta}\\left[\\left( \\nabla \\log \\pi_{\\theta}(a|s) \\right) \\right] =\\int \\pi_{\\theta}(a|s)\\nabla \\log \\pi_{\\theta}(a|s)d\\tau =  \\int \\nabla \\pi_{\\theta}(a|s)d\\tau= \\nabla\\int  \\pi_{\\theta}(a|s)d\\tau = \\nabla  1 = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La verificación meticulosa, se  deja como ejercicio al lector interesado. Sea $b$ una variable que no depende de la trayectoria $\\tau$.  Esta variable se denominará `línea base`. Debido al resultado anterior se tiene que\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\nabla \\mathbb{E}_{\\pi_{\\theta}}[r(\\tau)] = \\mathbb{E}_{\\pi_{\\theta}}\\left[  \\sum_{t=1}^{T}(G_t-b)\\nabla\\log \\pi_{\\theta}(a_t|s_t) \\right].\n",
    "$$\n",
    "\n",
    "Usando $b$ en teoría y en la práctica la varianza puede ser reducida, manteniendo el gradiente de la política insesgado. Un buen valor que puede usarse como línea base el valor del estado actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Método actor-crítico </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\"> Reescritura de la recompensa total</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que en la lección de extensiones de DQN, en particular en la extensión Dueling DQN indicamos que  la recompensa total puede escribirse en la forma\n",
    "\n",
    "$$\n",
    "Q(s,a) = V(s) + A(s,a)\n",
    "$$\n",
    "\n",
    "en donde $V(s)$ es el valor del estado $s$ y $A(a,s)$ es la ventaja de seleccionar la acción $a$.\n",
    "\n",
    "Entonces la pregunta que surge es: ¿Y si usamos como línea base $V(s)$?. Esto parece una idea interesante porque ahora el gradiente tendría un peso asociado directamente a la ventaja de selecciona la acción $a$.\n",
    "\n",
    "En realidad, esta es una gran idea. Pero tenemos un problema. No disponemos del valor $V(s)$.\n",
    "\n",
    "Para resolverlo introducimos una segunda red que estime el valor del estado.\n",
    "\n",
    "Entonces, tendremos dos redes, que pueden ser implementadas en una sola unidad neuronal. Esta redes son\n",
    "\n",
    "+ Red de la política (actor)\n",
    "+ Red del valor (crítico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"../Imagenes/actor-critic_01.png\"  width=\"400\" height=\"400\" align=\"center\"/> \n",
    "</figure>\n",
    "\n",
    "Fuente: Maxi Lapan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea central detrás de este método es la siguiente. La red política (actor) dice que hacer. A su vez la red valor (crítico) dice que tan buenas fueron las acciones.\n",
    "\n",
    "En la práctica las dos redes se traslapan principalmente por razones de eficiencia y convergencia.  Ambas redes se implementan como dos cabeza de una red con un cuerpo común, tal como se muestra en la siguiente imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"../Imagenes/actor-critico_2.png\"  width=\"400\" height=\"400\" align=\"center\"/> \n",
    "</figure>\n",
    "\n",
    "Fuente: Maxi Lapan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\"> Algortimo actor-crítico (A2C)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Inicializa los parámetros de la red con valores aleatorios.\n",
    "2. Corre N pasos en el ambiente usando la actual política $\\pi_{\\theta}$, y almacenando $(s_t, a_t, r_t)$.\n",
    "3. $R=0$ si el final del episodio es alcanzado. Si no $R = V_{\\theta}(s)$.\n",
    "4. Para $i=t-1, \\ldots, t_{start}$ (los pasos son procesados hacia atrás)\n",
    "    + $R = r_i + \\gamma R$\n",
    "    + Acumula gradientes de política: $\\partial \\theta_{\\pi} = \\partial \\theta_{\\pi} + \\nabla_{\\theta} \\log \\pi_{\\theta}(a_i|s_i)(R-V_{\\theta}(s_i))$\n",
    "    + Acumula gradientes de valor: $\\partial \\theta_{\\nu} = \\partial \\theta_{\\nu} + \\tfrac{\\partial(R- V_{\\theta}(s_i))}{\\partial \\theta_{\\nu}}$\n",
    "5. Actualiza los parámetros de la red usando los gradientes acumulados, moviendo en la dirección de los gradientes de la política, $\\partial \\theta_{\\pi}$ y en la dirección opuesta de los gradientes de valor $\\partial \\theta_{\\nu}$.\n",
    "6. Repite desde 2 hasta alcanzar convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\"> Nota de implementación</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo general la función de pérdida incluye la pérdida de la política, la pérdida de valor y una perdida entropía, la cual introduce el efecto de empujar al agente lejos de tener certidumbre acerca de sus acciones. La entropía introducida es $$\n",
    "\\mathfrak{L}_H = \\beta\\sum_t \\pi_{\\theta}(s_i)\\log \\pi_{\\theta}(s_i)\n",
    "$$\n",
    "Aquí $\\beta$ es un hiperparámetro en el problema. Por ejemplo tomaremos $\\beta= 0.01$ en nuestro ejemplo. \n",
    "\n",
    "Si $\\mathfrak{L}_{\\pi}$ denota la función de pérdida asociada a la política y $\\mathfrak{L}_V$ la pérdida asociada al valor del estado, la pérdida total de la técnica actor crítico (A2C) es dada por \n",
    "\n",
    "$$\n",
    "\\mathfrak{L}_{A2C} = \\mathfrak{L}_{\\pi} + \\mathfrak{L}_V + \\mathfrak{L}_H\n",
    "$$\n",
    "\n",
    "Observe que $\\mathfrak{L}_H$ actúa como un regularizador para la función de pérdida.\n",
    "\n",
    "Finalmente, para mejorar la estabilidad es usual trabajar con varios ambientes, los cuales entregan observaciones de manera concurrente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Ejemplo: A2C en Pong </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es una implementación del entrenamiento de actor-crítico para el juego Pong. Usaremos la librería [Ptan](https://github.com/Shmuma/ptan) del profesor Max Lapan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">  Importa módulos</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "import argparse\n",
    "import collections\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.optim as optim\n",
    "\n",
    "from lib import common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">  Hiperparámetros</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.0001\n",
    "ENTROPY_BETA = 0.01\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "REWARD_STEPS = 10\n",
    "BASELINE_STEPS = 1000000\n",
    "GRAD_L2_CLIP = 0.1\n",
    "\n",
    "ENV_COUNT = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\"> Clase Atari2C</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer objeto importante es la implementación de la red neuronal que se utilizará. La siguiente celda muestra la implementación completa para el ejemplo que hemos considerado.\n",
    "\n",
    "Como se dijo antes, se tiene un cuerpo común y dos cabezas de red, una para la política (policy) y la otra para el valor (value). \n",
    "\n",
    "Similarmente a como hicimos en el método [Reinforcement](ar_Gradientes_Politica.ipynb), la red tiene dos cabezas que se desprenden del cuerpo común. Sin embargo, en este caso, el método *forward()* retorna el tensor de los logits de la política y el tensor del valor en forma separada. Es decir, la salida de la red es $[q_{\\theta}(s), V_{\\theta}(s)]$. La red entrega una pre-probabilidad (logits). Para obtener $\\pi_{\\theta}(s)$, se calcula \n",
    "\n",
    "$$\n",
    "\\pi_{\\theta}(s) = \\text{softmax} (q_{\\theta}(s))\n",
    "$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariA2C(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(AtariA2C, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "            )\n",
    "        \n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.policy(conv_out), self.value(conv_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\"> Función unpack_batch()</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos una función grande e importante, que toma la\n",
    "lote de transiciones de entorno y devuelve tres tensores: el lote de estados,\n",
    "lote de acciones realizadas y lote de valores Q calculados utilizando la fórmula\n",
    "\n",
    "$$\n",
    "Q(s,a) = \\sum_{i=0}^{N-1} \\gamma^i r_i + \\gamma^NV(s_N)\n",
    "$$\n",
    "\n",
    "Este valor $Q$ se utilizará en dos lugares: \n",
    " * para calcular pérdida del **error cuadrático medio (MSE)** para mejorar la aproximación del valor en el mismo manera como DQN, y \n",
    " * para calcular la ventaja de la acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_batch(batch, net, device='cpu'):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    not_done_idx = []\n",
    "    last_states = []\n",
    "    for idx, exp in enumerate(batch):\n",
    "        states.append(np.array(exp.state, copy=False))\n",
    "        actions.append(int(exp.action))\n",
    "        rewards.append(exp.reward)\n",
    "        if exp.last_state is not None:\n",
    "            not_done_idx.append(idx)\n",
    "            last_states.append(\n",
    "                np.array(exp.last_state, copy=False))\n",
    "\n",
    "    states_v = torch.FloatTensor(\n",
    "        np.array(states, copy=False)).to(device)\n",
    "    actions_t = torch.LongTensor(actions).to(device)\n",
    "\n",
    "    rewards_np = np.array(rewards, dtype=np.float32)\n",
    "\n",
    "    if not_done_idx:\n",
    "        last_states_v = torch.FloatTensor(\n",
    "            np.array(last_states, copy=False)).to(device)\n",
    "        last_vals_v = net(last_states_v)[1]\n",
    "        last_vals_np = last_vals_v.data.cpu().numpy()[:, 0]\n",
    "        last_vals_np *= GAMMA ** REWARD_STEPS\n",
    "        rewards_np[not_done_idx] += last_vals_np\n",
    "\n",
    "    ref_vals_v = torch.FloatTensor(rewards_np).to(device)\n",
    "    return states_v, actions_t, ref_vals_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\"> Entrenamiento</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente presentamos el código de entrenamiento.   \n",
    "\n",
    "**Para hacer (TODO):** Transformar este código en un Trainer como hemos hecho en otras lecciones.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # preparativos\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--cuda\", default=False,\n",
    "                        action=\"store_true\", help=\"Enable cuda\")\n",
    "    parser.add_argument(\"-n\", \"--name\", required=True,\n",
    "                        help=\"Name of the run\")\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "    # instancia los ambientes que trabajaran en paralelo\n",
    "    make_env = lambda: ptan.common.wrappers.wrap_dqn(\n",
    "            gym.make(\"PongNoFrameskip-v4\"))\n",
    "    envs = [make_env() for _ in range(NUM_ENVS)]\n",
    "    writer = SummaryWriter(comment=\"-pong-a2c_\" + args.name)\n",
    "\n",
    "    # instancia la red neuronal\n",
    "    net = AtariA2C(envs[0].observation_space.shape,\n",
    "                    envs[0].action_space.n).to(device)\n",
    "    print(net)\n",
    "    \n",
    "    # instancia el agente\n",
    "    agent = ptan.agent.PolicyAgent(\n",
    "            lambda x: net(x)[0], apply_softmax=True, device=device)\n",
    "    \n",
    "    # instancia el buffer de experiencia esta equipado para \n",
    "    # trabajar en paralelo  con varios ambientes\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "            envs, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "    # instancia el optimizador para los parámetros de la red neuronal\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE,\n",
    "                eps=1e-3)\n",
    "    \n",
    "    # crea la lista para recibir los lotes de datos del buffer de experiencia\n",
    "    batch = []\n",
    "    \n",
    "    # crea un contexto para ejecutar el ciclo de entrenamiento para controlar cuando se alcanza \n",
    "    # la frontera de recompensa  (18 en este caso)\n",
    "    with common.RewardTracker(writer, stop_reward=18) as tracker:\n",
    "        # crea un contexto para enviar la información a Tensorboard\n",
    "        with ptan.common.utils.TBMeanTracker(writer,\n",
    "            batch_size=10) as tb_tracker:\n",
    "            # empieza a extraer los datos del buffer de experiencia y\n",
    "            # a colocarlos en la lista batch\n",
    "            for step_idx, exp in enumerate(exp_source):\n",
    "                batch.append(exp)\n",
    "                new_rewards = exp_source.pop_total_rewards()\n",
    "                # termina si alcnza el umbral de recompensa\n",
    "                if new_rewards:\n",
    "                    if tracker.reward(new_rewards[0], step_idx):\n",
    "                        break\n",
    "                # verifica si ya tiene lel tamaño del batch definido\n",
    "                if len(batch) < BATCH_SIZE:\n",
    "                    continue\n",
    "                # usa la función unpack_batch para extraer estados, acciones y q-valores\n",
    "                # del lote (batch) de datos\n",
    "                states_v, actions_t, vals_ref_v = \\\n",
    "                        unpack_batch(batch, net, device=device)\n",
    "                # limpia la lista del lot3e de datos para el siguiente paso\n",
    "                batch.clear()\n",
    "                \n",
    "                # paso de optimización\n",
    "                \n",
    "                # gradientes en cero\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # calcula logits de la política y valores de estados con la red neuronal\n",
    "                logits_v, value_v = net(states_v)\n",
    "                \n",
    "                # calcula la función de pérdida (mse) para el valor del estado V(s): L_V\n",
    "                loss_value_v = F.mse_loss(\n",
    "                                value_v.squeeze(-1), vals_ref_v)\n",
    "                    \n",
    "                # calcula el logaritmo de la política a partir de los logits, usando log (softmax)\n",
    "                log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "                \n",
    "                # Calcula la ventaja V(a,s) normalizando con el valor del estado\n",
    "                # para disminuir la varianza en el muestreo para calcular la integral\n",
    "                adv_v = vals_ref_v - value_v.detach()\n",
    "                \n",
    "                # calcula el logaritmo de la probabilidad para toda slas acciones del lote de datos\n",
    "                log_p_a = log_prob_v[range(BATCH_SIZE), actions_t]\n",
    "                # multiplica por la ventaja\n",
    "                log_prob_actions_v = adv_v * log_p_a\n",
    "                # aproxima la integral (MCMC) para obtener la pérdida de la política: L_{\\pi}\n",
    "                loss_policy_v = -log_prob_actions_v.mean()    \n",
    "                \n",
    "                # Calcula la entropia para regularizar la funcipon de peŕdida: L_{\\beta}\n",
    "                prob_v = F.softmax(logits_v, dim=1)\n",
    "                ent = (prob_v * log_prob_v).sum(dim=1).mean()\n",
    "                entropy_loss_v = ENTROPY_BETA * ent\n",
    "                \n",
    "                #  Calcula los gradientes de la política\n",
    "                loss_policy_v.backward(retain_graph=True)\n",
    "                grads = np.concatenate([\n",
    "                            p.grad.data.cpu().numpy().flatten()\n",
    "                            for p in net.parameters()  if p.grad is not None\n",
    "                            ])\n",
    "                \n",
    "                # calcula los gradientes del valor de estado (adiciona la regularización L_{\\beta})\n",
    "                loss_v = entropy_loss_v + loss_value_v\n",
    "                loss_v.backward()\n",
    "                # recorta los gradientes de ser necesario\n",
    "                nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)\n",
    "                \n",
    "                # paso por el optimizador\n",
    "                optimizer.step()\n",
    "                \n",
    "                # acumula la pérdida total para colocar en Tensorboard\n",
    "                loss_v += loss_policy_v\n",
    "                \n",
    "                # para seguimiento en tensorboard\n",
    "                tb_tracker.track(\"advantage\", adv_v, step_idx)\n",
    "                tb_tracker.track(\"values\", value_v, step_idx)\n",
    "                tb_tracker.track(\"batch_rewards\", vals_ref_v, step_idx)\n",
    "                tb_tracker.track(\"loss_entropy\", entropy_loss_v, step_idx)\n",
    "                tb_tracker.track(\"loss_policy\", loss_policy_v, step_idx)\n",
    "                tb_tracker.track(\"loss_value\", loss_value_v, step_idx)\n",
    "                tb_tracker.track(\"loss_total\", loss_v, step_idx)\n",
    "                g_l2 = np.sqrt(np.mean(np.square(grads)))\n",
    "                tb_tracker.track(\"grad_l2\", g_l2, step_idx)\n",
    "                g_max = np.max(np.abs(grads))\n",
    "                tb_tracker.track(\"grad_max\", g_max, step_idx)\n",
    "                g_var = np.var(grads)\n",
    "                tb_tracker.track(\"grad_var\", g_var, step_idx)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
