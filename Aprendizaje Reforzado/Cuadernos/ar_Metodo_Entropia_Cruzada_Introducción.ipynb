{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6f8f0e",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718a9ce",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Método entropía cruzada</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d95e0eb-d692-405b-b0df-385ead144345",
   "metadata": {},
   "source": [
    "<center>Introducción al método</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321bc449-4793-4745-af77-7306a1228453",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/trainer.png\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282839f1-1628-4bfa-8977-e804bc2a5ee3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50553552-630a-47f0-a777-cb119510094d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Coordinador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f891a-1be2-4288-8961-64c0d4595f20",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Campo Elías Pardo, PhD, cepardot@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a56d2bd-4ca7-41d6-8788-3b8b8b57bbd2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Conferencistas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c38b7f-39a5-4dd0-84d8-51a85b2d3e16",
   "metadata": {},
   "source": [
    "- Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "- Daniel  Montenegro, Msc, dextronomo@gmail.com \n",
    "- Oleg Jarma, Estadístico, ojarmam@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b2df23-796e-430e-881c-681c3b6f6dbc",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2824bc41-3275-4632-a470-040704bd2887",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3c7a0d-6c1e-447b-8f9a-efc559a578da",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06032bac-c2b2-4610-87e4-b80dd4e8d002",
   "metadata": {},
   "source": [
    "- Nayibe Yesenia Arias, naariasc@unal.edu.co\n",
    "- Venus Celeste Puertas, vpuertasg@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd06c80a-262c-4050-ab8d-923e80d1b66e",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec7f62",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ac7d2",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Richard S. Sutton, Andrew G. Barto, Reinforcement learning: an introduction, 2nd edition, 2020](http://library.lol/main/6502B74CE247C4CD4D4FB54747AD7C7E)\n",
    "1. [Praveen Palanisamy - Hands-On Intelligent Agents with OpenAI Gym_ Your Guide to Developing AI Agents Using Deep Reinforcement Learning, 2020](http://library.lol/main/E4FD128CF9B93E0F7A542B053330517A)\n",
    "1. [Turing Paper 1936](http://www.thocp.net/biographies/papers/turing_oncomputablenumbers_1936.pdf)\n",
    "1. [Solving a Reinforcement Learning Problem Using Cross-Entropy Method](https://towardsdatascience.com/solving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd63d24",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ee4c4",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [El método de entropía cruzada](#El-método-de-entropía-cruzada)\n",
    "* [El algoritmo de entropía cruzada](#El-algoritmo-de-entropía-cruzada)\n",
    "* [Los detalles del método de la entropía cruzada](#Los-detalles-del-método-de-la-entropía-cruzada)\n",
    "* [Modificación del método para el  ambiente FrozenLake](#Modificación-del-método-para-el-ambiente-FrozenLake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be027523",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaee1cf1-9a63-4312-a3b7-50c153ff1582",
   "metadata": {},
   "source": [
    "La entropía cruzada se considera un algoritmo evolutivo: algunos individuos se muestrean de una población, y solo los de `élite` gobiernan las características de las generaciones futuras.\n",
    "\n",
    "Esencialmente, lo que hace el método de `entropía-cruzada`(cross-entropy) es tomar un montón de entradas, ver las salidas producidas, elegir las entradas que han llevado a las mejores salidas y ajustar el agente hasta que estemos satisfechos con las salidas que vemos.\n",
    "\n",
    "Antes de describir el método técnicamente vamos a repasar los elementos esenciales del aprendizaje reforzado (AR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24ca8ea-c4ee-4b12-af85-5f65c76ddc83",
   "metadata": {},
   "source": [
    "Recordamos que tenemos los siguientes elementos esenciales:\n",
    "\n",
    "\n",
    "- Ambiente: Espacio en donde se encuentra el problema por resolver. El ambiente contiene la información del espacio de observaciones y de las recompensas  que le entrega al agente cada vez que recibe del agente una acción.\n",
    "- Agente: Objeto que interactúa con el ambiente. El agente recibe del ambiente observaciones y recompensas. Con base en la observación recibida, el agente utiliza la política para decidirse por una acción que le entrega al ambiente. El proceso de intercambio entre el agente y el ambiente puede ocurrir en lotes, es decir, un intercambio de acciones y observaciones entre agente y ambiente puede ocurrir puede constar de varios eventos de intercambios.\n",
    "- Política: Objeto que recibe como entrada una observación y entrega como salida una distribución de probabilidades para que el agente pueda decidir que acción tomar. En esta lección, la política es implementada como una red neuronal. \n",
    "- Acción: Lo que el agente decide en cada momento, de acuerdo con la observación que tiene actualmente del ambiente, la cual procesa con la política.\n",
    "- Recompensa. Valor numérico que el ambiente le entrega al agente por arribar a un determinado lugar (observación) de acuerdo con la acción que este decidió.\n",
    "\n",
    "El objetivo del entrenamiento es determinar la política que le permite al agente recibir la mayor recompensa total. La siguiente imagen resume todo el proceso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b4575-2a33-434c-b8d0-6fe0132a6d28",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/agente-ambiente.png\" width=\"600\" height=\"500\" align=\"center\"/>\n",
    "</center> \n",
    "</figure>\n",
    "<center> El proceso del aprendizaje reforzado</center>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b5afe-ca07-43a6-9618-34bb7c4f9bb6",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">El método de entropía cruzada</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d91ad-bf7f-4688-860c-9835bd9363ec",
   "metadata": {},
   "source": [
    "Recuerde que una política, denotada por $\\pi(a|s)$, indica qué acción debe tomar el agente para cada estado observado. En esta lección, consideraremos que el cerebro de nuestro agente será una red neuronal que produce la política.\n",
    "\n",
    "\n",
    "Nos referimos a los métodos que resuelven este tipo de problemas como `métodos basados en políticas que entrenan la red neuronal que produce la política`. \n",
    "\n",
    "En la práctica, la política suele representarse como una distribución de probabilidad sobre las acciones (que el agente puede realizar en un estado determinado), lo que la hace muy similar a un problema de clasificación presentado anteriormente, con la cantidad de clases siendo igual a la cantidad de acciones que podemos realizar. En nuestro caso la salida de nuestra red neuronal es un vector de acción que representa una distribución de probabilidad como se muestra visualmente en la siguiente figura:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c8ab2-9809-4575-95d4-8c1480925c5e",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/frozen_action.png\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center> \n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f04bc1-6d2d-4148-a877-4632ca4c4f20",
   "metadata": {},
   "source": [
    "En este caso, se dice que tenemos  una política estocástica, porque devuelve una distribución de probabilidad sobre acciones en lugar de devolver una única acción determinista.\n",
    "\n",
    "Queremos una política, una distribución de probabilidad, y la inicializamos al azar. Luego, mejoramos nuestra política reproduciendo algunos episodios y luego ajustando nuestra política (parámetro de la red neuronal) de manera que sea más eficiente. Luego repita este proceso para que nuestra política mejore gradualmente. Esta es la base del método entropía-cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7381cd76-58c5-4f99-ad47-81e767a4e1c3",
   "metadata": {},
   "source": [
    "### Conjunto de datos de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3927d32b-d950-47e7-b774-a82922f54631",
   "metadata": {},
   "source": [
    "Dado que consideraremos una red neuronal como el cerebro  de este agente, necesitamos encontrar alguna forma de obtener datos que podamos asimilar como un conjunto de datos de entrenamiento, que incluye datos de entrada y sus respectivas etiquetas.\n",
    "\n",
    "\n",
    "La forma en que vamos a hacer esto es considerando el problema como un problema de aprendizaje supervisado donde los estados observados se consideran las características (datos de entrada) y las acciones constituyen las etiquetas.\n",
    "\n",
    "Durante la vida del agente, su experiencia se presenta como episodios. Cada episodio es una secuencia de observaciones de los estados que el agente obtuvo del ambiente, las acciones que emitió y las recompensas por estas acciones. \n",
    "\n",
    "El núcleo del método entropía-cruzada es descartar los malos episodios y entrenar en los mejores, entonces, \n",
    "\n",
    "**<center>¿cómo encontramos los mejores? </center>**\n",
    "\n",
    "\n",
    "Imaginemos que nuestro agente ha jugado varios episodios de este tipo. Para cada episodio, podemos calcular el retorno (recompensa total) que ha reclamado el agente. Recuerde que un agente intenta acumular la mayor cantidad posible de recompensas interactuando con el ambiente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177df21b-08eb-4fbf-a23a-dd0ede2b5bff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">El algoritmo de entropía cruzada </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ee5efb-b72d-410e-8a72-4daad136c424",
   "metadata": {
    "tags": []
   },
   "source": [
    "Desde un punto de vista biológico, es un Algoritmo Evolutivo. Algunos individuos son muestreados de una población y solo los mejores gobiernan las características de las generaciones futuras.\n",
    "\n",
    "El núcleo del método entropía cruzada es simple. Básicamente, \n",
    "\n",
    "+ se generan lotes de episodios, \n",
    "+ desecha los episodios malos en un lote para entrenar la red neuronal del agente en los mejores. \n",
    "\n",
    "Para decidir cuáles desechar, usamos el percentil digamos 70 en nuestro ejemplo, lo que significa que solo conservamos el 30 % que lo hizo mejor que el 70 % de los demás.\n",
    "\n",
    "Como resultado, la red neuronal aprende a repetir acciones que conducen a que los resultados de las decisiones de la red neuronal sean cada vez mejores a medida que usamos nuevos lotes de episodios de élite. El agente debe ser entrenado hasta alcanzar una determinada recompensa promedio por el umbral del lote de episodios.\n",
    "\n",
    "Un pseudocódigo del método se puede describir mediante los siguientes pasos:\n",
    "\n",
    "1. Inicializar el modelo de red neuronal del agente. Cree un lote de episodios que se reproducen en el entorno utilizando nuestro modelo de agente actual.\n",
    "1. Calcule el retorno esperado para cada episodio y decida un límite de retorno utilizando un percentil de todas las recompensas.\n",
    "1. Deseche todos los episodios con un retorno por debajo del límite de retorno.\n",
    "1. Entrene la red neuronal del Agente utilizando los pasos del episodio, es decir, las transiciones <s,a,r>) de los episodios \"élite\" restantes, utilizando el estado s como entrada y las acciones emitidas a como etiqueta.\n",
    "1. Repita desde el paso 2 hasta que estemos satisfechos con la Recompensa promedio promedio para el lote de episodios.\n",
    "\n",
    "\n",
    "Una variante del método,  es que podemos mantener los episodios “élite” durante más tiempo. Quiero decir que la versión predeterminada del algoritmo muestra episodios del entorno, entrena con los mejores y los descarta. Sin embargo, cuando la cantidad de episodios exitosos es pequeña, los episodios de \"élite\" se pueden mantener por más tiempo, manteniéndolos durante varias iteraciones para entrenarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d58e196-e811-4c81-8fcc-89789d0fa51d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Los detalles del método de la entropía cruzada</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84d03d2-54a0-4cba-b503-2df38836d06d",
   "metadata": {},
   "source": [
    "Como se puede observar, en la lección [Implementación del Método de entropía cruzada](ar_Metodo_Entropia_Cruzada_implementacion.ipynb), el método funciona bastante bien para el ejemplo CartPole. En este caso, se recibe la recompensa en cada paso. Por otro lado, en el ejemplo del ambiente FrozenLake el algoritmo termina por número máximo de iteraciones. Vamos a revisar la razón por la cual el método no funciona en este caso. En la siguiente sección se introducen algunas modificaciones que harán que el método converja para el problema FrozenLake. Más adelante retomaremos este ambiente con otro tipo de entrenamiento.  En la siguiente imagen se observan respectivamente la distribución de la recompensa para los casos de CartPole y FrozenLake. El problema con FrozenLake es que la recompensa solamente se entrega al fnal de cada episodio.\n",
    "\n",
    "\n",
    "Vamos a describir algunos detalles mínimos del método de entropía cruzada. Para obtener información ampliada del método puede consultar [A Tutorial on the Cross-Entropy Method](http://web.mit.edu/6.454/www/www_fall_2003/gew/CEtutorial.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53085bf8-edd2-468c-8e26-cf8b664cdc14",
   "metadata": {},
   "source": [
    "Fuente: [Maxim Lapan](http://library.lol/main/2E612EDEF1D325B87C7F5B1FB5542964)\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/distribucion_Reward_CartPole.png\" width=\"400\" height=\"400\" align=\"left\"/>\n",
    "<img src=\"../Imagenes/distribucion_Reward_FrozenLake.png\" width=\"400\" height=\"400\" align=\"right\"/>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231823c5-0f1d-47e8-8bc2-eb64df7e1674",
   "metadata": {},
   "source": [
    "Debido a que en el caso de CartPole, la recompensa es entregada por cada paso en cada episodio, cada episodio entrega valores de recompensa que permite tener una distribución, que puede verse de forma aproximada como normal. No lo es en realidad, pero ciertamente hay valores diferentes para cada episodio, que dan cuenta de como estuvo el agente en el episodio. Por otro lado, FrozenLake solamente entrega dos tipos  de recompensa por episodio: uno o cero, según el agente haya arribado a la meta o no. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc92e9-a38f-478c-b9d1-af44ce9261c8",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/probility_space.png\" width=\"500\" height=\"500\" align=\"centert\"/>\n",
    "</center>\n",
    "</figure>\n",
    "<center> Espacio de probabilidad</center>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9366eb-9f2b-454c-a5f0-f14aedfd988f",
   "metadata": {},
   "source": [
    "El objetivo del algoritmo de AR es estimar la distribución de las acciones en el espacio de las observaciones. Si se dispone de una muestra de la distribución, la estimación es posible mediante el entrenamiento de una red neuronal. Eso es lo que lo se hace exáctamente cuando se entrena una red neuronal de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d89c7e-024d-4efe-a87b-a472aede5a60",
   "metadata": {},
   "source": [
    "El problema que tenemos en aprendizaje reforzado es que tal muestra no está disponible inicialmente. Entonces, el método de entropía cruzada (MEC) provee un procedimiento estocástico para resolver el problema.\n",
    "\n",
    "El MEC se basa en que en cada paso del algoritmo se obtiene una aproximación nejorada de la distribución de tal manera que cada vez se obtienen mejores muestras, en el sentido que la distribución aproximadamente se parece cada vez más a la distribución de las acciones.\n",
    "\n",
    "La distribución aproximadamente es calculada por la red neuronal. El propósito de tomar los episodios *élite* por supuesto es mantener aquellos episodios que entregan mayor recompensa. \n",
    "\n",
    "El MEC funciona en CarPole, porque los episodios más largos son aquellos en donde el agente usa mejor la información de las observaciones para evitar que la viga caiga. \n",
    "\n",
    "El problema en el ambiente frozeLake, es que la recompensa de cada episodio es 1 o 0, sin importar que sucedió en cada paso del episodio. Como ilustra la imagen de la distribución en este caso. Entonces, no hay realmente forma de obtener episodios élite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801b309-6826-4f16-abf6-954123c8588f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Modificación del método para el ambiente FrozenLake</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7418d3f9-1ca1-4f01-8a98-6a885d75dee0",
   "metadata": {},
   "source": [
    "Más adelante en este curso volveremos a este ambiente para resolver las limitaciones del MEC con otros métodos de aprendizaje reforzado.\n",
    "\n",
    "De momento haremos unas mejoras que ayuden al MEC a estimar la distribución de las acciones en el espacio de las observaciones. Haremos los siguiente:\n",
    "\n",
    "* Lotes de episodios más largos. Pasaremos a 100 episodios por lote.\n",
    "* Aplicaremos  el factor de descuento $\\gamma$ a la recompensa. Asi episodios más largos tendrán una menor recompensa y viceversa. Esto incrementa la variabilidad de la distribución de la recompensa.\n",
    "* Mantendremos episodios élite por más largo tiempo.\n",
    "* Decreceremos la rata de aprendizaje. Esto implica que la red neuronal tendrá más tiempo para ver en promedio mas muestras de entrenamiento.\n",
    "* Mucho mayor tiempo de entrenamiento.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
