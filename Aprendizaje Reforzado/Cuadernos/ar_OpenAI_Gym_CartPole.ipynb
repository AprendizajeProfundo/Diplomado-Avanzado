{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6f8f0e",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718a9ce",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Introducción al ambiente Gym-CartPole</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c383b39b-7af1-4d0d-8dcc-97809d8dd444",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/curricle.jpg\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente [curricle - Wiki commons](https://commons.wikimedia.org/w/index.php?search=cart+pole&title=Special:MediaSearch&go=Go&type=image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362eed29-816d-4f5d-b1fe-4c23570ddb96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c3018b-55e3-4acd-96b0-36413cd12086",
   "metadata": {},
   "source": [
    "- Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "- Daniel  Montenegro, Msc, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca023c8-76e4-4ca9-9c74-8640edb4049f",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecf396e-35a1-4315-9c6e-e36f086b541f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec7f62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ac7d2",
   "metadata": {},
   "source": [
    "1. [OpenAI Gym - Github](https://github.com/openai/gym)\n",
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Richard S. Sutton, Andrew G. Barto, Reinforcement learning: an introduction, 2nd edition, 2020](http://library.lol/main/6502B74CE247C4CD4D4FB54747AD7C7E)\n",
    "1. [Praveen Palanisamy - Hands-On Intelligent Agents with OpenAI Gym_ Your Guide to Developing AI Agents Using Deep Reinforcement Learning, 2020](http://library.lol/main/E4FD128CF9B93E0F7A542B053330517A)\n",
    "1. [Turing Paper 1936](http://www.thocp.net/biographies/papers/turing_oncomputablenumbers_1936.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd63d24",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ee4c4",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Una sesión con CartPole](#Una-sesión-con-CartPole)\n",
    "    * [Observaciones en CartPole](#Observaciones-en-CartPole)\n",
    "    * [Clase Agent](#Clase-Agent)\n",
    "    * [Ejecución de un episodio](#Ejecución-de-un-episodio)\n",
    "* [Envolturas para el ambiente (wrappers)](#Envolturas-para-el-ambiente-(wrappers))\n",
    "    * [Clase RandomActionWrapper](#Clase-RandomActionWrapper)\n",
    "    * [Corriendo un episodio](#Corriendo-un-episodio)\n",
    "    * [Ejercicio](#Ejercicio)\n",
    "    * [Monitor](#Monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be027523",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a7700-4d56-46f6-b7ef-95b3c38f8656",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Ejemplo_polea.gif\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "<center>Ejemplo CartPole en OpenAI Gym</center>\n",
    "\n",
    "Fuente [OpenAI Gym](https://gym.openai.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc08e0",
   "metadata": {},
   "source": [
    "Este primer  ejemplo de OpneAI Gym conocido como *CartPole* es un clásico en la teoría de control. En la animación *gif* arriba, se  observa el objeto de interés. Es un palo, sostenido en la parte inferior por un pasador unido a una base. Ignoramos posibles complicaciones como la fricción. Es decir suponemos que el palo se mueve libremente sobre el pasador. \n",
    "\n",
    "Obviamente el palo puede caer muy fácilmente, por lo que el propósito es mover la base hacia la izquierda o hacia la derecha para evitar que el palo caiga. Ese es el problema a resolver. El problema puede ser abordado como un problema de control que lleva a diseñar un modelo en ecuaciones diferenciales. Pero en aprendizaje reforzado buscamos resolver el problema, sin utilizar los conceptos correspondientes de la física . De momento vamos a explorar un poco el  ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb416050-c486-47dc-9a9f-041bbb0d2cc8",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Una sesión con CartPole</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa16297-0cb2-4c51-b444-a20d6a4263f7",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Observaciones en CartPole</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f22f8d-a004-4d38-8898-74b10c0c850b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04493975,  0.00877326, -0.04641033, -0.01915632], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observaciones del ambiente CartPole\n",
    "import gym\n",
    "\n",
    "# crea una instancia del ambiente y lo configura en el estado inicial\n",
    "e = gym.make('CartPole-v1')\n",
    "obs = e.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e471783-c9ea-4fc6-8a23-68eee6d30394",
   "metadata": {},
   "source": [
    "El ambiente ha regresado una observación que consiste de una línea cuyos valores corresponden respectivamente a \n",
    "\n",
    "* coordenada del centro de masas,\n",
    "* velocidad,\n",
    "* ángulo con respecto a la plataforma, y\n",
    "* velocidad angular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e845a0-fc48-4034-ac5a-ccebdaea851c",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Clase Agent</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b29e817-0ded-4be0-a468-151c1d58f52b",
   "metadata": {},
   "source": [
    "El constructor de la clase recibe un ambiente con el cual interactuará. Un objeto *Agent* mantiene la cuenta de la recompensa total recibida, el número de pasos que ha ejecutado en el episodioy la actual observación. El método *action()* solicita al ambiente una acción aleatoria. El método *take_action()* solicita una acción y la entrega al ambiente a través del método *step()* del ambiente. El ambiente regresa la lista conteniendo *[observación, recompensa, is_done, info]*. El último elemento de la lista es un diccionario que en este caso viene vacío. Los demás componentes son obvios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c859a4c-4479-4618-b498-f37992486fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Define una clase Agent básica. \n",
    "    lleva la cuenta de la recompensa acumulada\"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.total_reward = 0.0\n",
    "        self.total_steps = 0\n",
    "        self.current_observation = None\n",
    "        self.env = env\n",
    "        \n",
    "        \n",
    "    def action(self):\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    def take_action(self):\n",
    "        action = self.action()\n",
    "        observation, reward, is_done, info = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "        self.current_observation = observation\n",
    "        self.total_steps += 1\n",
    "        return is_done\n",
    "    \n",
    "    def get_total_reward(self):\n",
    "        return self.total_reward\n",
    "    \n",
    "    def get_current_observation(self):\n",
    "        return self.current_observation\n",
    "    \n",
    "    def get_total_steps(self):\n",
    "        return self.total_steps\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99111ca-e68a-4acb-b7af-686b5336d388",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejecución de un episodio</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c176c0-3ad2-4ece-9d64-65bcf866e115",
   "metadata": {},
   "source": [
    "En este experimento no entrenaremos al agente. Solamente vdeseamos que  interactúe con el ambiente de forma aleatoria. Por ello, no diseñaremos ningún entrenador (*Trainer*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b0d6a1-c249-447e-a9ed-ae4b1aeef7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa total recibida: 25.0000\n",
      "Posición final:  [-0.24828029 -0.54381543  0.21167627  0.93399453]\n",
      "número de pasos en el episodio 25\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# crea el ambiente\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# inicializa el ambiente\n",
    "_ = env.reset()\n",
    "\n",
    "# crea el agente y ejecuta una primera acción\n",
    "agent = Agent(env)\n",
    "is_done = agent.take_action()\n",
    "    \n",
    "while not is_done:\n",
    "    is_done = agent.take_action()\n",
    "\n",
    "print(\"Recompensa total recibida: %.4f\" % agent.get_total_reward())\n",
    "print(\"Posición final: \", agent.get_current_observation())\n",
    "print(\"número de pasos en el episodio\", agent.get_total_steps())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94082efd-27e8-4603-82dd-1804b4443c0d",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Envolturas para el ambiente (wrappers)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12242952-bec7-43c4-8c88-aebed337c16d",
   "metadata": {},
   "source": [
    "Esta funcionalidad de Gym permite reescribir el comportamiento del ambiente. Observe que la clase *Wrapper* se deriva (envuelve) a la clase Env. Las clases *ObservationWrapper*, *RewardWrapper* y *ActionWrapper*, derivan de Wrapper y permiten extender las respectivas funcionalidades. En general, nosotros implementamos esta clases para hacer modificaciones al comportamiento implementado originalmente en el respectivo ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7cfe3-166d-4a2e-995a-4365f55bf6a9",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/wrappers.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<caption><center>Jerarquía de las clases Wrapper de OpenAI Gym. </center></caption>\n",
    "</figure>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f71c03-d8f0-4b23-be35-eb4c1676ae85",
   "metadata": {},
   "source": [
    "Para ejemplificar, vamos a crear una clase derivada de *ActionWrapper* la cual modifica la acción determinada en el ambiente CartPole. La idea es la siguiente.\n",
    "\n",
    "1. Establecemos un umbral $\\epsilon$, que es un número en el intervalo abierto (0.0, 1.0). Por defecto colocaremos $\\epsilon=0.1$.\n",
    "2. Con probabilidad $\\epsilon$ cambiamos la acción solicitada por una seleccionada aleatoriamente por el ambiente.\n",
    "3. Cada vez que se cambie la acción por otra seleccionada por el ambiente en forma aleatoria imprimimos la palabra *Random*, para informar que ocurrió un cambio aleatorio en la elección de la acción.\n",
    "\n",
    "Veamos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597a7197-2698-41fb-b303-99ac0e1fac4b",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Clase RandomActionWrapper</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2758ff5-64fe-450f-bc40-4e9768aa9a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from typing import TypeVar\n",
    "import random\n",
    "\n",
    "Action = TypeVar('Action')\n",
    "\n",
    "class RandomActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, epsilon=0.1):\n",
    "        super(RandomActionWrapper, self).__init__(env)\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def action(self, action: Action) -> Action:\n",
    "        if random.random() < self.epsilon:\n",
    "            print('Random!')\n",
    "            return self.env.action_space.sample()\n",
    "        return action\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8122e61-9d17-467a-a717-a85cf1cf3aa1",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Corriendo un episodio</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed56e2f-0551-4870-81dd-fc4de3498752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random!\n",
      "Recompensa total recibida: 15.0000\n",
      "Posición final:  [-0.16114676 -1.0257902   0.20952614  1.7508649 ]\n",
      "número de pasos en el episodio 15\n"
     ]
    }
   ],
   "source": [
    "# Crea el ambiente con la envoltura RandomActionWrapper\n",
    "env = RandomActionWrapper(gym.make('CartPole-v1'))\n",
    "\n",
    "# inicializa el ambiente\n",
    "_ = env.reset()\n",
    "\n",
    "# crea el agente y ejecuta una primera acción\n",
    "agent = Agent(env)\n",
    "is_done = agent.take_action()\n",
    "    \n",
    "while not is_done:\n",
    "    is_done = agent.take_action()\n",
    "\n",
    "print(\"Recompensa total recibida: %.4f\" % agent.get_total_reward())\n",
    "print(\"Posición final: \", agent.get_current_observation())\n",
    "print(\"número de pasos en el episodio\", agent.get_total_steps())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffe610d-cd77-43e8-9672-d86c231582aa",
   "metadata": {},
   "source": [
    "Observe que en uno de los 15 pasos la acción entregada por el ambiente fue cambiada por una selección aleatoria de otra acción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5c719-f4c1-4dae-8fa7-642b807b744b",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejercicio</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb8ad0-fcf2-4f6d-8356-8ec4df9c84e1",
   "metadata": {},
   "source": [
    "Escriba un clase que modifique la recompensa. Ejecute el correspondiente episodio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059a0fe-2b61-4439-83ae-6e312b8eeb80",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Monitor</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab937607-3a7d-4293-9650-bb5287a584ee",
   "metadata": {},
   "source": [
    "Monitor es un tipo de Wrapper que permite monitorar gráficamente un episodio. Por ejemplo, podemos grabar un episodio en formato mp4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277ad1ca-d606-4599-988e-e5e77619aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fdf7f5d-4793-447e-ac89-7e6d66b5dd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa total recibida: 11.0000\n",
      "Posición final:  [ 0.21101531  1.3672142  -0.23490441 -2.1950264 ]\n",
      "número de pasos en el episodio 11\n"
     ]
    }
   ],
   "source": [
    "# crea el ambiente\n",
    "from gym import wrappers\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = gym.wrappers.Monitor(env, \"recording\")\n",
    "\n",
    "# inicializa el ambiente\n",
    "_ = env.reset()\n",
    "\n",
    "# crea el agente y ejecuta una primera acción\n",
    "agent = Agent(env)\n",
    "is_done = agent.take_action()\n",
    "    \n",
    "while not is_done:\n",
    "    is_done = agent.take_action()\n",
    "\n",
    "print(\"Recompensa total recibida: %.4f\" % agent.get_total_reward())\n",
    "print(\"Posición final: \", agent.get_current_observation())\n",
    "print(\"número de pasos en el episodio\", agent.get_total_steps())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b60ee12-b0d5-4e7d-bfc6-4eaa641b1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
