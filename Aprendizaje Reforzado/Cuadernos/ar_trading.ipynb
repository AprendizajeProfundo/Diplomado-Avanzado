{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf27a948-429e-48e2-87cc-ea1a84df035a",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5fb777-1dbe-476a-9ad2-3b591365515e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:red\"><center>Bot Financiero con Aprendizaje Reforzado</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9cd8fb-6018-4908-b6db-4995f95c77db",
   "metadata": {},
   "source": [
    "<center>Entrenamiento de una agente para hacer algorithm trading</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac24db1-3338-4ab9-a313-a6aab392e443",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center><img src=\"../Imagenes/trading.jpeg\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Algorithmic trading</p>\n",
    "</figcaption>\n",
    "</center> \n",
    "</figure>\n",
    "\n",
    "Fuente: [Money control](https://www.moneycontrol.com/news/business/markets/explained-what-is-algo-trading-and-why-sebi-wants-to-regulate-retail-investor-activity-in-this-segment-7812111.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ebc3d-1c45-4ef0-95ea-20c9faafd864",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e24147-6943-4450-9352-cd776180b00b",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fa92ee-9485-4873-bfd3-999b287b4402",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b8567e-4755-4a62-ab1a-5b2479338993",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821dd03-252e-4af0-b3ef-3041dd01f68f",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47fff43-b072-4a21-b7c2-c5455f75690b",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9c3f21-3032-4821-9131-98c28ed992aa",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Adaptado de Rowel Atienza, Advance Deep Learning with Tensorflow 2 and Keras,Pack,2020](https://www.amazon.com/-/es/Rowel-Atienza-ebook/dp/B0851D5YQQ).\n",
    "1. [Sutton, R. S., & Barto, A. G. (2018).Reinforcement learning: An introductio, MIT Press, 2018](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)\n",
    "1. [Ejecutar en Colab](https://colab.research.google.com/drive/1ExE__T9e2dMDKbxrJfgp8jP0So8umC-A#sandboxMode=true&scrollTo=2XelFhSJGWGX)\n",
    "1. [Human-level control through deep reinforcement\n",
    "learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a3c20e-68f1-4b27-ad99-757ed896f3ec",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf6604-7b63-4947-8bde-604a646e873c",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Los datos](#Los-datos)\n",
    "* [Gráfico de velas](#Gráfico-de-velas)\n",
    "* [Descripción del problema y primeras decisiones](#Descripción-del-problema-y-primeras-decisiones)\n",
    "* [Librería aplr](#Librería-aplr)\n",
    "* [Librería trading](#Librería-trading)\n",
    "* [Modelos-Dueling](#Modelos-Dueling)\n",
    "* [Implementación de los Modelos](#Implementación-de-los-Modelos)\n",
    "* [Entrenamiento del modelo](#Entrenamiento-del-modelo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca2dd41-0e16-461d-b410-27e8e50681e1",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3cf294-c5fd-482e-b2e3-585a66f84ca8",
   "metadata": {},
   "source": [
    "En lección entrenaremos una agente capaz de invertir en la bolsa. Para desarrollar este proyecto, nos hemos basado en la lección 10 del libro [Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67).\n",
    "\n",
    "El repositorio del libro puede encontrarlo en [Github](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On). Allí encuentra los datosa y ejemplos. También puede instalar si lo prefiere  la librería [PTAN](https://github.com/Shmuma/ptan) del mismo autor.\n",
    "\n",
    "Paea esta lección hemos extraido algunos módulos de la lección y hemos construido nuestro módulo `aplr`, en el cual hemos colocado las herramientas que usaremos y que describimos en las siguientes secciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df1479-4846-4766-ac3f-fb13c774f0b4",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Los datos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee82d5f9-564c-4085-8465-5d194928d61e",
   "metadata": {},
   "source": [
    "En nuestro ejemplo, utilizaremos los precios del mercado de valores ruso del período 2015-2016. \n",
    "\n",
    "Dentro del archivo, tenemos archivos CSV con barras M1, lo que significa que cada fila\n",
    "en cada archivo CSV corresponde a un solo minuto en el tiempo, y el movimiento del precio\n",
    "durante ese minuto se captura con cuatro precios: apertura, máximo, mínimo y cierre. Aquí,\n",
    "un precio de apertura es el precio al comienzo del minuto, alto es el máximo\n",
    "precio durante el intervalo, bajo es el precio mínimo, y el precio de cierre es el\n",
    "último precio del intervalo de tiempo minuto. \n",
    "\n",
    "Cada intervalo de un minuto se llama barra y nos permite tener una idea del movimiento del precio dentro del intervalo. por ejemplo, en el archivo YNDX_160101_161231.csv (que tiene acciones de la empresa Yandex para 2016), tenemos 130k líneas en este formulario:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3dbb4b30-ba9f-4f03-9f04-eee0597300ca",
   "metadata": {},
   "source": [
    "<DATE>,<TIME>,<OPEN>,<HIGH>,<LOW>,<CLOSE>,<VOL>\n",
    "20160104,100100,1148.9,1148.9,1148.9,1148.9,0\n",
    "20160104,100200,1148.9,1148.9,1148.9,1148.9,50\n",
    "20160104,100300,1149.0,1149.0,1149.0,1149.0,33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716d3ade-f0c3-4b38-b857-fff80f0ad37d",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Gráfico de velas</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b4c33e-111e-476b-9c47-4cb96bef4f22",
   "metadata": {},
   "source": [
    "La forma típica de representar esos precios se llama gráfico de velas japonesas, donde cada\n",
    "la barra se muestra como una vela. Parte de las cotizaciones de Yandex para un día en febrero de 2016 se muestra en el siguiente cuadro. La carpeta de datos contiene dos archivos con datos M1 para 2016 y 2015. Usaremos datos de 2016 para entrenamiento de modelos y datos de 2015 para\n",
    "validación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb55ae-0b63-4818-b5d5-9e9a5808ec44",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center><img src=\"../Imagenes/candelabro.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Gráfico de velas</p>\n",
    "</figcaption>\n",
    "</center> \n",
    "</figure>\n",
    "\n",
    "Fuente: [Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020, pag 261](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e3e16-ac6d-4457-b8e2-5968e8d2edbb",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Descripción del problema y primeras decisiones</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1f5f58-0dd7-48d9-b722-7d66becf1143",
   "metadata": {},
   "source": [
    "Implementaremos el agente para trading básico básico en su forma más simple. La observación incluirá la siguiente información:\n",
    "\n",
    "* N barras pasadas, donde cada una tiene precios de apertura, máximo, mínimo y cierre\n",
    "* Una indicación de que la acción fue comprada hace algún tiempo (solo una acción en un tiempo será posible)\n",
    "* Ganancia o pérdida que tenemos actualmente de nuestra posición actual (la participación comprada)\n",
    "\n",
    "En cada paso, después de cada barra de minutos, el agente puede tomar una de las siguientes\n",
    "comportamiento:\n",
    "\n",
    "* **No hacer nada**: saltar la barra sin realizar ninguna acción.\n",
    "* **Comprar una acción**: si el agente ya tiene la acción, no se comprará nada; de lo contrario, pagaremos la comisión, que suele ser una pequeña porcentaje del precio actual.\n",
    "* **Cerrar la posición**: si no tenemos una acción comprada  previamente, nada pasará; de lo contrario, pagaremos la comisión por la transacción\n",
    "\n",
    "\n",
    "La `recompensa` que recibe el agente se puede expresar de varias formas. Por una lado, podemos dividir la recompensa en varios pasos durante nuestra propiedad de la acción. En ese caso, la recompensa en cada paso será igual al movimiento de la última barra. \n",
    "\n",
    "Por el otro lado, el agente recibirá la recompensa solo después de cerrar la acción y\n",
    "recibir la recompensa completa a la vez. A primera vista, ambas variantes deberían tener el mismo resultado final, pero tal vez con diferentes velocidades de convergencia. Sin embargo, en la práctica, la diferencia podría ser drástica.\n",
    "\n",
    "Una última decisión a tomar es cómo representar los precios en las observaciones del ambiente. Idealmente, nos gustaría que nuestro agente fuera independiente del precio real\n",
    "valores y tener en cuenta el movimiento relativo, como \"la acción ha crecido un 1%\n",
    "durante la última barra\" o \"la acción ha perdido un 5%\". Esto tiene sentido, ya que\n",
    "los precios pueden variar, pero pueden tener patrones de movimiento similares.\n",
    "\n",
    "Para lograr esto, convertiremos los valores de apertura, alto, bajo y y precios de cierre a tres números que muestran precios máximos, mínimos y de cierre representados como un porcentaje del precio de apertura.\n",
    "\n",
    "Esta decisión tiene algunos inconvenientes prácticos como el hecho que a veces los precios se quedan pegados por un tiempo al llegar a cifras redondas, como  $500.000,oo, lo cual no podría detectar el agente. Pero por otro lado esta es una decisión muy conveniente para facilitarle el trabajo a la red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6dcad4-bda9-42a0-bffa-0ab4de86277f",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Librería aplr</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b6b8ef-4280-436d-997f-7734b1797d55",
   "metadata": {},
   "source": [
    "Escribir módulos en Python es muy sencillo. Para crear un módulo propio, simplemente creamos un nuevo archivo . py con el nombre del módulo, y después lo importamos usando el nombre del archivo Python (sin la extensión `.py`) usando el comando `import`.\n",
    "\n",
    "El primer módulo será su librería. En nuestro caso, creamo al carpeta `aplr`que será nuestra librería.\n",
    "\n",
    "Para construir su propia librería, lo que tiene que hacer es lo siguiente. \n",
    "\n",
    "1. Cree una carpeta en el sitio de su preferencia. Nosotros hemos creado el módulo dentro de la carpeta de Cuadernos de este tutorial, para facilitar el acceso directo a la librería. Sin embargo para hacer su deploy, Puede ser necesario crear una acceso en el path de Python, para tener una acceso más genérico. Para los detalles puede cosnultar cualquier tutorial por ejemplo [learnpython.org](https://www.learnpython.org/es/Modules%20and%20Packages#:~:text=Escribiendo%20módulos,py)\n",
    "2. Dentro de la carpeta cree un archivo vacío con el nombre `__init__.py`. \n",
    "3. Coloque los archivos con extensión `py` que conforman parte de su librería. Por ejemplo hemos colocado los archivos `actions.py`, `agent.py`, .... que serán nuestros módulos baśicos.\n",
    "4. Si desea crear un sublibrería, cree la carpeta dentro de la carpeta de su librería principal, agregue allí un archivo vacío con el nombre `__init__.py` y agrege sus archivos `.py`. Por ejemplo, para este tutorial hemos creado la librería  `trading` dentro de `aprl` y allí hemos colocado los archivos que representan los módulos específicos para este tutorial de trainig.\n",
    "\n",
    "Ya estamos listos para importar los objetos que vamos a usar. Primero importamos los módulos y submódulos  y luego los archivos específicos. Por ejemplo:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70b5a172-5aa0-4dd5-ac7d-310dd6af575e",
   "metadata": {
    "tags": []
   },
   "source": [
    "import aprl\n",
    "from aprl import actions\n",
    "\n",
    "from aprl import trading\n",
    "from aprl.trading import environ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07127b-2a41-49ce-9f7b-275dd60da56f",
   "metadata": {},
   "source": [
    "A continuación describimos los módulos que usaremos en este ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3673feb8-e7d1-48c8-8ed5-446748605943",
   "metadata": {},
   "source": [
    "### Módulo actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9f76f-426e-4aa0-9812-64f046a5f954",
   "metadata": {},
   "source": [
    "Este módulo implementa una jerarquía de clases para dar acceso a las diferentes formas de seleccionar acciones que hemos estudiado. Las clases disponibles son\n",
    "\n",
    "- **ActionSelector**: Clase abstracta que convierte puntuaciones (scores) en acciones, para un lote scores.\n",
    "- **ArgmaxActionSelector**: Selecciona acciones usando argmax.\n",
    "- **EpsilonGreedyActionSelector**: Selecciona acciones usando epsilon-gredy.\n",
    "- **ProbabilityActionSelector**: Convierte probabilidades de acciones en acciones al muestrearlas.\n",
    "- **EpsilonTracker**: Actualiza epsilon de acuerdo con un horario lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1982256-9d72-49ba-ae78-d58765825641",
   "metadata": {},
   "source": [
    "### Módulo agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb02623-af20-4860-9168-1aa2c7f61461",
   "metadata": {},
   "source": [
    "Este módulo implementa una jerarquía de clases para dar acceso a las diferentes formas de seleccionar acciones que hemos estudiado. Las clases disponibles son:\n",
    "\n",
    "- **BaseAgent**: Interfaz abstracta de una agente. incluye la creación de un estado inicial del agente y la función __call__ para cuando es llamado. Ambas deben implementarse en clase derivadas.\n",
    "- **DQNAgent**: es un agente DQN sin memoria que calcula valores Q de las observaciones y las convierte en acciones usando action_selector\n",
    "-  **TargetNet**: Envoltura (Wrapper) alrededor del modelo que proporciona una copia en lugar de pesos entrenadas.\n",
    "- **PolicyAgent**: espera que la red produzca una distribución de políticas en un conjunto discreto de acciones La distribución de políticas puede ser logits (no normalizada) o normalizada. distribución. En la práctica, siempre debe usar logits para mejorar la estabilidad numérica  del proceso de entrenamiento.\n",
    "- **ActorCriticAgent** agente para el método actor-crítico que estudiamos en la siguiente lección\n",
    "\n",
    "Los siguientes métodos utilitarios son incluidos en el módulo:\n",
    "\n",
    "- *default_states_preprocessor*:   Convierte la lista de estados en la forma adecuada para el modelo.\n",
    "- *float32_preprocessor*: transforma los valores de los estados en float32\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac3702f-e5a4-4849-988a-527094930b01",
   "metadata": {},
   "source": [
    "### Módulo experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20585619-bbfb-450c-be28-36b782dc2987",
   "metadata": {},
   "source": [
    "La abstracción de agentes descrita en la sección anterior nos permite implementar\n",
    "comunicaciones del entorno de forma genérica. Estas comunicaciones ocurren en la\n",
    "forma de trayectorias, producidas al aplicar las acciones del agente al entorno de Gym.\n",
    "\n",
    "En un alto nivel, las clases de fuente de experiencia toman la instancia y el entorno del agente\n",
    "y proporciona los datos paso a paso de las trayectorias. La funcionalidad\n",
    "de esas clases incluye:\n",
    "\n",
    "* Soporte de múltiples entornos que se comunican al mismo tiempo. Esto permite una utilización eficiente de la GPU a medida que se realiza un lote de observaciones. procesados ​​por el agente a la vez.\n",
    "* Una trayectoria se puede preprocesar y presentar en una forma conveniente para el entrenamiento. Por ejemplo, hay una implementación de lananzamiento de  subtrayectoria con acumulación de la recompensa. \n",
    "* Compatibilidad con entornos vectorizados de OpenAI Universe\n",
    "\n",
    "\n",
    "Hay tres clases proporcionadas por el módulo:\n",
    "\n",
    "* **ExperienceSource**: utilizando el agente y el conjunto de entornos, produce subtrayectorias de n pasos con todos los pasos intermedios\n",
    "* **ExperienceSourceFirstLast**: esto es lo mismo que ExperienceSource, pero\n",
    "* **ExperienceSourceRollouts**: sigue la ventaja asíncrona en lugar de una subtrayectoria completa (con todos los pasos), mantiene solo el primero y el último pasos, con la acumulación adecuada de recompensas en el medio. \n",
    "\n",
    "Todas las clases están escritas para ser eficientes tanto en términos de unidad central de procesamiento\n",
    "(CPU) y la memoria, que no es muy importante para los problemas de juguete, pero podría volverse\n",
    "un problema cuando quiere resolver juegos de Atari y necesitas mantener 10 millones de muestras en el\n",
    "búfer de reproducción utilizando hardware básico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194dc648-e0b0-4482-a1a8-bffa34b29d2e",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Librería trading</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7caef8f-f794-4a01-a57b-b9779ed5a16e",
   "metadata": {},
   "source": [
    "### Módulo data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad90bf-e861-4328-90a7-344500983b75",
   "metadata": {},
   "source": [
    "Este módulo está diseñado para leer un archivo de datos con las siguientes columnas: \n",
    "- `<DATE>,<TIME>,<OPEN>,<HIGH>,<LOW>,<CLOSE>,<VOL>`\n",
    "\n",
    "Los datos son anotados en una tupla nombrada:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc48f32a-3387-4e8a-a1d8-514ac4dd0707",
   "metadata": {},
   "source": [
    "Prices = collections.namedtuple('Prices', field_names=['open', 'high', 'low', 'close', 'volume'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae95a69-5002-4f7b-a5f2-3cf3b0e8ebcb",
   "metadata": {},
   "source": [
    "Se tienen las siguientes funciones\n",
    "\n",
    "- **read_csv**: lee un archivo de datos de tipo csv. Omite registros  cuando la diferencia entre el valor de entrada y los demás valores está pode debajo de 1e-8. Además asegura que el precio de apertura (open) de un registro coincida con el precio de cierre(close) del resgistro anterior.\n",
    "- **prices_to_relative**: Convierte precios a relativos con respecto al precio de apertura: `(precio -precio_open)/precio_open`.\n",
    "- **load_relative**: Lee el archivo y convierte a precios relativos\n",
    "- **price_files**: construye una lista con los archivos de tipo `csv` encontrados en un directorio. Por ejemplo en los datos tenemos archivos separados para cada año: 2015 y 2016.\n",
    "- **load_year_data**: determina el año de los datos a a partir del nombre de los archivos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e0efa-f566-4548-bc6f-564fee1b73a6",
   "metadata": {},
   "source": [
    "### Módulo environ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78454d97-1da0-48b8-b7c8-d0d0fdd99893",
   "metadata": {},
   "source": [
    "En este módulo se implementa un ambiente específico para nuestro problema.\n",
    "La clase más importante del módulo es \n",
    "\n",
    "- **class StocksEnv(gym.Env)**: ESta la clase que implementa el ambiente y es derivada de la clase base gym.Env. El ambientese basa en gym.Env para aprovechar toda la implemtación básica de OpenAI gym.Env. \n",
    "\n",
    "Por su importancia vamos a describir con alg+un detalle la API pública de este módulo.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "159dcae8-ef20-4e41-91e7-ff653463c57b",
   "metadata": {},
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "from gym.utils import seeding\n",
    "from gym.envs.registration import EnvSpec\n",
    "import enum\n",
    "import numpy as np\n",
    "\n",
    "from . import data\n",
    "\n",
    "class Actions(enum.Enum):\n",
    "    Skip = 0\n",
    "    Buy = 1\n",
    "    Close = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab4ead7-f29d-46d0-8fdc-6f2ed2f312f7",
   "metadata": {},
   "source": [
    "Las acciones posibles para el problemea son codificadas como campos de enumeración. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "46126b7e-6643-4ad6-a923-e4fa396f467e",
   "metadata": {},
   "source": [
    "class StocksEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    spec = EnvSpec(\"StocksEnv-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30949e22-64d6-43a0-9938-73e7ad3b4955",
   "metadata": {},
   "source": [
    "Los atributos *metadata* y *spec* son requeridos para compatibilidad con *gym.Env*. No vamos a hacer ningún tipo de renderización, por lo que ignoraremso esto."
   ]
  },
  {
   "cell_type": "raw",
   "id": "82033556-8f9e-4f69-984c-9a010bcfa49e",
   "metadata": {},
   "source": [
    "@classmethod\n",
    "    def from_dir(cls, data_dir, **kwargs):\n",
    "        prices = {\n",
    "        file: data.load_relative(file)\n",
    "            for file in data.price_files(data_dir)\n",
    "}\n",
    "return StocksEnv(prices, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ddd93-d4ac-4c72-9531-e2fdb700c907",
   "metadata": {},
   "source": [
    "Esto método de clase permite tener dos constructores para la clase.  En este caso, por jemplo puede instanciar un objeto de tipo *StocksEnv* entregándole la dirección de la carpeta en donde estan los rachivos de datos. Puede usar este métod así"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9fd1818a-0b77-4628-aae1-af9e206bae2f",
   "metadata": {},
   "source": [
    "stock_env = StocksEnv.from_dir(directorio _datos, otros_parametro_nombreados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef195b9e-deb7-43ad-928b-221b8d3ec8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "El constructor básico de la clase es:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "afce3589-be49-4260-8cc2-06f3cb65f07f",
   "metadata": {},
   "source": [
    "def __init__(self, prices, bars_count=DEFAULT_BARS_COUNT,\n",
    "    commission=DEFAULT_COMMISSION_PERC,\n",
    "    reset_on_close=True, conv_1d=False,\n",
    "    random_ofs_on_reset=True, reward_on_close=False,\n",
    "    volumes=False):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd2616c-0ff9-47b7-921f-1114021bb0f4",
   "metadata": {},
   "source": [
    "* *prices* : Contiene uno o más precios de acciones para uno o más instrumentos como un diccionario, donde las claves son el nombre del instrumento y el valor es un contenedor de objetos `data.Prices` , que contiene matrices de datos de precios.\n",
    "* *bars_count* : El conteo de barras que pasamos en la observación. Por defecto, esto es 10 barras. Revise la gráfica abajo.\n",
    "* *comission* : El porcentaje del precio de la acción que tenemos que pagar al intermediario (broker) en la compra y venta de acciones. Por defecto, es 0.1%.\n",
    "* *reset_on_close*: si este parámetro se establece en *True*, por defecto es así, cada vez que el agente nos pide cerrar la posición existente (es decir,  vender una acción), detenemos el episodio. De lo contrario, el episodio continuará hasta que el final de nuestra serie de tiempo, que es un año de datos.\n",
    "* *conv_1d* : este argumento booleano cambia entre diferentes representaciones  de datos de precios en la observación pasada al agente. Si se establece en *True*, observaciones tienen una forma 2D, con diferentes componentes de precio para barras posteriores organizadas en filas. Por ejemplo, precios altos (high) (precio máximo para la barra) se colocan en la primera fila, los precios bajos (low) en la segunda y  precios de cierre (close) en el tercero. Esta representación es adecuada para hacer convolución 1D en series de tiempo, donde cada fila en los datos tiene el mismo significado que diferentes planos de color (rojo, verde o azul) en imágenes 2D de Atari. si establecemos esta opción a False, tenemos una única matriz de datos con cada barra componentes colocados juntos. Esta organización es conveniente para una completa arquitectura de red conectada. Ambas representaciones se ilustran en figura abajo.\n",
    "* *random_ofs_on_reset*: si el parámetro es *True*, lo es por defecto, en cada reinicio del entorno, se elegirá un desplazamiento aleatorio en la serie temporal. De lo contrario, comenzaremos desde el principio de los datos.\n",
    "* *reward_on_close* : este parámetro booleano cambia entre  dos esquemas de recompensa: luego de cada iteración, o,  al final del episodio. Si se establece en *True* , el agente reciba una recompensa solo con la  acción \"close\". De lo contrario, le daremos un pequeña recompensa cada barra, correspondiente al movimiento del precio durante esa barra.\n",
    "* *volumes* : este argumento activa los volúmenes en las observaciones y es deshabilitado por defecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619f083f-2a44-49b1-b0aa-a3df55afa0df",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center><img src=\"../Imagenes/data_representation_trading.png\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Represntacion de los datos para diferentes arquitecturas de  la red</p>\n",
    "</figcaption>\n",
    "</center> \n",
    "</figure>\n",
    "\n",
    "Fuente: [Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020, pag 265](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81951065-4555-470e-a593-6c7d65c5527f",
   "metadata": {},
   "source": [
    "El cuerpo del constructor continua así:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a827300-1c11-4a35-a8e8-0cc19531281d",
   "metadata": {},
   "source": [
    "    assert isinstance(prices, dict)\n",
    "    self._prices = prices\n",
    "    if conv_1d:\n",
    "        self._state = State1D(\n",
    "        bars_count, commission, reset_on_close,\n",
    "        reward_on_close=reward_on_close, volumes=volumes)\n",
    "    else:\n",
    "        self._state = State(\n",
    "        bars_count, commission, reset_on_close,\n",
    "        reward_on_close=reward_on_close, volumes=volumes)\n",
    "    self.action_space = gym.spaces.Discrete(n=len(Actions))\n",
    "    self.observation_space = gym.spaces.Box(\n",
    "        low=-np.inf, high=np.inf,\n",
    "        shape=self._state.shape, dtype=np.float32)\n",
    "    self.random_ofs_on_reset = random_ofs_on_reset\n",
    "    self.seed()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff661da5-918e-42ff-bee8-91d8a167d24c",
   "metadata": {},
   "source": [
    "La mayor parte de la funcionalidad de la clase `StocksEnv` está implementada en las clases \n",
    "\n",
    "- **State**: prepara los datos como una lista de barras (high, low, close, volume). Observe la imagen arriba.\n",
    "- **State1D**: Prepara los datos en forma de matriz. Como se ve en la imagen arriba.\n",
    "\n",
    "El constructor crea los datos de acuerdo con el parámetro *conv_1d*. Adicionalmente, el constructor crea crea el espacio de acciones y el de observaciones de acuerdo con la API de Gym.\n",
    "\n",
    "El resto de la API es como sigue:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4879df4d-012f-4903-8123-273bee410cf3",
   "metadata": {},
   "source": [
    "    def reset(self):\n",
    "        self._instrument = self.np_random.choice(\n",
    "            list(self._prices.keys()))\n",
    "        prices = self._prices[self._instrument]\n",
    "        bars = self._state.bars_count\n",
    "        if self.random_ofs_on_reset:\n",
    "            offset = self.np_random.choice(\n",
    "                prices.high.shape[0]-bars*10) + bars\n",
    "        else:\n",
    "            offset = bars\n",
    "        self._state.reset(prices, offset)\n",
    "        return self._state.encode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83973096-cbd8-4af1-9734-cb997fa6354f",
   "metadata": {},
   "source": [
    "Este método define la funcionalidad *reset()* para nuestro entorno. De acuerdo a\n",
    "a la semántica gym.Env, cambiamos aleatoriamente la serie de tiempo que trabajaremos\n",
    "y seleccionamos el desplazamiento inicial en esta serie temporal. El precio seleccionado y el desplazamiento se pasan a la instancia de estado interno, que luego solicita una observación inicial usando su función *encode()*."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c70314d-9762-483a-905e-7c85bfbbeb32",
   "metadata": {},
   "source": [
    "    def step(self, action_idx):\n",
    "        action = Actions(action_idx)\n",
    "        reward, done = self._state.step(action)\n",
    "        obs = self._state.encode()\n",
    "        info = {\n",
    "            \"instrument\": self._instrument,\n",
    "            \"offset\": self._state._offset\n",
    "        }\n",
    "        return obs, reward, done, info "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7345a4d-8bf5-48ad-9366-12f1725217c7",
   "metadata": {},
   "source": [
    "Este método tiene que manejar la acción elegida por el agente y devolver la siguiente observación, recompensa y marca de hecho. Toda la funcionalidad real está implementada en nuestro clases de estado, por lo que este método es un contenedor (wrapper) muy simple alrededor de la llamada al los métodos de estado de Gym."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d30dfb82-0509-4a5e-9ed2-5741b38441e6",
   "metadata": {},
   "source": [
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77323908-fb7c-4f31-b4b4-e415fb656f03",
   "metadata": {},
   "source": [
    "La API de Gym permite definir el método *render()* para visualizar el proceso. El método *close()* es llamado por el ambiente para destruir todos los objetos libres. No haremos ninguna implementación de estos métodos de Gym."
   ]
  },
  {
   "cell_type": "raw",
   "id": "68d99179-df9d-4778-a97c-0acb87cf10d3",
   "metadata": {},
   "source": [
    "def seed(self, seed=None):\n",
    "    self.np_random, seed1 = seeding.np_random(seed)\n",
    "    seed2 = seeding.hash_seed(seed1 + 1) % 2 ** 31\n",
    "    return [seed1, seed2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2944044b-e667-4104-8da6-5f3006e5b4c6",
   "metadata": {},
   "source": [
    "Este método permite corregir problemas con el generador de números pseudo-aleatorios de Python, para las situaciones de múltiples ambientes simultáneos. No se usará aquí.\n",
    "\n",
    "A continuación vamos a revisar brevemente la clase  *State*. La clase *State* es similar con los cambios correspondientes al manejo apropiado de los datos.\n",
    "\n",
    "Esta clases son las encargadas del trabajo fuerte del ambiente. comencemos con el constructor."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ecf29d1-c86d-482a-bce3-cfdc839948dc",
   "metadata": {},
   "source": [
    "class State:\n",
    "    def __init__(self, bars_count, commission_perc,\n",
    "        reset_on_close, reward_on_close=True,\n",
    "        volumes=True):\n",
    "    assert isinstance(bars_count, int)\n",
    "    assert bars_count > 0\n",
    "    assert isinstance(commission_perc, float)\n",
    "    assert commission_perc >= 0.0\n",
    "    assert isinstance(reset_on_close, bool)\n",
    "    assert isinstance(reward_on_close, bool)\n",
    "    self.bars_count = bars_count\n",
    "    self.commission_perc = commission_perc\n",
    "    self.reset_on_close = reset_on_close\n",
    "    self.reward_on_close = reward_on_close\n",
    "    self.volumes = volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f297a2e1-2cdb-494d-bddf-c59d055002bf",
   "metadata": {},
   "source": [
    "Simplemente verifica los tipos de datos y los anota para recordarlos localmente."
   ]
  },
  {
   "cell_type": "raw",
   "id": "64372710-3edf-4438-b862-0cea7db4263e",
   "metadata": {},
   "source": [
    "    def reset(self, prices, offset):\n",
    "        assert isinstance(prices, data.Prices)\n",
    "        assert offset >= self.bars_count-1\n",
    "        self.have_position = False\n",
    "        self.open_price = 0.0\n",
    "        self._prices = prices\n",
    "        self._offset = offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d444d8e-3200-48d2-858e-5c5d9b8a4818",
   "metadata": {},
   "source": [
    "El método *reset()* se llama cada vez que se le pide al entorno que se reinicie y tiene que guardar los datos de precios pasados  y la sl desplamiento inicial para empezar a leer datos. Al principio,  no se  tiene acciones compradas, por lo que se tiene *have_position=False* y *open_\n",
    "precio=0.0*."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b0f632f-3833-417f-8b10-8043bc1e9eb5",
   "metadata": {},
   "source": [
    "    @property\n",
    "        def shape(self):\n",
    "        # [h, l, c] * bars + position_flag + rel_profit\n",
    "        if self.volumes:\n",
    "            return 4 * self.bars_count + 1 + 1,\n",
    "        else:\n",
    "            return 3 * self.bars_count + 1 + 1,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670b1004-365a-4765-9406-55c077d43593",
   "metadata": {},
   "source": [
    "Esta propiedad retorna el tamaño de cada dato de *State* en un arreglo de Numpy. \n",
    "\n",
    "El siguiente método es el que hace la codificación (encode) de la observación para este caso un vector simple que incluye precios con opción volúmenes y dos números que indican la presencia de un activo comprado y ganancia por la  posición \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "960b0380-0a13-4f92-b8de-45d410173fb4",
   "metadata": {},
   "source": [
    "    def encode(self):\n",
    "        res = np.ndarray(shape=self.shape, dtype=np.float32)\n",
    "        shift = 0\n",
    "        for bar_idx in range(-self.bars_count+1, 1):\n",
    "            ofs = self._offset + bar_idx\n",
    "            res[shift] = self._prices.high[ofs]\n",
    "            shift += 1\n",
    "            res[shift] = self._prices.low[ofs]\n",
    "            shift += 1\n",
    "            res[shift] = self._prices.close[ofs]\n",
    "            shift += 1\n",
    "            if self.volumes:\n",
    "            res[shift] = self._prices.volume[ofs]\n",
    "            shift += 1\n",
    "        res[shift] = float(self.have_position)\n",
    "        shift += 1\n",
    "        if not self.have_position:\n",
    "            res[shift] = 0.0\n",
    "        else:\n",
    "            res[shift] = self._cur_close() / self.open_price - 1.0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a9c95b-4127-4c74-a076-915ee3d23071",
   "metadata": {},
   "source": [
    "El método *_cur_close()* entrega el precio de cierre de la barra actual. Precios pasados a la clase *State* tienen la forma relativa con respecto al precio de apertura: el máximo, mínimo y cierre. Los componentes son proporciones relativas al precio de apertura. Esta representación ya se había discutido cuando hablamos sobre los datos de entrenamiento, y (probablemente) ayudará a nuestro agente para aprender patrones de precios que son independientes del valor del precio real."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c0b4496-4286-4e4e-aeed-d15f81a5c18f",
   "metadata": {},
   "source": [
    "    def _cur_close(self):\n",
    "        open = self._prices.open[self._offset]\n",
    "        rel_close = self._prices.close[self._offset]\n",
    "        return open * (1.0 + rel_close)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3056840b-95c3-4c9a-9f29-d1d628d2b5d4",
   "metadata": {},
   "source": [
    "Revisamos ahora, la pieza de código más complicada de la clase *State*. El método *step* el cual esresponsable de realizar un paso en nuestro entorno. A la salida, tiene que devolver la\n",
    "recompensa en un porcentaje y una indicación del final del episodio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641548c9-aca9-4c4a-9f29-cb156baecb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def step(self, action):\n",
    "        assert isinstance(action, Actions)\n",
    "        reward = 0.0\n",
    "        done = False\n",
    "        close = self._cur_close()\n",
    "        if action == Actions.Buy and not self.have_position:\n",
    "            self.have_position = True\n",
    "            self.open_price = close\n",
    "            reward -= self.commission_perc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5ce2aa-5668-4581-8a05-8cfa4919b633",
   "metadata": {},
   "source": [
    "Si el agente ha decidido comprar una acción, cambiamos nuestro estado y pagamos la comisión.\n",
    "En nuestro estado, asumimos la ejecución instantánea de la orden al precio de cierre de la barra actual,\n",
    "lo cual es una simplificación por nuestra parte; normalmente, una orden se puede ejecutar en un\n",
    "precio diferente, lo que se denomina deslizamiento de precios (price slippage)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a99d969c-583f-4d99-87b9-f2c0486f2377",
   "metadata": {},
   "source": [
    "        elif action == Actions.Close and self.have_position:\n",
    "            reward -= self.commission_perc\n",
    "            done |= self.reset_on_close\n",
    "        if self.reward_on_close:\n",
    "            reward += 100.0 * (close / self.open_price - 1.0)\n",
    "        self.have_position = False\n",
    "        self.open_price = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a37d78f-02b2-4903-a283-33a129a9a23d",
   "metadata": {},
   "source": [
    "Si tenemos una posición y el agente nos pide que la cerremos, volvemos a pagar comisión,\n",
    "cambiamos la bandera done  si estamos en modo *reset_on_close*, y se otorga una recompensa final por toda la posición, y cambia nuestro estado.\n",
    "\n",
    "En el resto de la función, modificamos el desplazamiento actual y damos la recompensa por el\n",
    "movimiento de la última barra. Eso es todo para la clas *State*. Omitimos la calse *State1D* por ser similar."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2291581-b7af-4a47-8dd5-0cc88884fca5",
   "metadata": {},
   "source": [
    "        self._offset += 1\n",
    "        prev_close = close\n",
    "        close = self._cur_close()\n",
    "        done |= self._offset >= self._prices.close.shape[0]-1\n",
    "        if self.have_position and not self.reward_on_close:\n",
    "            reward += 100.0 * (close / prev_close - 1.0)\n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cff06e-77e4-486b-a18f-97ce2038447c",
   "metadata": {},
   "source": [
    "### Módulo common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eb585f-ea9f-4e79-97f2-f545e08f3744",
   "metadata": {},
   "source": [
    "Este módulo contiene  la siguiente clase utilitaria.\n",
    "\n",
    "- **RewardTracker**: Esta es una envoltura que calcula la recompensa completa para los últimos 100 episodios  y además indica cuando la recompensa promedio excede un umbral deseado.\n",
    "\n",
    "Además incluye las funciones utilitarias\n",
    "\n",
    "- *calc_values_of_states*: calcula el valor de los estados.\n",
    "- *unpack_batch*: desempaqueta los objetos en los lotes (batches de datos).\n",
    "- *calc_loss: Calcula la funcuón de pérdida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16046696-44aa-430d-bc64-2c2994864035",
   "metadata": {},
   "source": [
    "### Módulo validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0756c0-df32-4f84-abef-b76f8141219b",
   "metadata": {},
   "source": [
    "Este módulo solamente contiene la función\n",
    "\n",
    "- *validation_run*: corre un validación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431cd159-ee36-4dae-a6d5-4957e993e188",
   "metadata": {},
   "source": [
    "### Módulo models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf4069-5693-4e74-8c09-db16dca1beac",
   "metadata": {},
   "source": [
    "Contiene los modelos de redes neuronales presentados abajo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ee89f4-ca22-4fc5-ab76-75394139acff",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Modelos-Dueling</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b443d4-28c7-4a96-86f5-ba1656f469b1",
   "metadata": {},
   "source": [
    "El modelo Dueling es una mejora para DQN fue propuesta en 2015, en el documento llamado Wang et al., 2015, Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/pdf/1511.06581.pdf). \n",
    "\n",
    "La observación central de este artículo es que los valores Q, Q(s, a), que nuestra red están intentando\n",
    "para aproximar, se puede dividir en dos cantidades: el valor del estado, V(s), y el\n",
    "ventaja de las acciones en este estado, A(s, a).\n",
    "Hemos visto la cantidad V(s) antes, ya que era el núcleo de la iteración de valor.  Esta cantidad es es igual a la recompensa esperada descontada que se puede obtener de este estado. \n",
    "\n",
    "La ventaja A(s, a) se supone que cierra la brecha de A(s) a Q(s, a), ya que, por definición, Q(s, a) = V(s) +\n",
    "A(s, a). En otras palabras, la ventaja A(s, a) es solo el delta, que indica cuánto extra\n",
    "recompensa alguna acción particular del estado nos trae.\n",
    "\n",
    "\n",
    "\n",
    "Un problema por resolver es que la suma puede ser hecha don muy diversos valores. Los estadísticos dicen que se tiene un problema de identificabilidad, porque hay infinitas soluciones. Es necesario establer una restricción que lleve auna única solución. \n",
    "\n",
    "Esta restricción podría aplicarse de varias maneras, por ejemplo, a través de la pérdida\n",
    "función; pero en el artículo Dueling, los autores propusieron una muy elegante\n",
    "solución de restar el valor medio de la ventaja de la expresión Q\n",
    "en la red, lo que efectivamente lleva la media de la ventaja a cero:\n",
    "\n",
    "$$\n",
    "Q(s,a) = V(s) + A(s,a) - \\frac{1}{N}\\sum_k A(s,k).\n",
    "$$\n",
    "\n",
    "En los modelos de esta lección introducimos este modelo, lo cual es bastante simple implementado dos módulos dentro de la red, como se muestra en la siguiente imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57e665-45f0-4842-9397-1b5bf3c6afb1",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center><img src=\"../Imagenes/dueling.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Modelo Dueling</p>\n",
    "</figcaption>\n",
    "</center> \n",
    "</figure>\n",
    "\n",
    "Fuente: [Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020, pag 261](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1059b9-2378-4505-b15c-7e2d3e409fa1",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Implementación de Modelos</span>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ff923ab-62f4-4f22-822d-2c5167ff091f",
   "metadata": {},
   "source": [
    "\n",
    "class SimpleFFDQN(nn.Module):\n",
    "    def __init__(self, obs_len, actions_n):\n",
    "        super(SimpleFFDQN, self).__init__()\n",
    "\n",
    "        self.fc_val = nn.Sequential(\n",
    "            nn.Linear(obs_len, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "        self.fc_adv = nn.Sequential(\n",
    "            nn.Linear(obs_len, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, actions_n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        val = self.fc_val(x)\n",
    "        adv = self.fc_adv(x)\n",
    "        return val + adv - adv.mean(dim=1, keepdim=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5862c8f2-de17-4451-9ded-a0bf59acfb32",
   "metadata": {},
   "source": [
    "\n",
    "class DQNConv1D(nn.Module):\n",
    "    def __init__(self, shape, actions_n):\n",
    "        super(DQNConv1D, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(shape[0], 128, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, 5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        out_size = self._get_conv_out(shape)\n",
    "\n",
    "        self.fc_val = nn.Sequential(\n",
    "            nn.Linear(out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "        self.fc_adv = nn.Sequential(\n",
    "            nn.Linear(out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, actions_n)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        val = self.fc_val(conv_out)\n",
    "        adv = self.fc_adv(conv_out)\n",
    "        return val + adv - adv.mean(dim=1, keepdim=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c107fde-7933-4f62-8c93-bbdd9b101b28",
   "metadata": {},
   "source": [
    "class DQNConv1DLarge(nn.Module):\n",
    "    def __init__(self, shape, actions_n):\n",
    "        super(DQNConv1DLarge, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(shape[0], 32, 3),\n",
    "            nn.MaxPool1d(3, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 3),\n",
    "            nn.MaxPool1d(3, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 3),\n",
    "            nn.MaxPool1d(3, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 3),\n",
    "            nn.MaxPool1d(3, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        out_size = self._get_conv_out(shape)\n",
    "\n",
    "        self.fc_val = nn.Sequential(\n",
    "            nn.Linear(out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "        self.fc_adv = nn.Sequential(\n",
    "            nn.Linear(out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, actions_n)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        val = self.fc_val(conv_out)\n",
    "        adv = self.fc_adv(conv_out)\n",
    "        return val + adv - adv.mean(dim=1, keepdim=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f4c87c-5710-4340-8708-cfd1af51abe6",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Entrenamiento del modelo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed657e-6c48-49e0-b9b8-093cca4d88f9",
   "metadata": {},
   "source": [
    "En el entrenamiento usamos una envoltura (wrapper): `RewardTraker`. Esta envoltura calcula la recompensa completa para los últimos 100 episodios  y además indica cuando la recompensa promedio excede un umbral deseado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "133b5d11-c2e1-49e1-ad3d-deb8cadb7194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "#import ptan\n",
    "#import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import aprl\n",
    "from aprl import trading\n",
    "\n",
    "from aprl.trading import environ, data, models, common, validation\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0379bf3e-ba67-40e2-a4a1-36eaaa130438",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BARS_COUNT = 10\n",
    "TARGET_NET_SYNC = 1000\n",
    "DEFAULT_STOCKS = \"data/YNDX_160101_161231.csv\"\n",
    "DEFAULT_VAL_STOCKS = \"data/YNDX_150101_151231.csv\"\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "REPLAY_SIZE = 100000\n",
    "REPLAY_INITIAL = 10000\n",
    "\n",
    "REWARD_STEPS = 2\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "STATES_TO_EVALUATE = 1000\n",
    "EVAL_EVERY_STEP = 1000\n",
    "\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_STOP = 0.1\n",
    "EPSILON_STEPS = 1000000\n",
    "\n",
    "CHECKPOINT_EVERY_STEP = 1000000\n",
    "VALIDATION_EVERY_STEP = 100000\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n",
    "    parser.add_argument(\"--data\", default=DEFAULT_STOCKS, help=\"Stocks file or dir to train on, default=\" + DEFAULT_STOCKS)\n",
    "    parser.add_argument(\"--year\", type=int, help=\"Year to be used for training, if specified, overrides --data option\")\n",
    "    parser.add_argument(\"--valdata\", default=DEFAULT_VAL_STOCKS, help=\"Stocks data for validation, default=\" + DEFAULT_VAL_STOCKS)\n",
    "    parser.add_argument(\"-r\", \"--run\", required=True, help=\"Run name\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    saves_path = os.path.join(\"saves\", args.run)\n",
    "    os.makedirs(saves_path, exist_ok=True)\n",
    "\n",
    "    if args.year is not None or os.path.isfile(args.data):\n",
    "        if args.year is not None:\n",
    "            stock_data = data.load_year_data(args.year)\n",
    "        else:\n",
    "            stock_data = {\"YNDX\": data.load_relative(args.data)}\n",
    "        env = environ.StocksEnv(stock_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False, volumes=False)\n",
    "        env_tst = environ.StocksEnv(stock_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
    "    elif os.path.isdir(args.data):\n",
    "        env = environ.StocksEnv.from_dir(args.data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
    "        env_tst = environ.StocksEnv.from_dir(args.data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
    "    else:\n",
    "        raise RuntimeError(\"No data to train on\")\n",
    "    env = gym.wrappers.TimeLimit(env, max_episode_steps=1000)\n",
    "\n",
    "    val_data = {\"YNDX\": data.load_relative(args.valdata)}\n",
    "    env_val = environ.StocksEnv(val_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
    "\n",
    "    writer = SummaryWriter(comment=\"-simple-\" + args.run)\n",
    "    net = models.SimpleFFDQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "    tgt_net = agent.TargetNet(net)\n",
    "    selector = actions.EpsilonGreedyActionSelector(EPSILON_START)\n",
    "    agent = agent.DQNAgent(net, selector, device=device)\n",
    "    exp_source = .experience.ExperienceSourceFirstLast(env, agent, GAMMA, steps_count=REWARD_STEPS)\n",
    "    buffer = experience.ExperienceReplayBuffer(exp_source, REPLAY_SIZE)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # main training loop\n",
    "    step_idx = 0\n",
    "    eval_states = None\n",
    "    best_mean_val = None\n",
    "\n",
    "    with common.RewardTracker(writer, np.inf, group_rewards=100) as reward_tracker:\n",
    "        while True:\n",
    "            step_idx += 1\n",
    "            buffer.populate(1)\n",
    "            selector.epsilon = max(EPSILON_STOP, EPSILON_START - step_idx / EPSILON_STEPS)\n",
    "\n",
    "            new_rewards = exp_source.pop_rewards_steps()\n",
    "            if new_rewards:\n",
    "                reward_tracker.reward(new_rewards[0], step_idx, selector.epsilon)\n",
    "\n",
    "            if len(buffer) < REPLAY_INITIAL:\n",
    "                continue\n",
    "\n",
    "            if eval_states is None:\n",
    "                print(\"Initial buffer populated, start training\")\n",
    "                eval_states = buffer.sample(STATES_TO_EVALUATE)\n",
    "                eval_states = [np.array(transition.state, copy=False) for transition in eval_states]\n",
    "                eval_states = np.array(eval_states, copy=False)\n",
    "\n",
    "            if step_idx % EVAL_EVERY_STEP == 0:\n",
    "                mean_val = common.calc_values_of_states(eval_states, net, device=device)\n",
    "                writer.add_scalar(\"values_mean\", mean_val, step_idx)\n",
    "                if best_mean_val is None or best_mean_val < mean_val:\n",
    "                    if best_mean_val is not None:\n",
    "                        print(\"%d: Best mean value updated %.3f -> %.3f\" % (step_idx, best_mean_val, mean_val))\n",
    "                    best_mean_val = mean_val\n",
    "                    torch.save(net.state_dict(), os.path.join(saves_path, \"mean_val-%.3f.data\" % mean_val))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch = buffer.sample(BATCH_SIZE)\n",
    "            loss_v = common.calc_loss(batch, net, tgt_net.target_model, GAMMA ** REWARD_STEPS, device=device)\n",
    "            loss_v.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step_idx % TARGET_NET_SYNC == 0:\n",
    "                tgt_net.sync()\n",
    "\n",
    "            if step_idx % CHECKPOINT_EVERY_STEP == 0:\n",
    "                idx = step_idx // CHECKPOINT_EVERY_STEP\n",
    "                torch.save(net.state_dict(), os.path.join(saves_path, \"checkpoint-%3d.data\" % idx))\n",
    "\n",
    "            if step_idx % VALIDATION_EVERY_STEP == 0:\n",
    "                res = validation.validation_run(env_tst, net, device=device)\n",
    "                for key, val in res.items():\n",
    "                    writer.add_scalar(key + \"_test\", val, step_idx)\n",
    "                res = validation.validation_run(env_val, net, device=device)\n",
    "                for key, val in res.items():\n",
    "                    writer.add_scalar(key + \"_val\", val, step_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
