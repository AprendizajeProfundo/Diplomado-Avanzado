{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Q-learning</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Ecuación de Belman</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Oleg Jarma, ojarmam@unal.edu.co \n",
    "6. Laura Lizarazo, ljlizarazore@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Richard S. Sutton, Andrew G. Barto, Reinforcement learning: an introduction, 2nd edition, 2020](http://library.lol/main/6502B74CE247C4CD4D4FB54747AD7C7E)\n",
    "1. [Praveen Palanisamy - Hands-On Intelligent Agents with OpenAI Gym_ Your Guide to Developing AI Agents Using Deep Reinforcement Learning, 2020](http://library.lol/main/E4FD128CF9B93E0F7A542B053330517A)\n",
    "1.[Markel Sanz, Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-2-q-learning-883cd42fb48e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Función de valor](#Función-de-valor)\n",
    "* [Q-Learning](#Q-Learning)\n",
    "* [Ecuación de Bellman](#Ecuación-de-Bellman)\n",
    "* [Valor de la acción](#Valor-de-la-acción)\n",
    "* [Algortimo Q-learning determinístico](Algortimo-Q-learning-determinístico)\n",
    "* [Método de iteración de valores en práctica](#Método-de-iteración-de-valores-en-práctica)\n",
    "* [Método de iteración de acciones en práctica](#Método-de-iteración-de-acciones-en-práctica)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la lección del bandido multibrazo, hemos descrito el problema del bandido multibrazo, y hemos introducido varios conceptos, como el estado, la acción, la recompensa, etc. Sin embargo, el problema del bandido multibrazo no representa el problema completo del aprendizaje reforzado. En los problemas de bandidos multibrazo, cada acción es completamente independiente de las anteriores, y el estado siempre es el mismo, como en el ejemplo de la lección del bandido multibrazo, donde siempre teníamos los 5 mismos brazos y su probabilidad de éxito no cambiaba en ningún momento.\n",
    "\n",
    "\n",
    "En el problema completo de aprendizaje por refuerzo, el estado cambia cada vez que ejecutamos una acción. Podemos representar el problema general de aprendizaje reforzado de la siguiente manera. \n",
    "\n",
    "1. El **agente** recibe la siguiente observación(que llamaremos estados en esta lección) del  **ambiente** (environment). los estados se denotaran por $s_i$\n",
    "2. El agente ejecuta entonces la **acción** que elija y le informa ambiente. Las acciones de denotaran $a_i$\n",
    "3. Al ejecutar esa acción, el ambiente responde proporcionando una **recompensa** y una nueva observación que llamaremos estado en esta lección. La recompensas se denotarán $r_i$.\n",
    "\n",
    "Este ciclo se puede observar en la siguiente imagen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<img src=\"../Imagenes/environment.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "</figure> \n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo tanto, **la acción que el agente escoja no sólo depende de la recompensa  que vaya a recibir a corto plazo. Debe elegir las acciones que a largo plazo le traerán la máxima recompensa (o retorno) posible en todo el episodio (episode)**. \n",
    "\n",
    "Este ciclo trae una secuencia de estados, acciones y recompensas, desde el primer paso del ciclo hasta el último: \n",
    "\n",
    "$$\n",
    "s_1, a_1, r_1; s_2, a_2, r_2; \\ldots; s_T, a_T, r_T. \n",
    "$$\n",
    "\n",
    "Aquí, $T$ indica el fin del episodio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Función de valor</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cuantificar cuanta recompensa obtendrá el agente a largo plazo desde cada estado, introducimos la función de valor $V(s)$. Esta función produce una estimación de la recompensa que obtendrá el agente hasta el final del episodio, empezando desde el estado s. Si conseguimos estimar este valor correctamente, podremos decidir ejecutar la acción que nos lleve al estado con el valor más alto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Q-Learning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver el problema del aprendizaje reforzado, el agente debe aprender a escoger la mejor acción posible para cada uno de los estados posibles. Para ello, con el algoritmo **Q-Learning** el agente intenta aprender cuanta recompensa obtendrá a largo plazo para cada pareja de estados y acciones $(s,a)$. \n",
    "\n",
    "A esa función se la llama la **función de acción-valor** (action-value function) y este algoritmo la representa como la función $Q(s,a)$, la cual devuelve la recompensa que el agente recibirá al ejecutar la acción a desde el estado $s$, y asumiendo que seguirá la misma política dictada por la función $Q$ hasta el final del episodio. \n",
    "\n",
    "\n",
    "Por lo tanto, si desde el estado $s$, tenemos dos acciones disponibles, $a_1$ y $a_2$, la función $Q$ nos proporcionará los **valores-Q** (Q-values) de cada una de las acciones. Por ejemplo, si $Q(s,a_1)=1$ y $Q(s,a_2)=4$, el agente sabe que la acción $a_2$ es mejor y le traerá mayor recompensa, por lo que será la acción que ejecutará, una vez haya sido entrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Ecuación de Bellman</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos del capítulo de [introducción al aprendizaje reforzado](ar_Aprendizaje_Reforzado_Intro.ipynb)  que definimos el  `valor del estado` *s* como\n",
    "\n",
    "$$\n",
    "V(s) = E\\left[\\sum_{t=0}^{\\infty}r_t\\gamma^t \\right],\n",
    "$$\n",
    "\n",
    "en donde $r_t$ es la recompensa local obtenida en el paso $t$ del episodio y $\\gamma$ el factor de descuento. El valor es calculado  de acuerdo con una política que el agente sigue en el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/dos_estados.png\" width=\"300\" height=\"300\" align=\"center\"/>\n",
    "</figure>\n",
    "\n",
    "Fuente Maxim Lapan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos el siguiente ejemplo simple. El ambiente tiene tres estados:\n",
    "\n",
    "1. El estado inicial del agente.\n",
    "1. El estado final que se encuentra al ejecutar la acción *derecha* desde el estado inicial. La recompensa recibida al alcanzar este estado es 1.\n",
    "1. El estado final que se encuentra al ejecutar la acción *abajo* desde el estado inicial. La recompensa recibida al alcanzar este estado es 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo el ambiente es determinístico: Toda acción sucede y siempre empieza en el estado 1. Una vez se alcanza el estado 2 o el estado 3 el episodio termina. La pregunta es ¿Cuál es el valor del estado 1?.\n",
    "\n",
    "Esta pregunta carece de significado sin la información acerca del comportamiento (política) del agente. Aún en este ejemplo tan simple, el agente puede tener una gran cantidad de comportamientos. Por ejemplo:\n",
    "\n",
    "* El agente siempre va  la derecha.\n",
    "* El agente siempre va a abajo.\n",
    "* El agente va a la derecha o baja con probabilidad 50\\% en cada caso.\n",
    "* El agente va a la derecha con probabilidad 10\\% y baja con probabilidad 90\\%.\n",
    "\n",
    "El valor del estado 1 en cada caso es\n",
    "\n",
    "* siempre a la derecha: 1.\n",
    "* Siempre baja: 2.0\n",
    "* 50\\% y 50\\%: 1.0\\*0.5 +2.0\\*0.5 = 1.5.\n",
    "* Para el último caso, el valor del estado 1 es: 1.0\\*0.1 + 2.0\\*0.9 = 1.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuál es la política optimal para este agente?\n",
    "\n",
    "Como el propósito de los agentes en el AR es obtener la máxima recompensa, en este caso, la política optimal es siempre bajar. Sin embargo un caso tan simple como este, no es de interés práctico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ejemplo da la impresión de que el agente debe tomar siempre la siguiente acción que le entrega mayor recompensa inmediata. El asunto no es tan sencillo.\n",
    "\n",
    "Veamos el siguiente ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/cuatro_estados.png\" width=\"300\" height=\"300\" align=\"center\"/>\n",
    "</figure>\n",
    "\n",
    "Fuente: Maxim Lapan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se observa, siempre empezando en el estado 1, el siguiente estado que ofrece mayor recompensa es el estados 3, con recompensa=2. Pero  el agente solamente puede continuar para terminar el episodio hacia el estado en donde recibe recompensa= -20. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Por lo que que final del episodio, por este camino recibe una recompensa total de -18. Por el otro camino, la recompensa al terminar el episodio es 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcule el valor del estado 1, para las siguientes políticas:\n",
    "\n",
    "* Siempre a la derecha.\n",
    "* Siempre abajo.\n",
    "* 50% y 50%.\n",
    "* 10\\% derecha y 90% abajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecuación de optimalidad de Bellman para el caso determinístico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para llegar a la ecuación de Bellman, vamos a hacerlo por pasos. Examine el siguiente diagrama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<img src=\"../Imagenes/varios_estados.png\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "</figure>\n",
    "\n",
    "Fuente: Maxim Lapan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El agente observa el estado $s_0$ y tiene $N$ acciones disponibles que lo pueden llevar a los estados $s_1,\\ldots, s_N$ con respectivas recompensas $r_1,\\ldots, r_N$. Supongamos además que conocemos el valor $V_i$ de los estados conectados. ¿Cuál es el mejor camino a seguir para el agente?\n",
    "\n",
    "Observe que si el agente escoge la acción $a_i$, entonces el valor será\n",
    "\n",
    "$$\n",
    "V_0(a=a_i) = r_i + \\gamma V_i.\n",
    "$$\n",
    "\n",
    "Así que para encontrar el valor del estado 0, se tendrá\n",
    "\n",
    "$$\n",
    "V_0  = \\max_{a\\in 1\\ldots,N}(r_a + \\gamma V_a).\n",
    "$$\n",
    "\n",
    "Parece una situación de procedimiento ambicioso. Pero tenga en cuenta que la acción elegida no mira únicamente la recompensa inmediata más alta, sino que tiene en cuenta también el valor a largo plazo.\n",
    "\n",
    "Bellman demostró que con esta extensión a la acción ambiciosa se obtiene la recompensa maximal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensión al caso estocástico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/bellman_estocastico.png\" width=\"300\" height=\"300\" align=\"center\"/>\n",
    "</figure>\n",
    "\n",
    "Fuente: Maxim Lapan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yLa diferencia en esta caso, considerando una acción simple, es que ahora el camino a seguir tiene tres posibles estados siguientes, cada uno de los cuales es escogido de manera aleatoria con probabilidad $p_i$. En esta caso se tiene que\n",
    "\n",
    "$$\n",
    "V_0(a=1)= p_1(r_1 + \\gamma V_1  ) +p_2(r_2 + \\gamma V_2  ) +p_3(r_3 + \\gamma V_3 ).\n",
    "$$\n",
    "\n",
    "Por supuesto, suponemos $p_1+p_2+p_3 = 1$. Hemos incluido el factor de descuento $\\gamma$ para ganar en generalidad.\n",
    "\n",
    "Más formalmente escribimos\n",
    "\n",
    "$$\n",
    "V_0(a)= E_{s\\sim S}[r_{0,a,s} + \\gamma V_s ]  =\\sum_{s\\in S}p_{a, 0\\to s}(r_{0,a,s} + \\gamma V_s)\n",
    "$$\n",
    "\n",
    "\n",
    "Finalmente, combinando con el caso determinístico tenemos que para el caso con varias acciones se obtiene \n",
    "\n",
    "$$\n",
    "V_0 = \\max_{a\\in A} E_{s\\sim S}[r_{0,a,s} + \\gamma V_s ]  =\\max_{a\\in A} \\sum_{s\\in S}p_{a, 0\\to s}(r_{0,a,s} + \\gamma V_s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor optimal del estado se obtiene seleccionando la acción que da la máxima recompensa inmediata posible más la recompensa descontada a largo plazo del siguiente estado.\n",
    "\n",
    "Observe que este es un concepto puramente recursivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Valor de la acción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por razones de tipo práctico, adicional al valor del estado, se define el valor de la acción $Q(s,a)$. Este valor es básicamente el valor de la recompensa que se puede obtener al ejecutar la acción $a$ desde el estado $s$.   El valor de la acción se denomina Q-valor de la acción $a$ y se define por\n",
    "\n",
    "$$\n",
    "Q(s,a) = E_{s'\\in S}[r_{s,a,s'} +\\gamma V(s')] = \\sum_{s' \\in S}p_{a, s\\to s'}\n",
    "(r_{s,a,s'} +\\gamma V(s'))\n",
    "$$\n",
    "\n",
    "$Q$ para este estado $s$ y acción $a$, es igual a la recompensa inmediata más la recompensa  a largo plazo del estado destino. Se puede definir $V(s)$ vía $Q(s,a)$:\n",
    "\n",
    "$$\n",
    "V(s) = \\max_{a\\in A}Q(s,a)\n",
    "$$\n",
    "\n",
    "Esto significa que el valor de algún estado es igual al valor de la máxima acción que puede ser ejecutada desde este estado.\n",
    "\n",
    "Adicionalmente, se puede expresar $Q(s,a)$ en forma recursiva como \n",
    "\n",
    "$$\n",
    "Q(s,a) = r_{s,a,s'} + \\gamma \\max_{a'\\in A} Q(s',a')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota sobre la notación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la anterior formula el índice de la recompensa inmediata, $s,a$ depende de los detalles del ambiente. \n",
    "\n",
    "* Si la recompensa inmediata es dada después de cada ejecución de una acción particular, $a$, desde el estado $s$, el índice (s,a) es usado directamente en la formula.\n",
    "* Por otro lado, si la recompensa es dada al alcanzar un estado $s'$, vía un acción $a'$ la recompensa tendrá índice $s',a'$. En este caso es necesario mover la recompensa dentro del operador $\\max$, se decir, en este caso $\n",
    "Q(s,a) =  \\gamma \\max_{a'\\in A}[r(s',a') + Q(s',a')]\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/diagrama_transicion.png\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "</figure>\n",
    "\n",
    "Fuente: Maxim Lapan\n",
    "\n",
    "\n",
    "Esta una simplificación del ambiente frozen lake.\n",
    "\n",
    "En el diagrama se muestra el diagrama de transición de un ejemplo muy simple que tiene un estado inicial $s_0$ y cuatro estados finales $s_1$ arriba, $s_2$ izquierda,$s_3$ abajo y $s_4$ derecha.\n",
    "\n",
    "Las recompensas recibidas al llegar a cada estado siguen el mismo orden 1, 2, 3 y 4.\n",
    "Cada acción tiene la misma forma probabilística que en frozen lake. Es decir, con 33% de probabilidad se ejecuta la acción decidida por el agente, con 33% se ejecuta la acción lateral a la izquierda de la decidida y con 33% de probabilidad se ejecuta la acción lateral a la derecha.\n",
    "\n",
    "Por simplicidad asumiremos $\\gamma=1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores de las acciones para el esto $S_0$ pueden calcularse como siguen. Dado que todos lo demás estados son terminales, podemos asumir que el valor de cada uno de esos estados es la recompensa que entregan. En consecuencia $V_1=1, V_2=2, V_3=0, V_4=4$. En donde el valor de cada una de las acciones para cada posible transición están dada por\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q(s_0, arriba) &= 0.33V_1 + 0.33V_2 + 0.33V_4 = 0.33*1 + 0.33*2 + 0.33*4 = 2.31\\\\\n",
    "Q(s_0, izquierda) &= 0.33V_4 + 0.33V_1 + 0.33V_3 = 0.33*4 + 0.33*1 + 0.33*3 = 1.98\\\\\n",
    "Q(s_0, derecha) &= 0.33V_4 + 0.33V_1 + 0.33V_3 = 0.33*4 + 0.33*1 + 0.33*4 = 2.64\\\\\n",
    "Q(s_0, abajo) &= 0.33V_3 + 0.33V_2 + 0.33V_4 = 0.33*3 + 0.33*2 + 0.33*4 = 2.97\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "De donde se desprende que el valor del estado es $V(s_0)=2.97$.\n",
    "\n",
    "Nuestro problema es que en general, las probabilidad de transición son desconocidos. Por eso introducimos el siguiente algoritmo de cálculo del valor de las acciones y los estados, con probabilidades de transición desconocidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">El método iteración del valor</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método permite calcular numéricamente el valor de los estados y el valor de las acciones. de un `proceso de decisión de Markov` (PDM), con probabilidades de transición desconocidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método para estimar el valor de los estados $V(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El procedimiento para el valor de los estados es el siguiente:\n",
    "\n",
    "1. Inicializa el valor de todos los estados $V$, usualmente en cero.\n",
    "1. Para cada estado $s$ en el PDM, se ejecuta la actualización de Bellman\n",
    "$$\n",
    "V(s) \\gets \\max_{a} \\sum_{s'} p_{a,s\\to s'}(r_{s,a,s'} + \\gamma V(s'))\n",
    "$$\n",
    "1. Se repite el paso 2 un número largo de veces o hasta que los cambios llegan a ser muy pequeños."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método para estimar el valor de las acciones  $Q(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de las acciones, es decir, $Q$ se requieren unos cambios menores.\n",
    "\n",
    "1. Se inicializa todos los $Q_{s,a}$ en cero.\n",
    "1. Para cada estado $s$, y acción $a$ en este estado, se ejecuta al actualización\n",
    "$$\n",
    "Q(s,a) \\gets  \\sum_{s'} p_{a,s\\to s'}(r_{s,a,s'} + \\gamma \\max_{a'} Q_{s',a'})\n",
    "$$\n",
    "1. Repite el paso 2.\n",
    "\n",
    "Veamos a la práctica. Empezamos con ejemplo muy simple para estimar $Q$ en un ambiente determinístico de acciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Algortimo Q-learning determinístico</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En este caso, la acción seleccionada se ejecuta con probabilidad 1. El algoritmo  *Q-Learning* en este caso utiliza la ecuación de Bellman, reescrita de la siguiente forma\n",
    "\n",
    "$$\n",
    "Q(s,a) = r_{s,a} + \\gamma \\max_{a^{'}}  Q(s^{'},a^{'}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejemplo se usaran las siguientes estructuras de datos.\n",
    "\n",
    "1. Una lista numérica con las recompensas de cada estado. \n",
    "1. Un lista boolena indicando para cada estado si es terminal o no.\n",
    "1. La matriz $Q$ que tiene como número de filas el número de estados, y como número de columnas el número de posibles acciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Entorno de cuadrícula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/cuadricula.png\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: Maxim Lapan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Si llega al estado de más a la izquierda, el episodio termina y el agente recibe una recompensa de -5. \n",
    "2. Por otro lado, si llega al estado de más a la derecha, el episodio termina y el agente recibe una recompensa de +5. \n",
    "\n",
    "El agente debe aprender a evitar el estado de -5 y moverse hacia el estado de +5. Si la política que aprende siempre termina en el estado con mayor recompensa, diremos que ha encontrado la **política óptima**(optimal policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empecemos definiendo nuestro entorno. \n",
    "\n",
    "1. Las recompensas son 0 para todos los estados, excepto para el estado de más a la izquierda y el de más a la derecha, que tienen recompensas de -5 y +5 respectivamente. \n",
    "2. También definimos una lista que define si un estado es final/terminal o no. \n",
    "3. Por último creamos la lista de variables llamada *Q_values*, donde guardaremos los *valores-Q* para todos los pares de estados y acciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "state_rewards = [-5, 0, 0, 0, 0, 0, 5]\n",
    "final_state = [True, False, False, False, False, False, True]\n",
    "Q_values = [[0.0, 0.0], \n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0]] # (s,a) matriz. [izquierda, derecha]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora crearemos una función que escoja una acción usando la política **ε-voraz** para un estado que pasaremos como parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_epsilon_greedy_action(epsilon, state):\n",
    "    \"\"\"Toma una acción con probabilidad epsilon, sino toma la mejor acción.\"\"\"\n",
    "    result = np.random.uniform()\n",
    "    if result < epsilon:\n",
    "        # exploración\n",
    "        return np.random.randint(0, 2) # acción aleatoria\n",
    "    else:\n",
    "        # explotación\n",
    "        return np.argmax(Q_values[state]) # Acción voraz (greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q_values[state] contiene los Q-values para cada una de las dos acciones. Entonces la función responde con la acción que tiene mayor Q-value para el *state* pasado a la función (que corresponde al estado actual del environment), en el caso de explotación. En el caso de exploración regresa aleatoriamente y con probabilidad epsilon cualquier acción disponible (en este caso alguna de las dos posibles acciones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También crearemos una función que ejerza de entorno. Le pasaremos el estado y la acción seleccionada por el agente, y nos devolverá la recompensa **r** y el siguiente estado **s’**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_action(state, action):\n",
    "    \"\"\"Aplica la acción seleccionada y obtiene la recompensa y el siguiente estado.\"\"\"\n",
    "    if action == 0:\n",
    "        next_state = state-1\n",
    "    else:\n",
    "        next_state = state+1\n",
    "    \n",
    "    return state_rewards[next_state], next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, decidimos varios hiperparámetros y ejecutamos el algoritmo que aprende usando el algoritmo de *Q-Learning* y la *ecuación de Bellman*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "epsilon = 0.2\n",
    "discount = 0.9 # \n",
    "\n",
    "for episode in range(num_episodes+1):\n",
    "    initial_state = 3 # estado en el medio\n",
    "    state = initial_state\n",
    "    while not final_state[state]: # corre hasta el final del episodio\n",
    "        # el agente selecciona una acción\n",
    "        action = select_epsilon_greedy_action(epsilon, state)\n",
    "        # se informa la acción al ambiente\n",
    "        # el ambiente cambia su estado\n",
    "        # regresa al recompensa y el siguiente estado\n",
    "        reward, next_state = apply_action(state, action)\n",
    "        # mejora los Q-values con la ecuación de  Bellman\n",
    "        if final_state[next_state]:\n",
    "            Q_values[state][action] = reward\n",
    "        else:\n",
    "            Q_values[state][action] = reward + discount * max(Q_values[next_state])\n",
    "        state = next_state\n",
    "        # print\n",
    "        print('episode: ', episode, 'Q_values:', Q_values)\n",
    "         \n",
    "# Imprime los valores Q para ver si la acción a la derecha es siempre mejor que la acción a la izquierda\n",
    "# excepto para los estados 0 y 6, que son estados terminales y no puede tomar\n",
    "# cualquier acción de ellos, por lo que no importa.\n",
    "print('los Q-values son:')\n",
    "print(Q_values)\n",
    "action_dict = {0:'izquierda', 1:'derecha'}\n",
    "state = 0\n",
    "\n",
    "for Q_vals in Q_values:\n",
    "    print('La mejor acción para el estado {} es {}'.format(state, \n",
    "                                             action_dict[np.argmax(Q_vals)]))\n",
    "    state += 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "episode:  1000 Q_values: [[0.0, 0.0], [-5, 3.2805], [2.9524500000000002, 3.645], [3.2805, 4.05], [3.645, 4.5], [4.05, 5], [0.0, 0.0]]\n",
    "los Q-values son:\n",
    "[[0.0, 0.0], [-5, 3.2805], [2.9524500000000002, 3.645], [3.2805, 4.05], [3.645, 4.5], [4.05, 5], [0.0, 0.0]]\n",
    "La mejor acción para el estado 0 es izquierda\n",
    "La mejor acción para el estado 1 es derecha\n",
    "La mejor acción para el estado 2 es derecha\n",
    "La mejor acción para el estado 3 es derecha\n",
    "La mejor acción para el estado 4 es derecha\n",
    "La mejor acción para el estado 5 es derecha\n",
    "La mejor acción para el estado 6 es izquierda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al terminar, observamos los *valores-Q* aprendidos y la mejor acción para cada estado. Al ejecutarlo vemos que ha aprendido exactamente lo que queríamos, la política óptima, a moverse hacia la derecha siempre. Hay que tener en cuenta que los *valores-Q* han sido descontados por el factor de descuento, que en este caso es 0.9. Los estados de la derecha y la izquierda tienen valores de 0.0 porque son terminales y el episodio termina al llegar a ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforme este código para funcione de manera similar a un ambiente gym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Algortimo Q-learning determinístico usando clases</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Q:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.Q = np.zeros(shape =(num_states, num_actions))\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "    \n",
    "    def reset(self):\n",
    "        self.Q = np.zeros(shape =(self.num_states, self.num_actions))\n",
    "    \n",
    "    def update(self, state, action,reward):\n",
    "        self.Q[state][action] = reward\n",
    "    \n",
    "    def get_Q(self):\n",
    "        return self.Q\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.Q)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Env:\n",
    "    def __init__(self, rewards):\n",
    "        self.rewards = rewards\n",
    "        self.position = len(rewards) // 2 # initial position\n",
    "        self.last_position = len(rewards) - 1\n",
    "        self.first_position = 0\n",
    "    \n",
    "    def is_done(self):\n",
    "        if (self.position == self.first_position) or \\\n",
    "           (self.position == self.last_position):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.position += -1\n",
    "        else:\n",
    "            self.position += 1\n",
    "        # return [next_obs, reward, is_done, info]\n",
    "        return [self.position, self.rewards[self.position], self.is_done(), None]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.position = len(self.rewards) // 2\n",
    "        return [self.position, self.rewards[self.position], self.is_done(), None]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "        self.env = None\n",
    "        self.q = None\n",
    "        self.state = None\n",
    "    \n",
    "    def set_q(self, q):\n",
    "        self.q  = q\n",
    "    \n",
    "    def set_env(self, env):\n",
    "        self.env = env\n",
    "        self.state,_,_,_ = self.env.reset()\n",
    "    \n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def take_epsilon_greedy_action(self, state):\n",
    "        \"\"\"Take random action with probability epsilon, else take best action.\"\"\"\n",
    "        result = np.random.uniform()\n",
    "        # exploration\n",
    "        if result < self.epsilon:\n",
    "            self.state = np.random.randint(0, self.q.get_Q().shape[1]) # Random action\n",
    "        # explotation\n",
    "        else:\n",
    "            self.state = np.argmax(self.q.get_Q()[state]) # Greedy action\n",
    "        \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Ribbon:\n",
    "    def __init__(self,  q, agent, env, num_episodes=1000, discount=0.9 ):\n",
    "        self.q = q\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "        self.num_episodes = num_episodes\n",
    "        self.discount = discount\n",
    "    \n",
    "    def play(self, verbose=True):\n",
    "        for episode in range(self.num_episodes+1):\n",
    "            state,_,is_done,_ = self.env.reset()\n",
    "            while not is_done:\n",
    "                action = self.agent.take_epsilon_greedy_action(state)\n",
    "                # return [next_obs, reward, is_done, info]}\n",
    "                next_state, reward, is_done, _ = env.step(action)\n",
    "                if is_done:\n",
    "                    q_value = reward\n",
    "                else:\n",
    "                    q_value = reward + self.discount * max(q.get_Q()[next_state])\n",
    "                    \n",
    "                q.update(state, action, q_value)\n",
    "                state = next_state\n",
    "            if verbose:\n",
    "                print('episode: ', episode, 'Q_values:', self.q)\n",
    "    \n",
    "    def print_results(self):\n",
    "        print('los Q-values son:')\n",
    "        print(self.q.get_Q())\n",
    "        action_dict = {0:'izquierda', 1:'derecha'}\n",
    "        state = 0\n",
    "\n",
    "        for Q_vals in self.q.get_Q():\n",
    "            print('La mejor acción para el estado {} es {}'.format(state, \n",
    "                                                     action_dict[np.argmax(Q_vals)]))\n",
    "            state += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "#\n",
    "# imports\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# rewards\n",
    "rewards = [-5,0,0,0,0,0,5]\n",
    "\n",
    "# Q\n",
    "num_states = len(rewards)\n",
    "num_actions = 2\n",
    "q = Q(num_states, num_actions)\n",
    "\n",
    "# env\n",
    "env = Env(rewards)\n",
    "\n",
    "# agent\n",
    "epsilon = 0.2\n",
    "agent = Agent(epsilon)\n",
    "agent.set_q(q)\n",
    "\n",
    "# ribbon\n",
    "u = Ribbon(q, agent, env,num_episodes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "los Q-values son:\n",
      "[[ 0.       0.     ]\n",
      " [-5.       3.2805 ]\n",
      " [ 2.95245  3.645  ]\n",
      " [ 3.2805   4.05   ]\n",
      " [ 3.645    4.5    ]\n",
      " [ 4.05     5.     ]\n",
      " [ 0.       0.     ]]\n",
      "La mejor acción para el estado 0 es izquierda\n",
      "La mejor acción para el estado 1 es derecha\n",
      "La mejor acción para el estado 2 es derecha\n",
      "La mejor acción para el estado 3 es derecha\n",
      "La mejor acción para el estado 4 es derecha\n",
      "La mejor acción para el estado 5 es derecha\n",
      "La mejor acción para el estado 6 es izquierda\n"
     ]
    }
   ],
   "source": [
    "u.play(verbose=False)\n",
    "u.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Método de iteración de valores en práctica</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para terminar la lección vamos a desarrollar un ejemplo completo de los dos métodos descritos. Haremos una implementación completa con el ambiente *frozen-lake*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método para estimar el valor de los estados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Tabla de recompensas**. Un diccionario que tiene una llave compuesta (estado fuente, acción, estado destino). El valor contiene la recompensa inmediata.\n",
    "* **Tabla de transiciones**. Un diccionario que contiene el conteo de las tras transiciones experimentadas por el agente. La clave es (estado, acción) y el valor es otro diccionario que almacena el número de veces que un estado destino ha sido visitado desde el estado fuente. Por ejemplo si la clave es $(0, 1)$, indica estado inicial = 0 y acción = 1.  Si por ejemplo desde el estado 0 se ha ejecutado la acción 1, digamos 10 veces, el valor para esta clave podría ser \\{4: 3, 5: 7 \\} que indicaría que de las 10 veces se llegó en 3 ocasiones al estado 4 y en 7 ocasiones al estado 5.\n",
    "* **Tabla de valores**. Una tabla que almacena el valor calculado para cada estado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lógica general de la implementación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Primero se ejecutan 100 pasos aleatorios desde el ambiente. Es decir, se obtienen 100 acciones de forma aleatoria del ambiente. Las tablas de recompensa y transición se poblan a partir de la información recibida del ambiente en estos 100 pasos aleatorios. La función *play_n_random_steps* es la encarga de hacer esta tarea.\n",
    "1. Al finalizar la primera etapa se corre una iteración para actualizar la tabla valor. \n",
    "1. Se ejecutan varios episodios completos usando la tabla actualizada de valor para chequear las mejoras. Si la recompensa promedio para esos episodios de test está por encima de un umbral (digamos 0.8 para el caso de FrozenLake) terminamos el entrenamiento. Durante estos episodios de test se actualizan las tablas de recompensa y transiciones.\n",
    "\n",
    "A continuación describimos los principales componentes de nuestra implementación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la clase *Agent* que contiene las tablas que usaremos y los métodos que realizaran las tareas. El constructor de la clase crea las tablas, el ambiente y la variable que contendrá el estado actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método play_n_random_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la función que genera datos de experiencia para el agente. En la función se generan n (=100) acciones desde el ambiente en forma aleatoria y se poblan y actualizan las tablas de recompensa y transición. Es interesante anotar que el método no espera el final de un episodio. Si llega al final del episodio actual, simplemente inicia le siguiente episodio sin reiniciar las tablas. Esta es una diferencia importante con el método de entropía cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método calc_action_value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método hace el cálculo del valor de la acción $a$  desde el estado $s$.\n",
    "\n",
    "La siguiente imagen ilustra la lógica del cálculo de una acción empezando en el estado $s$. Por facilidad suponemos que solamente se llega a los estados $s_1$ y $s_2$. El cálculo involucra las tablas de transición, recompensa y valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/state_value.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: Maxim Lapan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método hace lo siguiente:\n",
    "\n",
    "1. Extracta la información de los contadores incluidos en en la tabla de transición. `Counters` es una colección que tiene una forma de diccionario con los estados destinos como clave y el conteo de las veces que el agente llegó allí (desde el estado $s$ con la acción $a$). La función suma todas las frecuencias de Counter para normalizar los conteos y estimar la probabilidad de llegar a a cada estado.\n",
    "1. La función itera sobre todos estos estados destino  que la acción alcanza  y calcula su contribución al valor total de la acción usando la ecuación de Bellman. Esta contribución es: la recompensa inmediata más el valor descontado ($\\gamma V(s_i)$) del estado destino. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método select_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método permite seleccionar la mejor acción a partir de un estado $s$. La acción con mayor ganancia posible es seleccionada. Esta selección es determinista. Este método es de tipo ambicioso (greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método *play_episodio*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método ejecuta un episodio completo usando el correspondiente ambiente. Para no generar un desorden con el actual estado del ambiente definido en la clase, se genera un ambiente de prueba (test) que es pasado al método como argumento. La lógica es muy simple. Ejecuta el ciclo del episodio, seleccionado en cada paso la mejor acción disponible y acumulando la recompensa  del episodio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método value_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro método final y por supuesto más importante, es *value_iteration*. Este método es sorprendentemente simple gracias a las métodos descritos arriba. El método recorre todos los estados del ambiente.\n",
    "\n",
    "Para cada estado calcula el valor para los estados alcanzables a partir de él, obteniendo candidatos para el valor del estado. Al finalizar se actualiza el valor del estado con el máximo valor de la acción disponible desde el estado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ciclo de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llegamos a la pieza final del código, que es el ciclo de entrenamiento.\n",
    "\n",
    "Antes del ciclo de entrenamiento se ejecutan las siguientes tareas.\n",
    "\n",
    "1. Se instancia un agente, un ambientes (de prueba) y el writer para Tensorboard.\n",
    "1. Se inicializa la mejor recompensa  y el número de iteraciones.\n",
    "\n",
    "Dentro del ciclo se ejecutan las siguientes acciones.\n",
    "\n",
    "1. Se ejecutan n=100 pasos aleatorios para ganar experiencia con el ambiente y poblar las tablas de recompensa y transición con datos frescos.\n",
    "1. Con base en esta información se corre una iteración para calcular el valor de los estados.\n",
    "1. Se corre un conjunto de episodios de prueba (*TEST_EPISODES = 20*) utilizando la tabla de valores actualizada arriba como la política del agente y se escribe en el writer las cantidades que se desea seguir: la recompensa promedio  y el número de iteraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por favor revise, reescriba, corra y analice los resultados. Asegúrese de entender todos los detalles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.100\n",
      "Best reward updated 0.100 -> 0.350\n",
      "Best reward updated 0.350 -> 0.450\n",
      "Best reward updated 0.450 -> 0.600\n",
      "Best reward updated 0.600 -> 0.700\n",
      "Best reward updated 0.700 -> 0.800\n",
      "Best reward updated 0.800 -> 0.900\n",
      "Solved in 48 iterations!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "import collections\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "#ENV_NAME = \"FrozenLake8x8-v1\"      # uncomment for larger version\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(\n",
    "            collections.Counter)\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "            self.state = self.env.reset() \\\n",
    "                if is_done else new_state\n",
    "\n",
    "    def calc_action_value(self, state, action):\n",
    "        target_counts = self.transits[(state, action)]\n",
    "        total = sum(target_counts.values())\n",
    "        action_value = 0.0\n",
    "        for tgt_state, count in target_counts.items():\n",
    "            reward = self.rewards[(state, action, tgt_state)]\n",
    "            val = reward + GAMMA * self.values[tgt_state]\n",
    "            action_value += (count / total) * val\n",
    "        return action_value\n",
    "\n",
    "    def select_action(self, state):\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_action_value(state, action)\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "\n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            state_values = [\n",
    "                self.calc_action_value(state, action)\n",
    "                for action in range(self.env.action_space.n)\n",
    "            ]\n",
    "            self.values[state] = max(state_values)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    agent = Agent()\n",
    "    writer = SummaryWriter(comment=\"-v-iteration\")\n",
    "\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        agent.play_n_random_steps(100)\n",
    "        agent.value_iteration()\n",
    "\n",
    "        reward = 0.0\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "        reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\", reward, iter_no)\n",
    "        if reward > best_reward:\n",
    "            print(\"Best reward updated %.3f -> %.3f\" % (\n",
    "                best_reward, reward))\n",
    "            best_reward = reward\n",
    "        if reward > 0.80:\n",
    "            print(\"Solved in %d iterations!\" % iter_no)\n",
    "            break\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay varias diferencias notables en relación con el método de entropía cruzada.\n",
    "\n",
    "1. Este método es esencialmente aleatorio en naturaleza. Se corren episodios puramente aleatorios para recoger información acerca de la recompensa y transición entre estados siguiendo distintas acciones.\n",
    "1. No se espera el final de los episodios en la parte de test. Simplemente se va recibido la recompensa de cada episodio, que siempre es 1 o 0 y se continua hasta terminar el ciclo de test.\n",
    "1. El valor de los estados se obtiene a partir de la exploración aleatoria que hace el agente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Método de iteración de acciones en práctica</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método Q-learning tiene pequeños cambios en relación con el método del valor. La siguiente es la relación de cambios.\n",
    "\n",
    "1. El cambio más importante en las estructuras de datos es en la tabla valor. Ahora necesitamos almacenar los valores de la Q funcipon como hicimos antes en el ejemplo simpl de la cuadrícula. Ahora el diccionario tiene una clave compuesta (estado, acción).\n",
    "1. La segunda diferencia importante es que ahora el método *calc_action_value* ya no se necesita porque los valores de las acciones se almacenan en la tabla de valores Q.\n",
    "1. Finalmente el cambio más importante es en el método *value_action*. Antes, era solo una envoltura alrededor de la llamada calc_action_value (), que hizo el trabajo de aproximación de Bellman. Ahora, como esta función se ha ido y ha sido reemplazado por una tabla valor (Q), necesitamos hacer la aproximación en el método  *value_iteration*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por favor revise escriba y corra el siguiete código que implementa el método Q-learnig. Identifique los cambios en relación con el código anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.550\n",
      "Best reward updated 0.550 -> 0.650\n",
      "Best reward updated 0.650 -> 0.700\n",
      "Best reward updated 0.700 -> 0.750\n",
      "Best reward updated 0.750 -> 0.900\n",
      "Solved in 25 iterations!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "import collections\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "#ENV_NAME = \"FrozenLake8x8-v0\"      # uncomment for larger version\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "            self.state = self.env.reset() if is_done else new_state\n",
    "\n",
    "    def select_action(self, state):\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.values[(state, action)]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "\n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            for action in range(self.env.action_space.n):\n",
    "                action_value = 0.0\n",
    "                target_counts = self.transits[(state, action)]\n",
    "                total = sum(target_counts.values())\n",
    "                for tgt_state, count in target_counts.items():\n",
    "                    key = (state, action, tgt_state)\n",
    "                    reward = self.rewards[key]\n",
    "                    best_action = self.select_action(tgt_state)\n",
    "                    val = reward + GAMMA * \\\n",
    "                          self.values[(tgt_state, best_action)]\n",
    "                    action_value += (count / total) * val\n",
    "                self.values[(state, action)] = action_value\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    agent = Agent()\n",
    "    writer = SummaryWriter(comment=\"-q-iteration\")\n",
    "\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        agent.play_n_random_steps(100)\n",
    "        agent.value_iteration()\n",
    "\n",
    "        reward = 0.0\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "        reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\", reward, iter_no)\n",
    "        if reward > best_reward:\n",
    "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "            best_reward = reward\n",
    "        if reward > 0.80:\n",
    "            print(\"Solved in %d iterations!\" % iter_no)\n",
    "            break\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementción alterna (AM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "#import collections\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "\n",
    "class Env:\n",
    "    def  __init__(self, env=\"FrozenLake-v1\"):\n",
    "        self.env = gym.make(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "    \n",
    "    def sample(self):\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return self.env.observation_space\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self.env.action_space\n",
    "    \n",
    "\n",
    "class Q:\n",
    "    def __init__(self, obs_size, num_actions):\n",
    "        self._obs_size = obs_size\n",
    "        self._num_actions = num_actions\n",
    "        self.q = np.zeros(shape=(obs_size, num_actions))\n",
    "\n",
    "    def __setitem__(self, index, value):\n",
    "        self.q[index] = value\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.q[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return(str(self.q))\n",
    "\n",
    "    @property\n",
    "    def obs_size(self):\n",
    "        return self._obs_size\n",
    "    \n",
    "    @property\n",
    "    def num_actions(self):\n",
    "        return self._num_actions\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q = np.zeros(shape=(self._obs_size, self._num_actions))\n",
    "\n",
    "class Action:\n",
    "    def _init_(self, env, epsilon=0.2):\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def set_env(env): \n",
    "        self.env = env\n",
    "    \n",
    "    def set_epsilon(self):\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def sample(self):\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    def probability_selector(self, prob):\n",
    "        assert isinstance(prob, np.ndarray)\n",
    "        return np.randomchoice(len(prob), p=prob)\n",
    "\n",
    "    def greedy_selector(self, scores): \n",
    "        assert isinstance(scores, np.ndarray)\n",
    "        return np.argmax(scores)\n",
    "\n",
    "        \n",
    "    def epsilon_greedy_selector(self, scores, prob):\n",
    "        assert isinstance(prob, np.ndarray)\n",
    "        assert isinstance(scores, np.ndarray)\n",
    "        if np.random.random() < self.eps:\n",
    "            return probability_selector(prob) \n",
    "        else:\n",
    "            return greedy_selector(scores) \n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, alpha, gamma):\n",
    "        self.env = None\n",
    "        self.state = None\n",
    "        self.q = None\n",
    "        self.action = None\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    \n",
    "    def set_env(self, env):\n",
    "        self.env = env\n",
    "        self.state = self.env.reset()\n",
    "        self.obs_size = len(env.observation_space.n) # discrete\n",
    "        self.action_size = len(env.action_space.n) # discrete\n",
    "        self.q = Q(self.obs_size, self.action_size)\n",
    "\n",
    "    \n",
    "    def set_q(self, q):\n",
    "        self.q = q\n",
    "    \n",
    "    def get_q(self):\n",
    "        return self.q\n",
    "\n",
    "    def set_action(self, action):\n",
    "        self.action = action\n",
    "\n",
    "    def set_alpha(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def set_gamma(self, gamma):\n",
    "        self.alpha = gamma\n",
    "    \n",
    "    def reset_q(self):\n",
    "        self.q.reset() if self.q is not None\n",
    "\n",
    "    def sample_sars(self):\n",
    "        action = self.action.sample()\n",
    "        old_state = self.state\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.state = self.env.reset() is is_done else new_state\n",
    "        # sars'\n",
    "        return old_state, action, reward, new_state\n",
    "    \n",
    "    def best_value_and_action(self, state):\n",
    "        best_value, best_action = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.q[state, action]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_value, best_action\n",
    "\n",
    "    def value_update(self, s, a, r, next_s):\n",
    "        best_v, _ = self.best_value_and_action(next_s)\n",
    "        new_v = r + self.gamma * best_v\n",
    "        old_v = self.q[s, a]\n",
    "        self.q[s, a] = old_v * (1-self.alpha) + new_v * self.alpha\n",
    "    \n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            _, action = self.best_value_and_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "\n",
    "\n",
    "class Q_leaning:\n",
    "    def __init__(self, env_name=\"FrozenLake-v1\", ALPHA, GAMMA, epsilon):\n",
    "        self.env = Env(env_name)\n",
    "        self.test_env = Env(env_name)\n",
    "        self.agent = Agent(ALPHA, GAMMA)\n",
    "        self.agent.set_env(env)\n",
    "        self.agent.set_action(Action(env, epsilon))\n",
    "\n",
    "    def play(self,  test_episodes=20, reset_q=False, writer=None):\n",
    "        if reset_q:\n",
    "            q.reset()\n",
    "        iter_no = 0\n",
    "        best_reward = 0.0\n",
    "        while True:\n",
    "            iter_no += 1\n",
    "            s, a, r, next_s = agent.sample_sars()\n",
    "            self.agent.value_update(s, a, r, next_s)\n",
    "\n",
    "            reward = 0.0\n",
    "            for _ in range(test_episodes):\n",
    "                reward += self.agent.play_episode(self.test_env)\n",
    "            reward /= test_episodes\n",
    "            if writer is not None:\n",
    "                writer.add_scalar(\"reward\", reward, iter_no)\n",
    "            if reward > best_reward:\n",
    "                print(\"Best reward updated %.3f -> %.3f\" % (\n",
    "                    best_reward, reward))\n",
    "                best_reward = reward\n",
    "            if reward > 0.80:\n",
    "                print(\"Solved in %d iterations!\" % iter_no)\n",
    "                break\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "############## main\n",
    "ALPHA = 0.2\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20\n",
    "EPSILON = 0.2\n",
    "ql = Q_leaning(self, env_name=\"FrozenLake-v1\", ALPHA, GAMMA, EPSILON)\n",
    "\n",
    "ql.play()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
