{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Q-learning: Ecuación de Bellman</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Fantasy_Art_Machine_Learning_The_Partnership.png\" width=\"500\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "<a href=\"https://commons.wikimedia.org/wiki/File:Fantasy_Art_Machine_Learning_The_Partnership.png\">David S. Soriano</a>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\">CC BY-SA 4.0</a>, via Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Richard S. Sutton, Andrew G. Barto, Reinforcement learning: an introduction, 2nd edition, 2020](http://library.lol/main/6502B74CE247C4CD4D4FB54747AD7C7E)\n",
    "1. [Praveen Palanisamy - Hands-On Intelligent Agents with OpenAI Gym_ Your Guide to Developing AI Agents Using Deep Reinforcement Learning, 2020](http://library.lol/main/E4FD128CF9B93E0F7A542B053330517A)\n",
    "1.[Markel Sanz, Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-2-q-learning-883cd42fb48e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Función de valor](#Función-de-valor)\n",
    "* [Q-Learning](#Q-Learning)\n",
    "* [Ecuación de Bellman](#Ecuación-de-Bellman)\n",
    "* [Valor de la acción](#Valor-de-la-acción)\n",
    "* [Algortimo Q-learning determinístico](Algortimo-Q-learning-determinístico)\n",
    "* [Método de iteración de valores en práctica](#Método-de-iteración-de-valores-en-práctica)\n",
    "* [Método de iteración de acciones en práctica](#Método-de-iteración-de-acciones-en-práctica)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la lección del bandido multibrazo, hemos descrito el problema del bandido multibrazo, y hemos introducido varios conceptos, como el estado, la acción, la recompensa, etc. Sin embargo, el ejemplo del bandido multibrazo no representa el problema completo del aprendizaje reforzado. En los problemas de bandidos multibrazo, cada acción es completamente independiente de las anteriores, y el estado siempre es el mismo. \n",
    "\n",
    "\n",
    "En el problema completo de aprendizaje por refuerzo, el estado cambia cada vez que ejecutamos una acción. Podemos representar el problema general de aprendizaje reforzado de la siguiente manera. \n",
    "\n",
    "1. El **agente** recibe la siguiente observación(que llamaremos estado en esta lección) del  **ambiente** (environment). Denotaremos a  los estados mediante $s_i$.\n",
    "2. El agente ejecuta entonces la **acción** que elija y le informa ambiente. Las acciones de denotaran $a_i$\n",
    "3. Al ejecutar esa acción, el ambiente responde proporcionando una **recompensa** y una nueva observación. La recompensas se denotarán $r_i$.\n",
    "\n",
    "Este ciclo se puede observar en la siguiente imagen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/agente-ambiente.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<figcaption>Proceso del aprendizaje reforzado</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo tanto, **la acción que el agente escoja no sólo depende de la recompensa  que vaya a recibir a corto plazo. Debe elegir las acciones que a largo plazo le traigan la máxima recompensa (o retorno) posible en todo el episodio (episode)**. \n",
    "\n",
    "Este ciclo trae una secuencia de estados, acciones y recompensas, desde el primer paso del ciclo hasta el último: \n",
    "\n",
    "$$\n",
    "s_1, a_1, r_1; s_2, a_2, r_2; \\ldots; s_T, a_T, r_T. \n",
    "$$\n",
    "\n",
    "Aquí, $T$ indica el número total de pasos del episodio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Función de valor</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cuantificar cuanta recompensa obtendrá el agente a largo plazo desde cada estado, introducimos la función de valor $V(s)$. Esta función produce una estimación de la recompensa que obtendrá el agente hasta el final del episodio, empezando desde el estado s. Si conseguimos estimar este valor correctamente, podremos decidir ejecutar la acción que nos lleve al estado con el valor más alto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Q-Learning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver el problema del aprendizaje reforzado, el agente debe aprender a escoger la mejor acción posible para cada uno de los estados posibles. Para ello, con el algoritmo **Q-Learning** el agente intenta aprender cuanta recompensa obtendrá a largo plazo para cada pareja de estados y acciones $(s,a)$. \n",
    "\n",
    "A esa función se la llama la **función de acción-valor** (action-value function) y en este algoritmo se representa como la función $Q(s,a)$, la cual devuelve la recompensa que el agente recibirá al ejecutar la acción $a$ desde el estado $s$, asumiendo que seguirá la misma política dictada por la función $Q$ hasta el final del episodio. \n",
    "\n",
    "\n",
    "Por lo tanto, si desde el estado $s$, tenemos dos acciones disponibles, $a_1$ y $a_2$, la función $Q$ nos proporcionará los **valores-Q** (Q-values) de cada una de las acciones. Por ejemplo, si $Q(s,a_1)=1$ y $Q(s,a_2)=4$, el agente sabe que la acción $a_2$ es mejor y le traerá mayor recompensa, por lo que será la acción que ejecutará, una vez haya sido entrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Ecuación de Bellman</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos del capítulo de [introducción al aprendizaje reforzado](ar_Aprendizaje_Reforzado_Intro.ipynb)  que definimos el  `valor del estado` *s* para una determinada siguiente acción *a* como\n",
    "\n",
    "$$\n",
    "V_s(a) = E\\left[\\sum_{t=0}^{\\infty}r_t\\gamma^t \\right],\n",
    "$$\n",
    "\n",
    "en donde $r_t$ es la recompensa local obtenida en el paso $t$ del episodio y $\\gamma$ el factor de descuento. El valor es calculado  de acuerdo con una política que el agente sigue en el entrenamiento.\n",
    "\n",
    "\n",
    "Esta formula puede reescribirse de la siguiente forma. Si con la acción *a* se pasa del estado *s* al estado *s'* y se obtiene una recompensa $r_{s,a,s'}$, entonces\n",
    "\n",
    "\n",
    "$$\n",
    "V_s(a) = E_{s'\\in {S}}\\left[r_{s,a,s'} + \\gamma V_{s'}(a') \\right],\n",
    "$$\n",
    "\n",
    "El valor del estado *s* es definod por\n",
    "\n",
    "$$\n",
    "V_s = \\max_a V_s(a),\n",
    "$$\n",
    "\n",
    "lo cual implica que\n",
    "\n",
    "$$\n",
    "V_s = \\max_{a\\in {A}}E_{s'\\in {S}}\\left[r_{s,a,s'} + \\gamma V_{s'} \\right],\n",
    "$$\n",
    "\n",
    "Esta froma recursiva de definir el valor de un estado se conoce como  `ecuación de Bellman`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplo simple con dos estados finales</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos el siguiente ejemplo simple. El ambiente tiene tres estados:\n",
    "\n",
    "1. El estado inicial del agente.\n",
    "1. El estado final que se encuentra al ejecutar la acción *derecha* desde el estado inicial. La recompensa recibida al alcanzar este estado es 1.\n",
    "1. El estado final que se encuentra al ejecutar la acción *abajo* desde el estado inicial. La recompensa recibida al alcanzar este estado es 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/dos_estados.png\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "<figcaption>Problema con dos estados finales</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo el ambiente es determinístico: Toda acción sucede y siempre empieza en el estado 1. Una vez se alcanza el estado 2 o el estado 3 el episodio termina. La pregunta es ¿Cuál es el valor del estado 1?.\n",
    "\n",
    "Esta pregunta carece de significado sin la información acerca del comportamiento (política) del agente. Aún en este ejemplo tan simple, el agente puede tener una gran cantidad de comportamientos. Por ejemplo:\n",
    "\n",
    "* El agente siempre va  la derecha.\n",
    "* El agente siempre va a abajo.\n",
    "* El agente va a la derecha o baja con probabilidad 50% en cada caso.\n",
    "* El agente va a la derecha con probabilidad 10% y baja con probabilidad 90%.\n",
    "\n",
    "El valor del estado 1 en cada caso es\n",
    "\n",
    "* siempre a la derecha: 1.\n",
    "* Siempre baja: 2.0\n",
    "* 50% y 50%: 1.0\\*0.5 +2.0\\*0.5 = 1.5.\n",
    "* Para el último caso, el valor del estado 1 es: 1.0\\*0.1 + 2.0\\*0.9 = 1.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">¿Cuál es la política optimal para este agente?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el propósito de los agentes en el AR es obtener la máxima recompensa, en este caso, la política optimal es siempre bajar. Sin embargo un caso tan simple como este, no es de interés práctico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Observación</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ejemplo da la impresión de que el agente debe tomar siempre la siguiente acción que le entrega mayor recompensa inmediata. El asunto no es tan sencillo.\n",
    "\n",
    "Veamos el siguiente ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/cuatro_estados.png\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "<figcaption>Problema con cuatro estados finales</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se observa, siempre empezando en el estado 1, el siguiente estado que ofrece mayor recompensa es el estado 3, con recompensa = 2. Pero  el agente solamente puede continuar para terminar el episodio hacia el estado en donde recibe recompensa= -20. \n",
    "\n",
    "Por lo que que final del episodio, por este camino recibe una recompensa total de -18. Por el otro camino, la recompensa al terminar el episodio es 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejercicio</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcule el valor del estado 1, para las siguientes políticas de inicio:\n",
    "\n",
    "* Siempre a la derecha.\n",
    "* Siempre abajo.\n",
    "* 50% y 50%.\n",
    "* 10% derecha y 90% abajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ecuación de optimalidad de Bellman para el caso determinístico</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para llegar a la ecuación de Bellman, vamos a hacerlo por pasos. Examine el siguiente diagrama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/varios_estados.png\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "<figcaption>Problema determinístico general con varios estados</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El agente observa el estado $s_0$ y tiene $N$ acciones disponibles que lo pueden llevar a los estados $s_1,\\ldots, s_N$ con respectivas recompensas $r_1,\\ldots, r_N$. Supongamos además que conocemos el valor $V_i$ de los estados conectados. ¿Cuál es el mejor camino a seguir para el agente?\n",
    "\n",
    "Observe que si el agente escoge la acción $a_i$, entonces el valor será\n",
    "\n",
    "$$\n",
    "V_0(a=a_i) = r_i + \\gamma V_i.\n",
    "$$\n",
    "\n",
    "Así que para encontrar el valor del estado 0, se tendrá\n",
    "\n",
    "$$\n",
    "V_0  = \\max_{a_i, i \\in 1\\ldots,N}(r_a + \\gamma V_a).\n",
    "$$\n",
    "\n",
    "Parece una situación de procedimiento ambicioso. Pero tenga en cuenta que la acción elegida no mira únicamente la recompensa inmediata más alta, sino que tiene en cuenta también el valor a largo plazo.\n",
    "\n",
    "Bellman demostró que con esta extensión a la acción ambiciosa se obtiene la recompensa maximal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Extensión al caso estocástico</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/bellman_estocastico.png\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "<figcaption>Problema estocástico</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La diferencia en esta caso, considerando una acción simple, es que ahora el camino a seguir tiene cuatro posibles estados siguientes, cada uno de los cuales es escogido de manera aleatoria con probabilidad $p_i$. En esta caso se tiene que\n",
    "\n",
    "$$\n",
    "V_0(a=1)= p_1(r_1 + \\gamma V_1  ) + p_2(r_2 + \\gamma V_2  ) +p_3(r_3 + \\gamma V_3 ) + p_4(r_4 + \\gamma V_4 ).\n",
    "$$\n",
    "\n",
    "Por supuesto, suponemos $p_1+p_2+p_3 + p_4 = 1$. Hemos incluido el factor de descuento $\\gamma$ para ganar en generalidad.\n",
    "\n",
    "Más formalmente escribimos\n",
    "\n",
    "$$\n",
    "V_0(a)= E_{s\\sim S}[r_{0,a,s} + \\gamma V_s ]  =\\sum_{s\\in S}p_{a, 0\\to s}(r_{0,a,s} + \\gamma V_s)\n",
    "$$\n",
    "\n",
    "\n",
    "Finalmente, combinando con todas las acciones disponibles tenemos que \n",
    "\n",
    "$$\n",
    "V_0 = \\max_{a\\in A} E_{s\\sim S}[r_{0,a,s} + \\gamma V_s ]  =\\max_{a\\in A} \\sum_{s\\in S}p_{a, 0\\to s}(r_{0,a,s} + \\gamma V_s)\n",
    "$$\n",
    "\n",
    "Esta es la famosa `ecuación de Bellman`, ya escrita arriba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Interpretación</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor optimal del estado se obtiene seleccionando la acción que da la máxima recompensa total, la cual se obtiene sumando ls recompensa  inmediata más la recompensa descontada a largo plazo del siguiente estado.\n",
    "\n",
    "Observe que este es un concepto puramente recursivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Valor de la acción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por razones de tipo práctico, adicional al valor del estado, se define el valor de la acción $Q(s,a)$. Este valor es básicamente el valor de la recompensa que se puede obtener al ejecutar la acción $a$ desde el estado $s$.   El valor de la acción se denomina Q-valor de la acción $a$ y se define por\n",
    "\n",
    "$$\n",
    "Q(s,a) = E_{s'\\in S}[r_{s,a,s'} +\\gamma V(s')] = \\sum_{s' \\in S}p_{a, s\\to s'}\n",
    "(r_{s,a,s'} +\\gamma V(s'))\n",
    "$$\n",
    "\n",
    "$Q$ para este estado $s$ y acción $a$, es igual a la recompensa inmediata más la recompensa  a largo plazo del estado destino. Se puede definir $V(s)$ vía $Q(s,a)$:\n",
    "\n",
    "$$\n",
    "V(s) = \\max_{a\\in A}Q(s,a)\n",
    "$$\n",
    "\n",
    "Esto significa que el valor de algún estado es igual al valor de la máxima acción que puede ser ejecutada desde este estado.\n",
    "\n",
    "Adicionalmente, se puede expresar $Q(s,a)$ en forma recursiva como \n",
    "\n",
    "$$\n",
    "Q(s,a) = E_{s' \\in S}\\left[r_{s,a,s'} + \\gamma \\max_{a'\\in A} Q(s',a')\\right]= \\sum_{s' \\in S}p_{a, s\\to s'}\n",
    "\\left[r_{s,a,s'} +\\gamma \\max_{a'\\in A} Q(s',a')\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Nota sobre la notación</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la anterior formula el índice de la recompensa inmediata, $s,a$ depende de los detalles del ambiente. \n",
    "\n",
    "* Si la recompensa inmediata es dada después de cada ejecución de una acción particular, $a$, desde el estado $s$, el índice (s,a) es usado directamente en la formula.\n",
    "* Por otro lado, si la recompensa es dada al alcanzar un estado $s'$, vía un acción $a'$ la recompensa tendrá índice $s',a'$. En este caso es necesario mover la recompensa dentro del operador $\\max$, se decir, en este caso $\n",
    "Q(s,a) =  E[\\gamma \\max_{a'\\in A}[r(s',a') + Q(s',a')]].\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente imagen constituye una simplificación del problema de *FrozenLake*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/diagrama_transicion.png\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "<figcaption>Diagrama de transición</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Maxim Lapan](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En el diagrama se muestra el diagrama de transición de un ejemplo muy simple que tiene un estado inicial $s_0$ y cuatro estados finales $s_1$ arriba, $s_2$ izquierda,$s_3$ abajo y $s_4$ derecha.\n",
    "\n",
    "Las recompensas recibidas al llegar a cada estado siguen el mismo orden 1, 2, 3 y 4.\n",
    "Cada acción tiene la misma forma probabilística que en *FrozenLake*. Es decir, con 33% de probabilidad se ejecuta la acción decidida por el agente, con 33% se ejecuta la acción lateral a la izquierda de la decidida y con 33% de probabilidad se ejecuta la acción lateral a la derecha.\n",
    "\n",
    "Por simplicidad asumiremos $\\gamma=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores de las acciones para el esto $S_0$ pueden calcularse como siguen. Dado que todos lo demás estados son terminales, podemos asumir que el valor de cada uno de esos estados es la recompensa que entregan. En consecuencia $V_1=1, V_2=2, V_3=0, V_4=4$. En donde el valor de cada una de las acciones para cada posible transición están dada por\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q(s_0, arriba) &= 0.33V_1 + 0.33V_2 + 0.33V_4 = 0.33*1 + 0.33*2 + 0.33*4 = 2.31\\\\\n",
    "Q(s_0, izquierda) &= 0.33V_4 + 0.33V_1 + 0.33V_3 = 0.33*1 + 0.33*2 + 0.33*3 = 1.98\\\\\n",
    "Q(s_0, derecha) &= 0.33V_4 + 0.33V_1 + 0.33V_3 = 0.33*4 + 0.33*1 + 0.33*3 = 2.64\\\\\n",
    "Q(s_0, abajo) &= 0.33V_3 + 0.33V_2 + 0.33V_4 = 0.33*3 + 0.33*2 + 0.33*4 = 2.97\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "De donde se desprende que el valor del estado es $V(s_0)=2.97$.\n",
    "\n",
    "Nuestro problema es que en general, las probabilidad de transición son desconocidos. Por eso introducimos el siguiente algoritmo de cálculo del valor de las acciones y los estados, con probabilidades de transición desconocidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">El método iteración del valor</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método permite calcular numéricamente el valor de los estados y el valor de las acciones. de un `proceso de decisión de Markov` (PDM), con probabilidades de transición desconocidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Método para estimar el valor de los estados $V(s)$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El procedimiento para el valor de los estados es el siguiente:\n",
    "\n",
    "1. Inicializa el valor de todos los estados $V$, usualmente en cero.\n",
    "1. Para cada estado $s$ en el PDM, se ejecuta la actualización de Bellman\n",
    "$$\n",
    "V(s) \\gets \\max_{a} \\sum_{s'} p_{a,s\\to s'}(r_{s,a,s'} + \\gamma V(s'))\n",
    "$$\n",
    "1. Se repite el paso 2 un número largo de veces o hasta que los cambios llegan a ser muy pequeños."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Método para estimar el valor de las acciones  $Q(s,a)$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de las acciones, es decir, $Q$ se requieren unos cambios menores.\n",
    "\n",
    "1. Se inicializa todos los valores $Q(s,a)$ en cero.\n",
    "1. Para cada estado $s$, y acción $a$ en este estado, se ejecuta la actualización\n",
    "$$\n",
    "Q(s,a) \\gets  \\sum_{s'} p_{a,s\\to s'}(r_{s,a,s'} + \\gamma \\max_{a'} Q(s',a'))\n",
    "$$\n",
    "1. Repite el paso 2.\n",
    "\n",
    "Veamos a la práctica. Empezamos con ejemplo muy simple para estimar $Q$ en un ambiente determinístico de acciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Algortimo Q-learning determinístico</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En este caso, la acción seleccionada se ejecuta con probabilidad 1. El algoritmo  *Q-Learning* en este caso utiliza la ecuación de Bellman, reescrita de la siguiente forma\n",
    "\n",
    "$$\n",
    "Q(s,a) = r_{s,a,s'} + \\gamma \\max_{a^{'}}  Q(s^{'},a^{'}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejemplo se usaran las siguientes estructuras de datos.\n",
    "\n",
    "1. Una lista numérica con las recompensas de cada estado. \n",
    "1. Un lista boolena indicando para cada estado si es terminal o no.\n",
    "1. La matriz $Q$ que tiene como número de filas el número de estados, y como número de columnas el número de posibles acciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplo: Entorno de cuadrícula</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/cuadricula.png\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "<figcaption>Aprendizaje-q. Ejemplo de la cuadrícula</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Si llega al estado de más a la izquierda, el episodio termina y el agente recibe una recompensa de -5. \n",
    "2. Por otro lado, si llega al estado de más a la derecha, el episodio termina y el agente recibe una recompensa de +5. \n",
    "\n",
    "El agente debe aprender a evitar el estado de -5 y moverse hacia el estado de +5. Si la política que aprende siempre termina en el estado con mayor recompensa, diremos que ha encontrado la **política óptima**(optimal policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">El código</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empecemos definiendo nuestro entorno. \n",
    "\n",
    "1. Las recompensas son 0 para todos los estados, excepto para el estado de más a la izquierda y el de más a la derecha, que tienen recompensas de -5 y +5 respectivamente. \n",
    "2. También definimos una lista que define si un estado es final/terminal o no. \n",
    "3. Por último creamos la matriz  *Q*, en la cual guardaremos los *valores-Q* para todos los pares de estados y acciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Implementación del algortimo Q-learning determinístico</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Función Create_Q</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Crea la matriz Q, que al final contendrá la política"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def create_Q(num_states, num_actions):\n",
    "    return np.zeros((num_states, num_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Clase Env</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Implementa el ambiente para este problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self, rewards):\n",
    "        self.rewards = rewards\n",
    "        self.position = len(rewards) // 2 # initial position\n",
    "        self.last_position = len(rewards) - 1\n",
    "        self.first_position = 0\n",
    "    \n",
    "    def is_done(self):\n",
    "        if (self.position == self.first_position) or \\\n",
    "           (self.position == self.last_position):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.position += -1\n",
    "        else:\n",
    "            self.position += 1\n",
    "        # return [next_obs, reward, is_done, info]\n",
    "        return [self.position, self.rewards[self.position], self.is_done(), None]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.position = len(self.rewards) // 2\n",
    "        return [self.position, self.rewards[self.position], self.is_done(), None]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Clase Agent</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementación del agente para el problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, Q, epsilon=0.2, discount=0.9):\n",
    "        self.env = env\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        #self.state,_,_,_ = self.env.reset()\n",
    "    \n",
    "    def set_Q(self, Q):\n",
    "        self.q  = Q\n",
    "    \n",
    "    def set_env(self, env):\n",
    "        self.env = env\n",
    "        #self.state,_,_,_ = self.env.reset()\n",
    "    \n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    #def get_state(self):\n",
    "    #   return self.state\n",
    "\n",
    "    def take_epsilon_greedy_action(self, state):\n",
    "        \"\"\"\n",
    "        elije una acción de forma epsilon-voraz\n",
    "        de acuerdo al estado en el que se encuentra\n",
    "        \"\"\"\n",
    "        num_actions = self.Q.shape[1]\n",
    "        result = np.random.uniform()\n",
    "        # exploración\n",
    "        if result < self.epsilon:\n",
    "            action = np.random.randint(0, num_actions) # Random action\n",
    "        # explotación\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state]) # Greedy action\n",
    "        # retorna acción\n",
    "        return action\n",
    "    \n",
    "    def run_episode(self):\n",
    "        state,_,is_done,_ = self.env.reset()\n",
    "        while not is_done:\n",
    "            # obtiene una acción\n",
    "            action = self.take_epsilon_greedy_action(state)\n",
    "            # entrega la acción seleccionada al ambiente\n",
    "            next_state, reward, is_done, _ = env.step(action)\n",
    "            # return [next_obs, reward, is_done, info]}\n",
    "            # calcula la recompensa total\n",
    "            if is_done:\n",
    "                q_value = reward\n",
    "            else:\n",
    "                q_value = reward + self.discount * max(self.Q[next_state])\n",
    "            # actualiza matriz Q\n",
    "            self.Q[state, action] = q_value\n",
    "            # actualiza al nuevo estado\n",
    "            state = next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Clase Trainer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenador  para el problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "    def __init__(self,  env, Q, num_episodes=1000, epsilon=0.2, discount=0.9):\n",
    "        self.agent = Agent(env, Q, epsilon, discount)\n",
    "        self.num_episodes = num_episodes\n",
    "        self.Q = Q\n",
    "    \n",
    "    def fit(self, verbose=True, delta=100):\n",
    "        for episode in range(1, self.num_episodes+1):\n",
    "            self.agent.run_episode()\n",
    "            if verbose and episode % delta == 0:\n",
    "                print('episode: ', episode, 'Q_values:', self.Q)\n",
    "    \n",
    "    def print_results(self):\n",
    "        print('los Q-values son:')\n",
    "        print(self.Q)\n",
    "        action_dict = {0:'izquierda', 1:'derecha'}\n",
    "        state = 0\n",
    "        for Q_vals in self.Q:\n",
    "            print('La mejor acción para el estado {} es {}'.format(state, \n",
    "                                                     action_dict[np.argmax(Q_vals)]))\n",
    "            state += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Entrenamiento</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos un entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 Q_values: [[ 0.  0.]\n",
      " [-5.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "episode:  100 Q_values: [[ 0.  0.]\n",
      " [-5.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "episode:  200 Q_values: [[ 0.    0.  ]\n",
      " [-5.    0.  ]\n",
      " [ 0.    0.  ]\n",
      " [ 0.    4.05]\n",
      " [ 0.    4.5 ]\n",
      " [ 0.    5.  ]\n",
      " [ 0.    0.  ]]\n",
      "episode:  300 Q_values: [[ 0.       0.     ]\n",
      " [-5.       3.2805 ]\n",
      " [ 2.95245  3.645  ]\n",
      " [ 3.2805   4.05   ]\n",
      " [ 3.645    4.5    ]\n",
      " [ 4.05     5.     ]\n",
      " [ 0.       0.     ]]\n",
      "episode:  400 Q_values: [[ 0.       0.     ]\n",
      " [-5.       3.2805 ]\n",
      " [ 2.95245  3.645  ]\n",
      " [ 3.2805   4.05   ]\n",
      " [ 3.645    4.5    ]\n",
      " [ 4.05     5.     ]\n",
      " [ 0.       0.     ]]\n",
      "episode:  500 Q_values: [[ 0.       0.     ]\n",
      " [-5.       3.2805 ]\n",
      " [ 2.95245  3.645  ]\n",
      " [ 3.2805   4.05   ]\n",
      " [ 3.645    4.5    ]\n",
      " [ 4.05     5.     ]\n",
      " [ 0.       0.     ]]\n",
      "episode:  600 Q_values: [[ 0.       0.     ]\n",
      " [-5.       3.2805 ]\n",
      " [ 2.95245  3.645  ]\n",
      " [ 3.2805   4.05   ]\n",
      " [ 3.645    4.5    ]\n",
      " [ 4.05     5.     ]\n",
      " [ 0.       0.     ]]\n",
      "episode:  700 Q_values: [[ 0.       0.     ]\n",
      " [-5.       3.2805 ]\n",
      " [ 2.95245  3.645  ]\n",
      " [ 3.2805   4.05   ]\n",
      " [ 3.645    4.5    ]\n",
      " [ 4.05     5.     ]\n",
      " [ 0.       0.     ]]\n",
      "episode:  800 Q_values: [[ 0.       0.     ]\n",
      " [-5.       3.2805 ]\n",
      " [ 2.95245  3.645  ]\n",
      " [ 3.2805   4.05   ]\n",
      " [ 3.645    4.5    ]\n",
      " [ 4.05     5.     ]\n",
      " [ 0.       0.     ]]\n",
      "episode:  900 Q_values: [[ 0.       0.     ]\n",
      " [-5.       3.2805 ]\n",
      " [ 2.95245  3.645  ]\n",
      " [ 3.2805   4.05   ]\n",
      " [ 3.645    4.5    ]\n",
      " [ 4.05     5.     ]\n",
      " [ 0.       0.     ]]\n",
      "episode:  1000 Q_values: [[ 0.       0.     ]\n",
      " [-5.       3.2805 ]\n",
      " [ 2.95245  3.645  ]\n",
      " [ 3.2805   4.05   ]\n",
      " [ 3.645    4.5    ]\n",
      " [ 4.05     5.     ]\n",
      " [ 0.       0.     ]]\n",
      "los Q-values son:\n",
      "[[ 0.       0.     ]\n",
      " [-5.       3.2805 ]\n",
      " [ 2.95245  3.645  ]\n",
      " [ 3.2805   4.05   ]\n",
      " [ 3.645    4.5    ]\n",
      " [ 4.05     5.     ]\n",
      " [ 0.       0.     ]]\n",
      "La mejor acción para el estado 0 es izquierda\n",
      "La mejor acción para el estado 1 es derecha\n",
      "La mejor acción para el estado 2 es derecha\n",
      "La mejor acción para el estado 3 es derecha\n",
      "La mejor acción para el estado 4 es derecha\n",
      "La mejor acción para el estado 5 es derecha\n",
      "La mejor acción para el estado 6 es izquierda\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "#\n",
    "# imports\n",
    "import numpy as np\n",
    "\n",
    "# rewards\n",
    "rewards = [-5,0,0,0,0,0,5]\n",
    "num_states = len(rewards)\n",
    "num_actions = 2\n",
    "# Matriz Q\n",
    "Q = create_Q(num_states, num_actions)\n",
    "# env\n",
    "env = Env(rewards)\n",
    "# epsilon, discount, num_episodes\n",
    "epsilon = 0.2\n",
    "discount = 0.9\n",
    "num_episodes = 1000\n",
    "# Trainer\n",
    "trainer = Trainer(env, Q, num_episodes, epsilon, discount)\n",
    "trainer.fit()\n",
    "trainer.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "los Q-values son:\n",
      "[[ 0.       0.     ]\n",
      " [-5.       3.2805 ]\n",
      " [ 2.95245  3.645  ]\n",
      " [ 3.2805   4.05   ]\n",
      " [ 3.645    4.5    ]\n",
      " [ 4.05     5.     ]\n",
      " [ 0.       0.     ]]\n",
      "La mejor acción para el estado 0 es izquierda\n",
      "La mejor acción para el estado 1 es derecha\n",
      "La mejor acción para el estado 2 es derecha\n",
      "La mejor acción para el estado 3 es derecha\n",
      "La mejor acción para el estado 4 es derecha\n",
      "La mejor acción para el estado 5 es derecha\n",
      "La mejor acción para el estado 6 es izquierda\n"
     ]
    }
   ],
   "source": [
    "trainer.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.       0.     ]\n",
      " [-5.       3.2805 ]\n",
      " [ 2.95245  3.645  ]\n",
      " [ 3.2805   4.05   ]\n",
      " [ 3.645    4.5    ]\n",
      " [ 4.05     5.     ]\n",
      " [ 0.       0.     ]]\n"
     ]
    }
   ],
   "source": [
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Método de iteración de valores en práctica</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para terminar la lección vamos a desarrollar un ejemplo completo de los dos métodos descritos. Haremos una implementación completa con el ambiente *FrozenLake*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método para estimar el valor de los estados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Tabla de recompensas**. Un diccionario que tiene una llave compuesta (estado fuente, acción, estado destino). El valor contiene la recompensa inmediata.\n",
    "* **Tabla de transiciones**. Un diccionario que contiene el conteo de las tras transiciones experimentadas por el agente. La clave es (estado, acción) y el valor es otro diccionario que almacena el número de veces que un estado destino ha sido visitado desde el estado fuente. Por ejemplo si la clave es $(0, 1)$, indica estado inicial = 0 y acción = 1.  Si por ejemplo desde el estado 0 se ha ejecutado la acción 1, digamos 10 veces, el valor para esta clave podría ser \\{4: 3, 5: 7 \\} que indicaría que de las 10 veces se llegó en 3 ocasiones al estado 4 y en 7 ocasiones al estado 5.\n",
    "* **Tabla de valores**. Una tabla que almacena el valor calculado para cada estado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Lógica general de la implementación</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Primero se ejecutan 100 pasos aleatorios desde el ambiente. Es decir, se obtienen 100 acciones de forma aleatoria del ambiente. Las tablas de recompensa y transición se poblan a partir de la información recibida del ambiente en estos 100 pasos aleatorios. La función *play_n_random_steps* es la encarga de hacer esta tarea.\n",
    "1. Al finalizar la primera etapa se corre una iteración para actualizar la tabla valor. \n",
    "1. Se ejecutan varios episodios completos usando la tabla actualizada de valor para chequear las mejoras. Si la recompensa promedio para esos episodios de test está por encima de un umbral (digamos 0.8 para el caso de FrozenLake) terminamos el entrenamiento. Durante estos episodios de test se actualizan las tablas de recompensa y transiciones.\n",
    "\n",
    "A continuación describimos los principales componentes de nuestra implementación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la clase *Agent* que contiene las tablas que usaremos y los métodos que realizaran las tareas. El constructor de la clase crea las tablas, el ambiente y la variable que contendrá el estado actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método play_n_random_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la función que genera datos de experiencia para el agente. En la función se generan n (=100) acciones desde el ambiente en forma aleatoria, se poblan y actualizan las tablas de recompensa y transición. Es interesante anotar que el método no espera el final de un episodio. Si llega al final del episodio actual, simplemente inicia le siguiente episodio sin reiniciar las tablas. Esta es una diferencia importante con el método de entropía cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método calc_action_value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método hace el cálculo del valor de la acción $a$  desde el estado $s$.\n",
    "\n",
    "La siguiente imagen ilustra la lógica del cálculo de una acción empezando en el estado $s$. Por facilidad suponemos que solamente se llega a los estados $s_1$ y $s_2$. El cálculo involucra las tablas de transición, recompensa y valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "</figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/state_value.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<figcaption>Cálculo del valor de la acción $Q(s,a)$ en el algoritmo</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Maxim Lapan](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método hace lo siguiente:\n",
    "\n",
    "1. Extrae la información de los contadores incluidos en la tabla de transición. `Counters` es una colección que tiene una forma de diccionario con los estados destinos como clave y el conteo de las veces que el agente llegó allí (desde el estado $s$ con la acción $a$). La función suma todas las frecuencias de Counter para normalizar los conteos y estimar la probabilidad de llegar a a cada estado.\n",
    "1. La función itera sobre todos estos estados destino  que la acción alcanza  y calcula su contribución al valor total de la acción usando la ecuación de Bellman. Esta contribución es: la recompensa inmediata más el valor descontado ($\\gamma V(s_i)$) del estado destino. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método select_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método permite seleccionar la mejor acción a partir de un estado $s$. La acción con mayor ganancia posible es seleccionada. Esta selección es determinista. Este método es de tipo ambicioso (greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método *play_episodio*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método ejecuta un episodio completo usando el correspondiente ambiente. Para no generar un desorden con el actual estado del ambiente definido en la clase, se genera un ambiente de prueba (test) que es pasado al método como argumento. La lógica es muy simple. Ejecuta el ciclo del episodio, seleccionado en cada paso la mejor acción disponible y acumulando la recompensa  del episodio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método value_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro método final y por supuesto más importante, es *value_iteration*. Este método es sorprendentemente simple gracias a las métodos descritos arriba. El método recorre todos los estados del ambiente.\n",
    "\n",
    "Para cada estado calcula el valor para los estados alcanzables a partir de él, obteniendo candidatos para el valor del estado. Al finalizar se actualiza el valor del estado con el máximo valor de la acción disponible desde el estado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ciclo de entrenamiento</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llegamos a la pieza final del código, que es el ciclo de entrenamiento.\n",
    "\n",
    "Antes del ciclo de entrenamiento se ejecutan las siguientes tareas.\n",
    "\n",
    "1. Se instancia un agente, un ambiente (de prueba) y el writer para Tensorboard.\n",
    "1. Se inicializa la mejor recompensa  y el número de iteraciones.\n",
    "\n",
    "Dentro del ciclo se ejecutan las siguientes acciones.\n",
    "\n",
    "1. Se ejecutan n=100 pasos aleatorios para ganar experiencia con el ambiente y poblar las tablas de recompensa y transición con datos frescos.\n",
    "1. Con base en esta información se corre una iteración para calcular el valor de los estados.\n",
    "1. Se corre un conjunto de episodios de prueba (*TEST_EPISODES = 20*) utilizando la tabla de valores actualizada arriba como la política del agente y se escribe en el writer las cantidades que se desea seguir: la recompensa promedio  y el número de iteraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejercicio</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por favor revise, reescriba, corra y analice los resultados. Asegúrese de entender todos los detalles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Clase Agent</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, rewards_collection, transits_collection, values_collection):\n",
    "               \n",
    "        self.rewards = rewards_collection\n",
    "        self.transits = transits_collection\n",
    "        self.values = values_collection\n",
    "        \n",
    "        self.env = env\n",
    "        self.state = self.env.reset()\n",
    "        \n",
    "\n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "            self.state = self.env.reset()  if is_done else new_state\n",
    "\n",
    "    def calc_action_value(self, state, action):\n",
    "        target_counts = self.transits[(state, action)]\n",
    "        total = sum(target_counts.values())\n",
    "        action_value = 0.0\n",
    "        for tgt_state, count in target_counts.items():\n",
    "            reward = self.rewards[(state, action, tgt_state)]\n",
    "            val = reward + GAMMA * self.values[tgt_state]\n",
    "            action_value += (count / total) * val\n",
    "        return action_value\n",
    "\n",
    "    def select_action(self, state):\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_action_value(state, action)\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "\n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            state_values = [\n",
    "                self.calc_action_value(state, action)\n",
    "                for action in range(self.env.action_space.n)\n",
    "            ]\n",
    "            self.values[state] = max(state_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Clase Trainer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "    def __init__(self,  env,  test_env, writer, test_episodes=20, random_episodes=100, epsilon=0.2, discount=0.9):\n",
    "        \n",
    "        self.env = env # ambiente formal para el agente\n",
    "        self.test_env = test_env # ambiente de prueba para correr episodios\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.values = collections.defaultdict(float)\n",
    "        \n",
    "        self.test_episodes = test_episodes\n",
    "       \n",
    "        self.writer = writer\n",
    "        \n",
    "        self.agent = Agent(self.env, self.rewards, self.transits, self.values)\n",
    "     \n",
    "    def fit(self, verbose=True, delta=1):\n",
    "        iter_no = 0\n",
    "        best_reward = 0.0\n",
    "        \n",
    "        while True:\n",
    "            iter_no += 1\n",
    "            self.agent.play_n_random_steps(100)\n",
    "            self.agent.value_iteration()\n",
    "\n",
    "            reward = 0.0\n",
    "            for _ in range(self.test_episodes):\n",
    "                reward += self.agent.play_episode(self.test_env)\n",
    "            reward /= self.test_episodes\n",
    "            writer.add_scalar(\"recompensa\", reward, iter_no)\n",
    "            if reward > best_reward and verbose:\n",
    "                print(\"Actualizada la mejor recompensa %.3f -> %.3f\" % (\n",
    "                    best_reward, reward))\n",
    "                best_reward = reward\n",
    "            if reward > 0.80:\n",
    "                print(\"Resuelto en %d iterations!\" % iter_no)\n",
    "                break\n",
    "    \n",
    "    \n",
    "    \n",
    "    def return_results(self):\n",
    "        return self.rewards, self.transits, self.values\n",
    "     \n",
    "    \n",
    "    def print_results(self):\n",
    "        print('colección de recompensas:')\n",
    "        print(self.rewards)\n",
    "        print('colección de transiciones:')\n",
    "        print(self.transits)\n",
    "        print('colección de valores:')\n",
    "        print(self.values)\n",
    " \n",
    "              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Entrenamiento</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actualizada la mejor recompensa 0.000 -> 0.200\n",
      "Actualizada la mejor recompensa 0.200 -> 0.300\n",
      "Actualizada la mejor recompensa 0.300 -> 0.450\n",
      "Actualizada la mejor recompensa 0.450 -> 0.650\n",
      "Actualizada la mejor recompensa 0.650 -> 0.700\n",
      "Actualizada la mejor recompensa 0.700 -> 0.750\n",
      "Actualizada la mejor recompensa 0.750 -> 0.900\n",
      "Resuelto en 39 iterations!\n",
      "colección de recompensas:\n",
      "defaultdict(<class 'float'>, {(0, 1, 1): 0.0, (1, 0, 1): 0.0, (1, 2, 1): 0.0, (1, 1, 0): 0.0, (0, 0, 4): 0.0, (4, 3, 0): 0.0, (0, 1, 0): 0.0, (0, 0, 0): 0.0, (0, 2, 4): 0.0, (4, 2, 0): 0.0, (0, 1, 4): 0.0, (4, 3, 5): 0.0, (4, 1, 8): 0.0, (8, 2, 4): 0.0, (8, 1, 12): 0.0, (0, 2, 0): 0.0, (1, 3, 1): 0.0, (1, 1, 2): 0.0, (2, 2, 2): 0.0, (2, 3, 3): 0.0, (3, 0, 7): 0.0, (0, 2, 1): 0.0, (1, 0, 0): 0.0, (4, 2, 5): 0.0, (1, 2, 5): 0.0, (4, 1, 4): 0.0, (0, 3, 0): 0.0, (4, 2, 8): 0.0, (8, 2, 12): 0.0, (8, 0, 8): 0.0, (1, 3, 0): 0.0, (8, 3, 9): 0.0, (9, 2, 13): 0.0, (13, 1, 13): 0.0, (13, 2, 13): 0.0, (13, 3, 12): 0.0, (1, 3, 2): 0.0, (2, 0, 6): 0.0, (6, 2, 10): 0.0, (10, 3, 6): 0.0, (6, 0, 2): 0.0, (3, 0, 3): 0.0, (3, 3, 2): 0.0, (2, 1, 1): 0.0, (4, 0, 0): 0.0, (4, 0, 4): 0.0, (4, 0, 8): 0.0, (8, 0, 12): 0.0, (8, 0, 4): 0.0, (1, 0, 5): 0.0, (1, 1, 5): 0.0, (4, 1, 5): 0.0, (0, 3, 1): 0.0, (1, 2, 2): 0.0, (2, 2, 3): 0.0, (3, 1, 3): 0.0, (2, 2, 6): 0.0, (6, 2, 7): 0.0, (2, 0, 1): 0.0, (8, 3, 8): 0.0, (8, 3, 4): 0.0, (4, 3, 4): 0.0, (2, 3, 2): 0.0, (3, 2, 3): 0.0, (3, 0, 2): 0.0, (3, 3, 3): 0.0, (2, 3, 1): 0.0, (2, 1, 6): 0.0, (6, 1, 7): 0.0, (2, 0, 2): 0.0, (6, 3, 5): 0.0, (6, 1, 10): 0.0, (10, 1, 14): 0.0, (14, 2, 10): 0.0, (10, 0, 9): 0.0, (9, 2, 10): 0.0, (10, 2, 11): 0.0, (8, 1, 9): 0.0, (13, 3, 14): 0.0, (14, 1, 15): 1.0, (8, 2, 9): 0.0, (9, 2, 5): 0.0, (6, 0, 5): 0.0, (9, 0, 5): 0.0, (6, 2, 2): 0.0, (6, 0, 10): 0.0, (10, 1, 9): 0.0, (9, 1, 10): 0.0, (10, 1, 11): 0.0, (6, 3, 2): 0.0, (8, 1, 8): 0.0, (3, 1, 2): 0.0, (2, 1, 3): 0.0, (9, 0, 13): 0.0, (13, 3, 9): 0.0, (6, 1, 5): 0.0, (9, 0, 8): 0.0, (9, 1, 8): 0.0, (10, 0, 6): 0.0, (14, 1, 14): 0.0, (14, 1, 13): 0.0, (13, 1, 14): 0.0, (13, 1, 12): 0.0, (10, 3, 11): 0.0, (6, 3, 7): 0.0, (10, 3, 9): 0.0, (13, 2, 9): 0.0, (9, 3, 8): 0.0, (3, 2, 7): 0.0, (3, 1, 7): 0.0, (9, 3, 10): 0.0, (10, 2, 14): 0.0, (14, 3, 10): 0.0, (9, 1, 13): 0.0, (13, 0, 13): 0.0, (13, 0, 12): 0.0, (10, 2, 6): 0.0, (14, 0, 14): 0.0, (14, 3, 13): 0.0, (9, 3, 5): 0.0, (13, 0, 9): 0.0, (13, 2, 14): 0.0, (14, 0, 10): 0.0, (10, 0, 14): 0.0, (14, 0, 13): 0.0, (14, 2, 15): 1.0, (14, 3, 15): 1.0, (14, 2, 14): 0.0})\n",
      "colección de transiciones:\n",
      "defaultdict(<class 'collections.Counter'>, {(0, 1): Counter({1: 247, 4: 245, 0: 223}), (1, 0): Counter({0: 62, 5: 57, 1: 47}), (1, 2): Counter({1: 78, 5: 71, 2: 68}), (1, 1): Counter({5: 66, 0: 58, 2: 50}), (0, 0): Counter({0: 5239, 4: 2499}), (4, 3): Counter({0: 52, 4: 47, 5: 45}), (0, 2): Counter({1: 238, 0: 232, 4: 207}), (4, 2): Counter({0: 66, 8: 60, 5: 53}), (4, 1): Counter({5: 60, 4: 59, 8: 49}), (8, 2): Counter({4: 29, 9: 20, 12: 13}), (8, 1): Counter({8: 30, 9: 18, 12: 14}), (1, 3): Counter({2: 167, 1: 145, 0: 143}), (2, 2): Counter({3: 170, 6: 134, 2: 130}), (2, 3): Counter({3: 33, 2: 20, 1: 15}), (3, 0): Counter({3: 17, 7: 13, 2: 12}), (0, 3): Counter({0: 284, 1: 154}), (8, 0): Counter({8: 122, 12: 117, 4: 110}), (8, 3): Counter({4: 1133, 8: 1119, 9: 1117}), (9, 2): Counter({10: 22, 13: 17, 5: 17}), (13, 1): Counter({14: 35, 12: 35, 13: 32}), (13, 2): Counter({13: 468, 9: 418, 14: 413}), (13, 3): Counter({14: 5, 9: 5, 12: 4}), (2, 0): Counter({1: 44, 6: 42, 2: 32}), (6, 2): Counter({7: 9, 10: 8, 2: 8}), (10, 3): Counter({6: 9, 11: 6, 9: 5}), (6, 0): Counter({5: 70, 10: 67, 2: 56}), (3, 3): Counter({3: 372, 2: 176}), (2, 1): Counter({1: 44, 6: 36, 3: 33}), (3, 1): Counter({3: 12, 2: 12, 7: 11}), (3, 2): Counter({3: 35, 7: 12}), (4, 0): Counter({0: 1945, 8: 1888, 4: 1855}), (5, 0): Counter(), (5, 1): Counter(), (5, 2): Counter(), (5, 3): Counter(), (6, 1): Counter({10: 21, 7: 17, 5: 15}), (6, 3): Counter({7: 10, 5: 8, 2: 2}), (7, 0): Counter(), (7, 1): Counter(), (7, 2): Counter(), (7, 3): Counter(), (9, 0): Counter({13: 7, 8: 5, 5: 3}), (9, 1): Counter({10: 587, 13: 582, 8: 567}), (9, 3): Counter({10: 8, 5: 8, 8: 6}), (10, 0): Counter({6: 66, 9: 58, 14: 56}), (10, 1): Counter({9: 188, 11: 164, 14: 158}), (10, 2): Counter({11: 4, 6: 4, 14: 2}), (11, 0): Counter(), (11, 1): Counter(), (11, 2): Counter(), (11, 3): Counter(), (12, 0): Counter(), (12, 1): Counter(), (12, 2): Counter(), (12, 3): Counter(), (13, 0): Counter({13: 8, 12: 5, 9: 3}), (14, 0): Counter({13: 4, 14: 3, 10: 1}), (14, 1): Counter({14: 346, 15: 334, 13: 313}), (14, 2): Counter({10: 6, 15: 5, 14: 3}), (14, 3): Counter({13: 2, 15: 2, 10: 1}), (15, 0): Counter(), (15, 1): Counter(), (15, 2): Counter(), (15, 3): Counter()})\n",
      "colección de valores:\n",
      "defaultdict(<class 'float'>, {4: 0.08496277503567812, 0: 0.06264529658167053, 1: 0.05735813776138046, 2: 0.07061185593796715, 5: 0.0, 6: 0.10521346697533289, 3: 0.05147140555366795, 7: 0.0, 8: 0.13698513293735826, 10: 0.27243287970534835, 12: 0.0, 9: 0.2340245444432758, 13: 0.368897174784, 11: 0.0, 14: 0.6384400952586692, 15: 0.0})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gym\n",
    "import collections\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# instancia el ambiente formal y ambinbte de prueba \n",
    "# para los episodios del agente \n",
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "#ENV_NAME = \"FrozenLake8x8-v1\"      # descomente para una versión más grande\n",
    "env = gym.make(ENV_NAME)\n",
    "test_env = gym.make(ENV_NAME)\n",
    "\n",
    "# factor de descuento\n",
    "GAMMA = 0.9\n",
    "\n",
    "# número de episodios de prueba\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "# nḿero de episodios aleatorios\n",
    "RANDOM_EPISODES = 100\n",
    "\n",
    "# instancia tensorboard\n",
    "writer = SummaryWriter(comment=\"-v-iteration\")\n",
    "\n",
    "# instancia un entrenador\n",
    "trainer = Trainer(env, test_env, writer, TEST_EPISODES, RANDOM_EPISODES, discount=GAMMA)\n",
    "\n",
    "# corre entrenamiento\n",
    "trainer.fit()\n",
    "# trae resultados\n",
    "rewards, transits, values = trainer.return_results()\n",
    "# imprime resultados\n",
    "trainer.print_results()\n",
    "# cierra el writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Conclusiones</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay varias diferencias notables en relación con el método de entropía cruzada.\n",
    "\n",
    "1. Este método es esencialmente aleatorio en naturaleza. Se corren episodios puramente aleatorios para recoger información acerca de la recompensa y transición entre estados siguiendo distintas acciones.\n",
    "1. No se espera el final de los episodios en la parte de test. Simplemente se va recibido la recompensa de cada episodio, que siempre es 1 o 0 y se continua hasta terminar el ciclo de test.\n",
    "1. El valor de los estados se obtiene a partir de la exploración aleatoria que hace el agente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Método de iteración de acciones en práctica</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método Q-learning tiene pequeños cambios en relación con el método del valor. La siguiente es la relación de cambios.\n",
    "\n",
    "1. El cambio más importante en las estructuras de datos es en la tabla valor. Ahora necesitamos almacenar los valores de la Q función como hicimos antes en el ejemplo simple de la cuadrícula. Ahora el diccionario tiene una clave compuesta (estado, acción).\n",
    "1. La segunda diferencia importante es que ahora el método *calc_action_value* ya no se necesita porque los valores de las acciones se almacenan en la tabla de valores Q.\n",
    "1. Finalmente el cambio más importante es en el método *value_action*. Antes, era solo una envoltura alrededor de la llamada calc_action_value (), que hizo el trabajo de aproximación de Bellman. Ahora, como esta función se ha ido y ha sido reemplazado por una tabla valor (Q), necesitamos hacer la aproximación en el método  *value_iteration*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejercicio</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por favor revise escriba y corra el siguiente código que implementa el método Q-learnig. Identifique los cambios en relación con el código anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.550\n",
      "Best reward updated 0.550 -> 0.650\n",
      "Best reward updated 0.650 -> 0.700\n",
      "Best reward updated 0.700 -> 0.750\n",
      "Best reward updated 0.750 -> 0.900\n",
      "Solved in 25 iterations!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "import collections\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "#ENV_NAME = \"FrozenLake8x8-v0\"      # uncomment for larger version\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "            self.state = self.env.reset() if is_done else new_state\n",
    "\n",
    "    def select_action(self, state):\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.values[(state, action)]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "\n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            for action in range(self.env.action_space.n):\n",
    "                action_value = 0.0\n",
    "                target_counts = self.transits[(state, action)]\n",
    "                total = sum(target_counts.values())\n",
    "                for tgt_state, count in target_counts.items():\n",
    "                    key = (state, action, tgt_state)\n",
    "                    reward = self.rewards[key]\n",
    "                    best_action = self.select_action(tgt_state)\n",
    "                    val = reward + GAMMA * \\\n",
    "                          self.values[(tgt_state, best_action)]\n",
    "                    action_value += (count / total) * val\n",
    "                self.values[(state, action)] = action_value\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    agent = Agent()\n",
    "    writer = SummaryWriter(comment=\"-q-iteration\")\n",
    "\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        agent.play_n_random_steps(100)\n",
    "        agent.value_iteration()\n",
    "\n",
    "        reward = 0.0\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "        reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\", reward, iter_no)\n",
    "        if reward > best_reward:\n",
    "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "            best_reward = reward\n",
    "        if reward > 0.80:\n",
    "            print(\"Solved in %d iterations!\" % iter_no)\n",
    "            break\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejercicio</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemente esta último código como hicimos antes. Implemente las clases Agent y Trainer "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
