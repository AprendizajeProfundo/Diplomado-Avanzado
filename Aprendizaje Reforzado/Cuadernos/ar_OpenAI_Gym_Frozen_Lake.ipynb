{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6f8f0e",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718a9ce",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Introducción al ambiente Gym - Frozen Lake</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c383b39b-7af1-4d0d-8dcc-97809d8dd444",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Survival_struggle_on_the_frozen_lake.jpg\" width=\"600\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente [Survival struggle on the frozen lake. Wiki commons](https://commons.wikimedia.org/w/index.php?search=frozen+lake&title=Special:MediaSearch&go=Go&type=image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71125e52-753b-4496-8a93-a2869b1b2685",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c72733-537d-44bd-a7c8-182d3b2747a3",
   "metadata": {},
   "source": [
    "- Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "- Daniel  Montenegro, Msc, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c3018b-55e3-4acd-96b0-36413cd12086",
   "metadata": {},
   "source": [
    "- Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "- Daniel  Montenegro, Msc, dextronomo@gmail.com \n",
    "- Oleg Jarma, Estadístico, ojarmam@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca023c8-76e4-4ca9-9c74-8640edb4049f",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecf396e-35a1-4315-9c6e-e36f086b541f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec7f62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ac7d2",
   "metadata": {},
   "source": [
    "1. [OpenAI Gym - Github](https://github.com/openai/gym)\n",
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Richard S. Sutton, Andrew G. Barto, Reinforcement learning: an introduction, 2nd edition, 2020](http://library.lol/main/6502B74CE247C4CD4D4FB54747AD7C7E)\n",
    "1. [Praveen Palanisamy - Hands-On Intelligent Agents with OpenAI Gym_ Your Guide to Developing AI Agents Using Deep Reinforcement Learning, 2020](http://library.lol/main/E4FD128CF9B93E0F7A542B053330517A)\n",
    "1. [Turing Paper 1936](http://www.thocp.net/biographies/papers/turing_oncomputablenumbers_1936.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd63d24",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ee4c4",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Una sesión con Forzen Lake](#Una-sesión-con-Frozen-Lake)\n",
    "    * [Observaciones en Frozen Lake](#Observaciones-en-Frozen-Lake)\n",
    "    * [Clase Agent](#Clase-Agent)\n",
    "    * [Ejecución de un episodio](#Ejecución-de-un-episodio)    \n",
    "* [Envolturas para el ambiente (wrappers)](#Envolturas-para-el-ambiente-(wrappers))\n",
    "    * [Envuelve el espacio de acciones](#Envuelve-el-espacio-de-acciones)\n",
    "    * [Corriendo un episodio](#Corriendo-un-episodio)\n",
    "    * [Ejercicio](#Ejercicio)\n",
    "    * [Monitor](#Monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a49d4d-3977-4000-9b78-06cdc1589c7a",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc08e0",
   "metadata": {},
   "source": [
    "La imagen es una representación del ambiente FrozenLake de Gym. En este ambiente, hay 16 posibles posiciones, por lo que el conjunto de observaciones es discreto con 16 valores. El agente siempre comienza en la parte superior izquierda y el objetivo es llegar a la parte inferior derecha.\n",
    "\n",
    "Los sitios marcado con una X son huecos. Un episodio termina si el agente cae en un hueco o si llega al objetivo. En el primer caso (hueco) el agente recibe recompensa cero (0) y en el segundo caso (meta) el agente recibe recompensa uno (1). No importa cuanto se tarde en llegar ya sea a un hueco o a la meta, la recompensa siempre es la misma, es decir, 0 o 1.\n",
    "\n",
    "Las acciones posible del agente son: arriba (up), abajo (down), izquierda (left) y derecha (right). Observe además que según en donde se encuentre el agente, no todas las acciones son posible en el siguiente paso del agente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb416050-c486-47dc-9a9f-041bbb0d2cc8",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Una sesión con Frozen Lake</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a6563-6721-4baf-aade-238d908c9e3c",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/frozenlake.png\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "<center>Ejemplo Frozen Lake en OpenAI Gym</center>\n",
    "\n",
    "Fuente [OpenAI Gym](https://gym.openai.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa16297-0cb2-4c51-b444-a20d6a4263f7",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Observaciones en Frozen Lake</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f22f8d-a004-4d38-8898-74b10c0c850b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observaciones del ambiente Fozen Lake\n",
    "import gym\n",
    "\n",
    "# crea una instancia del ambiente y lo configura en el estado inicial\n",
    "e = gym.make(\"FrozenLake-v1\")\n",
    "obs = e.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e471783-c9ea-4fc6-8a23-68eee6d30394",
   "metadata": {},
   "source": [
    "El ambiente ha regresado una observación que consiste de un valor 0. Este es el valor inicial y corresponde a la posición de la primera celda. Cada celda está identificada con un número entre 0 y 15, ordenados  de izquierda a derecha y de arriba hacia abajo. Así el espacio de observaciones completo es\n",
    "\n",
    "+ 0 1 2 3\n",
    "+ 4 5 6 7\n",
    "+ 8 9 10 11\n",
    "+ 12 13 14 15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b311afa4-2feb-4c99-8a03-721975e3c752",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Clase Agent</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf844e71-2ac3-4c17-81d2-8e0bb591ef98",
   "metadata": {},
   "source": [
    "El constructor de la clase recibe un ambiente con el cual interactuará. Un objeto *Agent* mantiene la cuenta de la recompensa total recibida, el número de pasos que ha ejecutado en el episodioy la actual observación. El método *action()* solicita al ambiente una acción aleatoria. El método *take_action()* solicita una acción y la entrega al ambiente a través del método *step()* del ambiente. El ambiente regresa la lista conteniendo *[observación, recompensa, is_done, info]*. El último elemento de la lista es un diccionario que en este caso viene vacío. Los demás componentes son obvios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c859a4c-4479-4618-b498-f37992486fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Define una clase Agent básica. \n",
    "    lleva la cuenta de la recompensa acumulada\"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.total_reward = 0.0\n",
    "        self.total_steps = 0\n",
    "        self.current_observation = None\n",
    "        self.env = env\n",
    "        \n",
    "        \n",
    "    def actions(self):\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    def take_action(self):\n",
    "        action = self.actions()\n",
    "        observation, reward, is_done, info = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "        self.current_observation = observation\n",
    "        self.total_steps += 1\n",
    "        return observation, reward, is_done, info\n",
    "    \n",
    "    def get_total_reward(self):\n",
    "        return self.total_reward\n",
    "    \n",
    "    def get_current_observation(self):\n",
    "        return self.current_observation\n",
    "    \n",
    "    def get_total_steps(self):\n",
    "        return self.total_steps\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca59228-c451-4e77-b86a-1c179d779fd5",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejecución de un episodio</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b0d6a1-c249-447e-a9ed-ae4b1aeef7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa total recibida: 0.0000\n",
      "Posición final:  11\n",
      "número de pasos en el episodio 5\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# crea el ambiente\n",
    "env = gym.make('FrozenLake-v1')\n",
    "\n",
    "# inicializa el ambiente\n",
    "_ = env.reset()\n",
    "\n",
    "# crea el agente y ejecuta una primera acción\n",
    "agent = Agent(env)\n",
    "_, _, is_done, _ = agent.take_action()\n",
    "    \n",
    "while not is_done:\n",
    "    is_done = agent.take_action()\n",
    "\n",
    "print(\"Recompensa total recibida: %.4f\" % agent.get_total_reward())\n",
    "print(\"Posición final: \", agent.get_current_observation())\n",
    "print(\"número de pasos en el episodio\", agent.get_total_steps())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94082efd-27e8-4603-82dd-1804b4443c0d",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Envolturas para el ambiente (wrappers)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12242952-bec7-43c4-8c88-aebed337c16d",
   "metadata": {},
   "source": [
    "Esta funcionalidad de Gym permite reescribir el comportamiento del ambiente. Observe que la clase *Wrapper* se deriva (envuelve) a la clase Env. Las clases *ObservationWrapper*, *RewardWrapper* y *ActionWrapper*, derivan de Wrapper y permiten extender las respectivas funcionalidades. En general, nosotros implementamos esta clases para hacer modificaciones al comportamiento implementado en el respectivo ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7cfe3-166d-4a2e-995a-4365f55bf6a9",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/wrappers.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "<center>Jerarquía de las clases Wrapper de OpenAI Gym. </center>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30608b1-63a6-4a4d-9bed-044302e215db",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Envuelve el espacio de acciones</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf009690-c095-48d0-a2f7-a64bf76ccf93",
   "metadata": {},
   "source": [
    "Con esta clase se envuelve el espacio de acciones para convertirlo en un espacio de tipo de tal manera que sea compatible con el tipo de espacio de CartPole. El nuevo tipo de *observation* será de  *Box* y contendrá un vector de tamaño 16, de tipo *onehot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3852d3c-37e5-4fe0-84cd-1cab2de4fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        shape = (env.observation_space.n, )\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            0.0, 1.0, shape=shape, dtype=np.float32)\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0351e6f-014e-41d0-a4d1-584a0aec9b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_size =  16\n",
      "n_actions =  4\n",
      "obs =  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# selecciona un ambiente FrozenLake\n",
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\"))\n",
    "\n",
    "# extrae tamaños de acciones y observaciones en el ambiente\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "# \n",
    "\n",
    "print('obs_size = ', obs_size)\n",
    "print('n_actions = ', n_actions)\n",
    "\n",
    "obs = env.reset()\n",
    "print('obs = ', obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8122e61-9d17-467a-a717-a85cf1cf3aa1",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Corriendo un episodio</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ed56e2f-0551-4870-81dd-fc4de3498752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa total recibida: 0.0000\n",
      "Posición final:  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "número de pasos en el episodio 5\n"
     ]
    }
   ],
   "source": [
    "# Crea el ambiente con la envoltura \n",
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\"))\n",
    "\n",
    "# inicializa el ambiente\n",
    "_ = env.reset()\n",
    "\n",
    "# crea el agente y ejecuta una primera acción\n",
    "agent = Agent(env)\n",
    "_, _, is_done, _ = agent.take_action()\n",
    "    \n",
    "while not is_done:\n",
    "    is_done = agent.take_action()\n",
    "\n",
    "print(\"Recompensa total recibida: %.4f\" % agent.get_total_reward())\n",
    "print(\"Posición final: \", agent.get_current_observation())\n",
    "print(\"número de pasos en el episodio\", agent.get_total_steps())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5c719-f4c1-4dae-8fa7-642b807b744b",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejercicio</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb8ad0-fcf2-4f6d-8356-8ec4df9c84e1",
   "metadata": {},
   "source": [
    "Escriba un clase que modifique la recompensa. Ejecute el correspondiente episodio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059a0fe-2b61-4439-83ae-6e312b8eeb80",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Monitor</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab937607-3a7d-4293-9650-bb5287a584ee",
   "metadata": {},
   "source": [
    "Monitor es un tipo de Wrapper que permite monitorar gráficamente un episosio. Por ejemplo, podemos grabar un episodio en formato mp4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277ad1ca-d606-4599-988e-e5e77619aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fdf7f5d-4793-447e-ac89-7e6d66b5dd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa total recibida: 0.0000\n",
      "Posición final:  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "número de pasos en el episodio 2\n"
     ]
    }
   ],
   "source": [
    "# crea el ambiente\n",
    "from gym import wrappers\n",
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\"))\n",
    "env = gym.wrappers.Monitor(env, \"recording\")\n",
    "\n",
    "# inicializa el ambiente\n",
    "_ = env.reset()\n",
    "\n",
    "# crea el agente y ejecuta una primera acción\n",
    "agent = Agent(env)\n",
    "is_done = agent.take_action()\n",
    "    \n",
    "while not is_done:\n",
    "    is_done = agent.take_action()\n",
    "\n",
    "print(\"Recompensa total recibida: %.4f\" % agent.get_total_reward())\n",
    "print(\"Posición final: \", agent.get_current_observation())\n",
    "print(\"número de pasos en el episodio\", agent.get_total_steps())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b60ee12-b0d5-4e7d-bfc6-4eaa641b1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
