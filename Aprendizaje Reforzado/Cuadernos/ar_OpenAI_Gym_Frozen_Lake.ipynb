{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6f8f0e",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718a9ce",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Introducción al ambiente Forzen Lake</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b7a1b-cdd8-434f-b27f-71aeec0cd67f",
   "metadata": {},
   "source": [
    "<center>Jugando con Frozen Lake</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c383b39b-7af1-4d0d-8dcc-97809d8dd444",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/frozenlake.png\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "<center>Ejemplo Frozen Lake en OpenAI Gym</center>\n",
    "\n",
    "Fuente [OpenAI Gym](https://gym.openai.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362eed29-816d-4f5d-b1fe-4c23570ddb96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f44f14-3825-4742-ae59-3073d12b6f9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Coordinador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8f967-f063-4d7a-accd-5396b4a191c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Campo Elías Pardo, PhD, cepardot@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abcc48e-1a20-4c23-81f1-f96c51d60b4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Conferencistas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c3018b-55e3-4acd-96b0-36413cd12086",
   "metadata": {},
   "source": [
    "- Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "- Daniel  Montenegro, Msc, dextronomo@gmail.com \n",
    "- Oleg Jarma, Estadístico, ojarmam@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca023c8-76e4-4ca9-9c74-8640edb4049f",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecf396e-35a1-4315-9c6e-e36f086b541f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816009e5-19af-4952-97bb-5ce03c925563",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ebc986-6c14-49dd-bf63-643292cfb68b",
   "metadata": {},
   "source": [
    "- Nayibe Yesenia Arias, naariasc@unal.edu.co\n",
    "- Venus Celeste Puertas, vpuertasg@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec7f62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ac7d2",
   "metadata": {},
   "source": [
    "1. [OpenAI Gym - Github](https://github.com/openai/gym)\n",
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Richard S. Sutton, Andrew G. Barto, Reinforcement learning: an introduction, 2nd edition, 2020](http://library.lol/main/6502B74CE247C4CD4D4FB54747AD7C7E)\n",
    "1. [Praveen Palanisamy - Hands-On Intelligent Agents with OpenAI Gym_ Your Guide to Developing AI Agents Using Deep Reinforcement Learning, 2020](http://library.lol/main/E4FD128CF9B93E0F7A542B053330517A)\n",
    "1. [Turing Paper 1936](http://www.thocp.net/biographies/papers/turing_oncomputablenumbers_1936.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd63d24",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ee4c4",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Una sesión con CartPole](#Una-sesión-con-CartPole)\n",
    "* [Agente Aleatorio CartPole](#Agente-Aleatorio-CartPole)\n",
    "* [Envolturas para el ambiente (wrappers)](#Envolturas-para-el-ambiente-(wrappers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a49d4d-3977-4000-9b78-06cdc1589c7a",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc08e0",
   "metadata": {},
   "source": [
    "La imagen es una representación del ambiente FrozenLake de Gym. En este ambiente, hay 16 posibles posiciones, por lo que el conjunto de observaciones es discreto con 16 valores. El agente siempre comienza en la parte superior izquierda y el objetivo es llegar a la parte inferior derecha.\n",
    "\n",
    "Los sitios marcado con una X son huecos. Un episodio termina si el agente cae en un hueco o si llega al objetivo. En el primer caso (hueco) el agente recibe recompensa cero (0) y en el segundo caso (meta) el agente recibe recompensa uno (1). No importa cuanto se tarde en llegar ya sea a un hueco o a la meta, la recompensa siempre es la misma, es decir, 0 o 1.\n",
    "\n",
    "Las acciones posible del agente son: arriba (up), abajo (down), izquierda (left) y derecha (right). Observe además que según en donde se encuentre el agente, no todas las acciones son posible en el siguiente paso del agente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb416050-c486-47dc-9a9f-041bbb0d2cc8",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Una sesión con Frozen</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a6563-6721-4baf-aade-238d908c9e3c",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/frozenlake.png\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "<center>Ejemplo Frozen Lake en OpenAI Gym</center>\n",
    "\n",
    "Fuente [OpenAI Gym](https://gym.openai.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa16297-0cb2-4c51-b444-a20d6a4263f7",
   "metadata": {},
   "source": [
    "### Observaciones en FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f22f8d-a004-4d38-8898-74b10c0c850b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observaciones del ambiente CartPole\n",
    "import gym\n",
    "\n",
    "# crea una instancia del ambiente y lo configura en el estado inicial\n",
    "e = gym.make(\"FrozenLake-v1\")\n",
    "obs = e.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e471783-c9ea-4fc6-8a23-68eee6d30394",
   "metadata": {},
   "source": [
    "El ambiente ha regresado una observación que consiste de un valor 0. Este es el valor inicial y corresponde a la posición de la primera celda. Cada celda está identificada con un número entre 0 y 15, ordenados  de izquierda a derecha y de arriba hacia abajo. Así el espacio de observaciones completo es\n",
    "\n",
    "+ 0 1 2 3\n",
    "+ 4 5 6 7\n",
    "+ 8 9 10 11\n",
    "+ 12 13 14 15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c669b7-7f61-41ac-8461-115548b4071c",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Agente Aleatorio FrozenLake</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f450237-c00b-4929-bbe1-53b6238ebd2d",
   "metadata": {},
   "source": [
    "### Clase Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c859a4c-4479-4618-b498-f37992486fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Define una clase Agent básica. \n",
    "    lleva la cuenta de la recompensa acumulada\"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.total_reward = 0.0\n",
    "        self.total_steps = 0\n",
    "        self.current_observation = None\n",
    "        self.env = env\n",
    "        \n",
    "        \n",
    "    def actions(self):\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    def take_action(self):\n",
    "        action = self.actions()\n",
    "        observation, reward, is_done, info = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "        self.current_observation = observation\n",
    "        self.total_steps += 1\n",
    "        return is_done\n",
    "    \n",
    "    def get_total_reward(self):\n",
    "        return self.total_reward\n",
    "    \n",
    "    def get_current_observation(self):\n",
    "        return self.current_observation\n",
    "    \n",
    "    def get_total_steps(self):\n",
    "        return self.total_steps\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca59228-c451-4e77-b86a-1c179d779fd5",
   "metadata": {},
   "source": [
    "### Ejecución de un episodio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b0d6a1-c249-447e-a9ed-ae4b1aeef7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa total recibida: 0.0000\n",
      "Posición final:  11\n",
      "número de pasos en el episodio 5\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# crea el ambiente\n",
    "env = gym.make('FrozenLake-v1')\n",
    "\n",
    "# inicializa el ambiente\n",
    "_ = env.reset()\n",
    "\n",
    "# crea el agente y ejecuta una primera acción\n",
    "agent = Agent(env)\n",
    "is_done = agent.take_action()\n",
    "    \n",
    "while not is_done:\n",
    "    is_done = agent.take_action()\n",
    "\n",
    "print(\"Recompensa total recibida: %.4f\" % agent.get_total_reward())\n",
    "print(\"Posición final: \", agent.get_current_observation())\n",
    "print(\"número de pasos en el episodio\", agent.get_total_steps())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94082efd-27e8-4603-82dd-1804b4443c0d",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Envolturas para el ambiente (wrappers)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12242952-bec7-43c4-8c88-aebed337c16d",
   "metadata": {},
   "source": [
    "Esta funcionalidad de Gym permite reescribir el comportamiento del ambiente. Observe que la clase *Wrapper* se deriva (envuelve) a la clase Env. Las clases *ObservationWrapper*, *RewardWrapper* y *ActionWrapper*, derivan de Wrapper y permiten extender las respectivas funcionalidades. En general, nosotros implementamos esta clases para hacer modificaciones al comportamiento implementado en el respectivo ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7cfe3-166d-4a2e-995a-4365f55bf6a9",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/wrappers.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "<center>Jerarquía de las clases Wrapper de OpenAI Gym. </center>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30608b1-63a6-4a4d-9bed-044302e215db",
   "metadata": {},
   "source": [
    "### Envuelve el espacio de acciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf009690-c095-48d0-a2f7-a64bf76ccf93",
   "metadata": {},
   "source": [
    "Con esta clase se envuelve el epacio de acciones para convertirlo en un espacio de tipo de tal manera que sea compatle con el tipo de espacio de CartPole. El nuevo tipo de observation será de  *Box* y contendrá un vector de tamaño 16, de tipo *onehot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3852d3c-37e5-4fe0-84cd-1cab2de4fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        shape = (env.observation_space.n, )\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            0.0, 1.0, shape=shape, dtype=np.float32)\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0351e6f-014e-41d0-a4d1-584a0aec9b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_size =  16\n",
      "n_actions =  4\n",
      "obs =  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# selecciona un ambiente FrozenLake\n",
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\"))\n",
    "\n",
    "# extrae tamaños de acciones y observaciones en el ambiente\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "# \n",
    "\n",
    "print('obs_size = ', obs_size)\n",
    "print('n_actions = ', n_actions)\n",
    "\n",
    "obs = env.reset()\n",
    "print('obs = ', obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8122e61-9d17-467a-a717-a85cf1cf3aa1",
   "metadata": {},
   "source": [
    "### Corriendo un episodio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ed56e2f-0551-4870-81dd-fc4de3498752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa total recibida: 0.0000\n",
      "Posición final:  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "número de pasos en el episodio 5\n"
     ]
    }
   ],
   "source": [
    "# Crea el ambiente con la envoltura \n",
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\"))\n",
    "\n",
    "# inicializa el ambiente\n",
    "_ = env.reset()\n",
    "\n",
    "# crea el agente y ejecuta una primera acción\n",
    "agent = Agent(env)\n",
    "is_done = agent.take_action()\n",
    "    \n",
    "while not is_done:\n",
    "    is_done = agent.take_action()\n",
    "\n",
    "print(\"Recompensa total recibida: %.4f\" % agent.get_total_reward())\n",
    "print(\"Posición final: \", agent.get_current_observation())\n",
    "print(\"número de pasos en el episodio\", agent.get_total_steps())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5c719-f4c1-4dae-8fa7-642b807b744b",
   "metadata": {},
   "source": [
    "### Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb8ad0-fcf2-4f6d-8356-8ec4df9c84e1",
   "metadata": {},
   "source": [
    "Escriba un clase que modifique la recompensa. Ejecute el correspondiente episodio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059a0fe-2b61-4439-83ae-6e312b8eeb80",
   "metadata": {},
   "source": [
    "### Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab937607-3a7d-4293-9650-bb5287a584ee",
   "metadata": {},
   "source": [
    "Monitor es un tipo de Wrapper que permite monitorar gráficamente un episosio. Por ejemplo, podemos grabar un episodio en formato mp4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277ad1ca-d606-4599-988e-e5e77619aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fdf7f5d-4793-447e-ac89-7e6d66b5dd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa total recibida: 0.0000\n",
      "Posición final:  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "número de pasos en el episodio 2\n"
     ]
    }
   ],
   "source": [
    "# crea el ambiente\n",
    "from gym import wrappers\n",
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\"))\n",
    "env = gym.wrappers.Monitor(env, \"recording\")\n",
    "\n",
    "# inicializa el ambiente\n",
    "_ = env.reset()\n",
    "\n",
    "# crea el agente y ejecuta una primera acción\n",
    "agent = Agent(env)\n",
    "is_done = agent.take_action()\n",
    "    \n",
    "while not is_done:\n",
    "    is_done = agent.take_action()\n",
    "\n",
    "print(\"Recompensa total recibida: %.4f\" % agent.get_total_reward())\n",
    "print(\"Posición final: \", agent.get_current_observation())\n",
    "print(\"número de pasos en el episodio\", agent.get_total_steps())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b60ee12-b0d5-4e7d-bfc6-4eaa641b1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
