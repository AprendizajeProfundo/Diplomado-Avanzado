{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Modelo del bandido multibrazo</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Política $\\epsilon$-voraz </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Tragamonedas.jpeg\" width=\"200\" height=\"200\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-1-el-problema-del-bandido-multibrazo-afe05c0c372e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Coordinador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Campo Elías Pardo, PhD, cepardot@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Conferencistas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "- Daniel  Montenegro, Msc, dextronomo@gmail.com \n",
    "- Oleg Jarma, Estadístico, ojarmam@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nayibe Yesenia Arias, naariasc@unal.edu.co\n",
    "- Venus Celeste Puertas, vpuertasg@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Richard S. Sutton, Andrew G. Barto, Reinforcement learning: an introduction, 2nd edition, 2020](http://library.lol/main/6502B74CE247C4CD4D4FB54747AD7C7E)\n",
    "1. [Praveen Palanisamy - Hands-On Intelligent Agents with OpenAI Gym_ Your Guide to Developing AI Agents Using Deep Reinforcement Learning, 2020](http://library.lol/main/E4FD128CF9B93E0F7A542B053330517A)\n",
    "1. [Turing Paper 1936](http://www.thocp.net/biographies/papers/turing_oncomputablenumbers_1936.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [El problema del bandido multibrazo](#El-problema-del-bandido-multibrazo)\n",
    "* [Política ε-voraz](#Política-ε-voraz)\n",
    "* [Implementación](#Implementación)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprendizaje reforzado (Reinforcement Learning).\n",
    "\n",
    "Ésta es una técnica de inteligencia artificial (IA) en la que un agente inteligente (**agent**) tiene que interactuar con un entorno (**environment**), escogiendo una de las acciones (**action**) que el entorno ofrece en cada uno de los posibles estados (**state**), e intentar conseguir la mayor recompensa (**reward**) posible a través de esas acciones.\n",
    "\n",
    "\n",
    "Al principio, el agente no conoce nada sobre el entorno, por lo que tomará acciones de forma aleatoria. \n",
    "\n",
    "Si una acción trae una recompensa positiva, el agente deberá aprender a escoger esa acción más frecuentemente, mientras que si una acción trae una recompensa negativa, el agente deberá aprender a escoger esa acción menos frecuentemente. \n",
    "\n",
    "Así, el *agente aprenderá a escoger las acciones que maximicen la suma de recompensas recibidas*, también conocida como el retorno (**return**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">El problema del bandido multibrazo </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El problema del bandido multibrazo se puede ver como un problema de máquinas tragamonedas en un casino. \n",
    "\n",
    "Si tenemos un número $N$  de tragamonedas una nos da una recompensa positiva con probabilidad $p$, y ninguna recompensa con probabilidad $(1-p)$, ¿podemos crear un agente que maximice las recompensas escogiendo jugar siempre en la tragamonedas que más beneficio nos vaya a proporcionar? \n",
    "\n",
    "\n",
    "Pues la idea es la misma; tenemos un bandido con $N$ brazos, y cada brazo tiene una probabilidad distinta de darnos una recompensa positiva. El objetivo es crear un agente que maximice esas recompensas.\n",
    "\n",
    "\n",
    "Para esta lección , usaremos un bandido de $N=5$ brazos (5-armed bandit). \n",
    "\n",
    "Éstas serán las probabilidades de cada brazo de dar una recompensa positiva: $[0.1, 0.3, 0.05, 0.55, 0.4]$. \n",
    "\n",
    "\n",
    "Como podemos ver, la mejor acción entre estas cinco es tirar del cuarto brazo. Sin embargo, el agente no dispone de esta información. \n",
    "\n",
    "Por lo tanto, deberá probar a tirar de todos los brazos varias veces, e ir aprendiendo cuál de todos es el mejor. Cuando vaya acumulando más información, empezará a tomar mejores decisiones, y a recibir mejores recompensas más frecuentemente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Tragamonedas.jpeg\" width=\"200\" height=\"200\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-1-el-problema-del-bandido-multibrazo-afe05c0c372e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Política ε-voraz  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ε-greedy policy**\n",
    "\n",
    "Ésta será la política (*policy*) que decidirá qué acciones toma nuestro agente. La política es una función de probabilidad asociada a la siguiente acción que tomará el agente dado que se encuentra en el estado actual $s$.\n",
    "\n",
    "La **política ε-voraz** consiste en que el agente casi siempre tomará la mejor acción posible dada la información que posee. \n",
    "\n",
    "Sin embargo, de vez en cuando, con una **probabilidad de ε, el agente tomará una acción completamente al azar**. \n",
    "\n",
    "De esta forma, si tras la primera acción el agente ha obtenido una recompensa positiva, no se quedará atascado escogiendo esa misma acción todo el tiempo.\n",
    "\n",
    "Con probabilidad $\\epsilon$ el agente explorará otras opciones.\n",
    "\n",
    "Este valor ε lo decidiremos nosotros, y será la forma que tengamos de equilibrar el problema de exploración y explotación (exploration vs. exploitation). \n",
    "\n",
    "La **exploración** consiste en explorar todas las acciones posibles varias veces para ver cuál es la mejor, a pesar de que durante esa exploración no obtengamos recompensas muy buenas. \n",
    "\n",
    "La **explotación** consiste en maximizar las recompensas, por lo que el agente escogerá la mejor de las acciones cada vez. \n",
    "\n",
    "Por ello, es importante **equilibrar la exploración y la explotación**: si solo exploramos dos de las cinco acciones posibles, no sabremos si las acciones que nunca hemos probado nos traerán mayores recompensas, por lo que la exploración es necesaria; y sin embargo, si nos pasamos todo el rato explorando todas las opciones una y otra vez, nunca utilizaremos ese conocimiento para poder escoger la mejor acción y conseguir la mayor recompensa posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Implementación</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos un agente que resuelva el problema del bandido multibrazo. \n",
    "\n",
    "Empecemos con el código. Desarrollamos las siguiente clases\n",
    "\n",
    "1. Jerarquía de clase Action. Esta es una introducción de como vamos a construir mas adelante un modelo para AR que funcione de manear mas o menos genérica. En esta sección se introduce una clase abstracta *Action* para iniciar la jerarquía. De esta clase se derivan las clases *ArgmaxActionArgmaxAction* (no usada en esta lección) y *EpsilonGreedyAction* para implementar la política $\\epsilon$-voraz. \n",
    "1. Clase *Environment*. Es un ejemplo de implementación de un ambiente para el problema del bandido multibrazo. La clase mantiene el vector de probabilidades de recompensa de cada brazo, que vamos a estimar, usando el agente. Observe que el ambiente siempre regresa 4 parámetros como respuesta a una acción, aunque solamente dos de ellos contienen información (paso *step*). Hacemos esto para mantener la convención *[next_obs, reward, is_done, info]*.\n",
    "1. Clase *Agent*. Es la implementación de nuestro agente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la implementación de las clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jerarquía de clases Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union\n",
    "\n",
    "class Action:\n",
    "    \"\"\"\n",
    "    Clase abstracta que convierte puntajes (scores) en acciones.\n",
    "    \"\"\"\n",
    "    def __call__(self, scores):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class ArgmaxAction(Action):\n",
    "    \"\"\"\n",
    "    Selecciona una acción usando argmax\n",
    "    \"\"\"\n",
    "    def __call__(self, scores):\n",
    "        assert isinstance(scores, np.ndarray)\n",
    "        action = np.argmax(scores)\n",
    "        return action\n",
    "\n",
    "\n",
    "class EpsilonGreedyAction(Action):\n",
    "    \"\"\"\n",
    "    Selecciona una acción usando la política epsilon-voraz\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def __call__(self, scores):\n",
    "        assert isinstance(scores, np.ndarray)\n",
    "        result = np.random.uniform()\n",
    "        if result < self.epsilon:\n",
    "            action = np.random.randint(0, len(scores)) \n",
    "        else:\n",
    "            action = np.argmax(scores)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    \"\"\"\n",
    "    Clase Environment para bandido multibrazo\n",
    "    bandits: vector con las probabilidades de entrega de recompensa de cada brazo\n",
    "    num_iteration: número de iteraciones previsto para el entrenamiento\n",
    "    itration: iteración actual del agente (número de pasos)\n",
    "    \"\"\"\n",
    "    def __init__(self, bandits, num_iterations):\n",
    "        self.bandid_prob = bandits\n",
    "        self.num_iterations = num_iterations\n",
    "        self.iteration = 0\n",
    "            \n",
    "    def reset(self):\n",
    "        self.iteration = 0    \n",
    "    \n",
    "    def set_bandits(self, bandits):\n",
    "        self.bandits = bandits\n",
    "        \n",
    "    def is_done(self):\n",
    "        return self.iteration >= self.num_iterations\n",
    "    \n",
    "    def get_reward(self, arm):\n",
    "        # devuelve: 1= recompensa; 0 = sin recompensa)\n",
    "        # con probabilidad bandits[arm] obtienen una recompensa.\n",
    "        result = np.random.uniform()\n",
    "        reward = int(result <= bandits[arm])\n",
    "        return reward\n",
    "    \n",
    "    def get_iteration(self):\n",
    "        return self.iteration\n",
    "        \n",
    "    def step(self, arm: Action):\n",
    "        \"\"\"Tome la action , calcule la recompensa\n",
    "        y verifique is_done\"\"\"\n",
    "        # calula la recompensa\n",
    "        reward = self.get_reward(arm)\n",
    "        # incrementa iteration del episodio\n",
    "        self.iteration += 1\n",
    "        # return [next_obs, reward, is_done, info]\n",
    "        return [None, reward, self.is_done(), None]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, action_selector, env, num_bandits):\n",
    "        self.action =  action_selector\n",
    "        self.env = env\n",
    "        self.total_rewards = np.array([0 for _ in range(num_bandits)])\n",
    "        self.total_attempts = np.array([0 for _ in range(num_bandits)])\n",
    "        self.average_rewards = np.array([0.0 for _ in range(num_bandits)])\n",
    "        \n",
    "    def take_action(self):\n",
    "        action = self.action(self.average_rewards)\n",
    "        _, reward, is_done, _ = env.step(action)\n",
    "        self.update(reward, action)\n",
    "        return  _, reward, is_done, _\n",
    "        \n",
    "    def update(self, reward, action):\n",
    "        # acumula the recompensa para este brazo\n",
    "        self.total_rewards[action] += reward \n",
    "        # acumula los intentos de este brazo\n",
    "        self.total_attempts[action] += 1  \n",
    "        # calcula la recompensa promedio actual para este brazo \n",
    "        self.average_rewards[action] = self.total_rewards[action] / float(self.total_attempts[action])\n",
    "    \n",
    "    def reset(self):\n",
    "        self.total_rewards = [0 for _ in range(num_bandits)]\n",
    "        self.total_attempts = [0 for _ in range(num_bandits)]\n",
    "        self.average_rewards = [0.0 for _ in range(num_bandits)]\n",
    "    \n",
    "    def set_action_selector(self, action_selector):\n",
    "        self.action =  action_selector\n",
    "    \n",
    "    def get_average_rewards(self):\n",
    "        return self.average_rewards\n",
    "    \n",
    "    def get_total_rewards(self):\n",
    "        return self.total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecuta un episodio de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El vector de probabilidades de los bandidos es: [0.1  0.3  0.05 0.55 0.4 ] \n",
      "\n",
      "Avance de la estimación cada 100 iteraciones\n",
      "\n",
      "Las recompensas de los  bandidos en la iteración 100 son ['0.14', '0.24', '0.00', '0.00', '0.00']\n",
      "Las recompensas de los  bandidos en la iteración 200 son ['0.14', '0.27', '0.00', '0.00', '0.00']\n",
      "Las recompensas de los  bandidos en la iteración 300 son ['0.18', '0.29', '0.00', '0.25', '0.27']\n",
      "Las recompensas de los  bandidos en la iteración 400 son ['0.17', '0.29', '0.12', '0.52', '0.29']\n",
      "Las recompensas de los  bandidos en la iteración 500 son ['0.16', '0.29', '0.10', '0.58', '0.32']\n",
      "Las recompensas de los  bandidos en la iteración 600 son ['0.15', '0.29', '0.07', '0.55', '0.30']\n",
      "Las recompensas de los  bandidos en la iteración 700 son ['0.14', '0.29', '0.07', '0.55', '0.26']\n",
      "Las recompensas de los  bandidos en la iteración 800 son ['0.14', '0.29', '0.06', '0.57', '0.28']\n",
      "Las recompensas de los  bandidos en la iteración 900 son ['0.14', '0.29', '0.05', '0.57', '0.27']\n",
      "Las recompensas de los  bandidos en la iteración 1000 son ['0.13', '0.29', '0.05', '0.56', '0.26']\n",
      "\n",
      "El mejor bandido es el número 3 con un promedio observado de recompensa  0.5581\n",
      "La recompensa total observada en los 1000 episodios ha sido 435\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "bandits = np.array([0.1, 0.3, 0.05, 0.55, 0.4])\n",
    "num_iterations = 1000 # episodes\n",
    "epsilon = 0.1\n",
    "verbose = True\n",
    "\n",
    "# seed para reproducibilidad\n",
    "np.random.seed(600)\n",
    "\n",
    "# crea los objetos\n",
    "env = Environment(bandits, num_iterations)\n",
    "action_selector = EpsilonGreedyAction(epsilon)\n",
    "agent = Agent(action_selector=action_selector,\n",
    "              env=env,\n",
    "              num_bandits=bandits.shape[0]\n",
    "             )\n",
    "# empezamos\n",
    "print('El vector de probabilidades de los bandidos es:', bandits,'\\n')\n",
    "\n",
    "if verbose:\n",
    "    print('Avance de la estimación cada 100 iteraciones\\n')\n",
    "\n",
    "is_done = False\n",
    "while not is_done:\n",
    "    # seleccione an action: selecione un brazo\n",
    "    _, reward, is_done, _ = agent.take_action()\n",
    "    # imprime resultados intermedios\n",
    "    if verbose and (env.get_iteration() % 100 == 0):\n",
    "        print('Las recompensas de los  bandidos en la iteración {} son {}'.format(env.get_iteration(),\n",
    "                              ['{:.2f}'.format(elem) for elem in agent.get_average_rewards()]))\n",
    "\n",
    "# imprime resultado finales\n",
    "best_bandit = np.argmax(agent.get_average_rewards())\n",
    "print('\\nEl mejor bandido es el número {} con un promedio observado de recompensa  {:.4f}'\n",
    "  .format(best_bandit, agent.get_average_rewards()[best_bandit]))\n",
    "print('La recompensa total observada en los {} episodios ha sido {}'\n",
    "  .format(env.get_iteration(), sum(agent.get_total_rewards())))\n",
    "    \n",
    "              "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
