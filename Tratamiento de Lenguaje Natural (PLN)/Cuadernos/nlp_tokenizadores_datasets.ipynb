{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc293c4a-2e1e-496d-89e3-ab2755e7ecfc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5725a1-922a-4698-84aa-bd3acde95846",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Hugging Face: Tokenizadores y datasets</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79df48-d239-44d3-87ad-350a9c22db83",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center>T5, Longformers, ST-Moe32B, PaLM </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ea67e-4059-4263-9839-a1a0af4b8645",
   "metadata": {
    "tags": []
   },
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8535f2c-407f-4469-9865-b5a6f3c68627",
   "metadata": {},
   "source": [
    "### Coordinador\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9299cf-5583-4183-a193-3e2409a8f0ac",
   "metadata": {},
   "source": [
    "- Campo El√≠as Pardo Turriago, cepardot@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38821aaf-ab62-4122-896f-ec956f83fa90",
   "metadata": {},
   "source": [
    "### Conferencistas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dcb774-31fc-4f60-a706-0589c5e0ce0b",
   "metadata": {},
   "source": [
    "- Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "- Daniel  Montenegro, Msc, dextronomo@gmail.com \n",
    "- Oleg Jarma, Estad√≠stico, ojarmam@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be563094-f120-4c65-8c32-a4d12f139dfb",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asesora Medios y Marketing digital</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a4fd42-e3b7-4d0c-b5a4-1a5f4ef4263d",
   "metadata": {},
   "source": [
    "- Maria del Pilar Montenegro, pmontenegro88@gmail.com \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ba12e5-1d2d-4b6e-818e-ba5f39f70d9c",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b516d987-273d-4208-9a6e-de1659131a21",
   "metadata": {},
   "source": [
    "- Nayibe Yesenia Arias, naariasc@unal.edu.co\n",
    "- Venus Celeste Puertas, vpuertasg@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccfdb82-6552-41d2-b3e7-684588492c2d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5fae6a-e1d4-4f2e-a39f-1dc5092f86dd",
   "metadata": {},
   "source": [
    "1. [What is Tokenization in NLP? Here‚Äôs All You Need To Know](https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/#:~:text=Hence%2C%20tokenization%20can%20be%20broadly,n%2Dgram%20characters%20tokenization.)\n",
    "1.[Byte Pair Encoding ‚Äî The Dark Horse of Modern NLP](https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10)\n",
    "1. [Aprendizaje Profundo-Diplomado](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [WordPiece: Subword-based tokenization algorithm](https://towardsdatascience.com/wordpiece-subword-based-tokenization-algorithm-1fbd14394ed7)\n",
    "1. [SentencePiece Tokenizer Demystified](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15)\n",
    "1. [Aprendizaje Profundo-PLN](https://github.com/AprendizajeProfundo/PLN)\n",
    "1. [Ashish Vaswani et al.,   Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf), diciembre 2017.\n",
    "1. [Dennis Rothman, Transformers for Natural Language processing](http://libgen.rs/search.php?req=Transformers+for+Natural+Language+processing&open=0&res=25&view=simple&phrase=1&column=def), enero 2021.\n",
    "1.[ Varios,  Dive into deep learning](https://d2l.ai/), enero 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafab141-e901-4884-b13b-2abc2370dfe4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0d29b-d230-4b96-8701-3cd537bbceb6",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "* [Introducci√≥n](#Introducci√≥n)\n",
    "* [Tipos de Tokenizaci√≥n Moderna](#Tipos-de-Tokenizaci√≥n-Moderna)\n",
    "    * [BPE](#BPE)\n",
    "    * [WordPiece](#WordPiece)\n",
    "    * [Unigram](#Unigram)\n",
    "    * [SentencePiece](#SentencePiece)\n",
    "* [Datasets](#SUPERGLUE-Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4d687-0d55-4387-981d-c566df2f6297",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducci√≥n</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51c3de-15d8-44bc-9e88-34f0e8cbab1e",
   "metadata": {},
   "source": [
    "Como hemos visto en lecciones anteriores, la tokenizaci√≥n juega un papel fundamental cuando se trabaja en NLP o NLU.\n",
    "\n",
    "Recordemos que tokenizar signfica segmentar el texto en partes fundamentales (**tokens**).\n",
    "\n",
    "*Los Tokens son los bloques de construcci√≥n del lenguaje natural.*\n",
    "\n",
    "**¬øQu√© signfica una parte fundamental?**\n",
    "\n",
    "La respuesta a esta pregunta puede variar:\n",
    "\n",
    "- Palabras (Word Tokenization)\n",
    "- Caracteres (Character Tokenization)\n",
    "- Subpalabras (Subword Tokenization)\n",
    "\n",
    "Cada una de las tres tiene sus ventajas y sus desventajas.\n",
    "\n",
    "Por un lado, una tokenizaci√≥n por palabras puede guardar informaci√≥n relevante sem√°ntica del texto que se est√° analizando, pero tenemos el problema de tener vocabularios con un grande e imanegable n√∫mero de tokens.\n",
    "\n",
    "Note por ejemplo, la palabra *jugar*. \n",
    "\n",
    "Algunas variantes son: *jugu√©, jugar√©, jugaremos, juguemos, ...*.\n",
    "\n",
    "Esto hace dif√≠cil el manejo de textos, pues si en textos nuevos que vea un modelo de NLP aparecen estas variantes y no se encuentran en el vocabulario con el que fue entrenado el modelo, se considerar√°n **OOV** (Out Of Vocabulary) y se perder√° informaci√≥n relevante al momento de hacer predicciones o clasificaci√≥n.\n",
    "\n",
    "Como ejemplo, recordemos que tokenizar una frase por palabras puede ser de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab3ff528-6def-4b1c-a399-6e7d3198ed5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Los',\n",
       " 'pajaritos',\n",
       " 'vuelan',\n",
       " ',',\n",
       " 'y',\n",
       " 'las',\n",
       " 'nubes',\n",
       " 'se',\n",
       " 'enlazan',\n",
       " 'con',\n",
       " 'un',\n",
       " 'azul',\n",
       " 'infinito',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "#String\n",
    "texto = 'Los pajaritos vuelan, y las nubes se enlazan con un azul infinito.'\n",
    "# Separar puntos y comas\n",
    "texto = texto.translate(str.maketrans({'.': ' .', ',': ' ,'}))\n",
    "# Dividir por espacios\n",
    "tokens = texto.split()\n",
    "# Ver tokenizaci√≥n\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c056a26-74e0-49e7-ac59-d72bbca4068f",
   "metadata": {},
   "source": [
    "Como vemos, si un modelo se entrenara con esta √∫nica frase y de repente le entraramos la palabra *nubes*, el modelo no podr√≠a indentificar dicho token como relacionado con *nube* y le dar√≠a una etiqueta especial **UNK** (Unknown).\n",
    "\n",
    "Esto no es √≥ptimo. Sin embargo, este tipo de tokenizaci√≥n se utiliza en **Word2Vec** y **GloVe**.\n",
    "\n",
    "Por otro lado, la tokenizaci√≥n por caracteres, si bien resuelve este problema de **OOV**, los tokens individuales carecen de significado por si s√≥lo, lo que har√≠a que el modelo pasara mal rato tratando de hacer tareas complicadas del lenguaje, como por ejemplo ahcer inferencias a partir de una frase. \n",
    "\n",
    "Adem√°s, la longitud de las secuencias de input del modelo se volver√°n r√°pidamente muy extensas, lo que har√≠a que el modelo tuviese que hacer una gran cantidad (en algunos casos imposible) de c√°lculos.\n",
    "\n",
    "Recordemos c√≥mo tokenizar por caracteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9431ed-7dc9-45b4-83c5-4b54d57488e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L|o|s| |p|a|j|a|r|i|t|o|s| |v|u|e|l|a|n| |,| |y| |l|a|s| |n|u|b|e|s| |s|e| |e|n|l|a|z|a|n| |c|o|n| |u|n| |a|z|u|l| |i|n|f|i|n|i|t|o| |.|"
     ]
    }
   ],
   "source": [
    "c_tokens = [t for t in texto]\n",
    "for c in c_tokens:\n",
    "    print(c,end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181649e-59d7-43c5-9155-45a4adca8594",
   "metadata": {},
   "source": [
    "Ante este tipo de dificultades, podr√≠amos preguntarnos qu√© pasar√≠a en un mundo intermedio, donde no nos quedemos con las palabras completas, pero tampoco simplifiquemos hasta el mundo de caracteres.\n",
    "\n",
    "Bienvenidos a la tokenizaci√≥n por subpalabras.\n",
    "\n",
    "En particular, el uso de n-gramas fue potenciado por Mikolov T. en su desarrollo de **FastText** en [Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf).\n",
    "\n",
    "Veamos a profundidad tipos de tokenizaci√≥n utilizados en los modelos modernos (Transformers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb344c0-b72c-4daf-9d65-84cd334ef68a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Tipos de Tokenizaci√≥n Moderna</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e85fe-ee2d-40d9-864d-c8d5720b636a",
   "metadata": {},
   "source": [
    "El fin √∫ltimo de la tokenizaci√≥n es la construcci√≥n del vocabulario que usar√° el modelo para convertir texto en n√∫meros.\n",
    "\n",
    "Un vocabulario eficiente no preservar√° toda la informaci√≥n de los tokens, sino los m√°s frecuentes seg√∫n alg√∫n umbral.\n",
    "\n",
    "Una tokenizaci√≥n por subpalabras, se puede entender de la siguiente manera:\n",
    "\n",
    "- Jugaremos -> Jugar-emos\n",
    "- Jugar√°s   -> Jugar-√°s\n",
    "\n",
    "Como podemos notar, esto puede reducir en gran parte el tama√±o del vocabulario a la vez que se preservan significados de palabras en su forma b√°sica.\n",
    "\n",
    "Todos los modelos modernos de Transformers utlizan alguna forma de tokenizaci√≥n por subpalabras.\n",
    "\n",
    "Veamos algunos de ellos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9baa571-9c18-4d72-85ad-3904ef3a5a54",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### <span style=\"color:blue\">BPE</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0108b11f-f23a-4923-99fb-032fb6844fd5",
   "metadata": {},
   "source": [
    "**BPE** viene de Byte-Pair Encoding, y se encarga de segmentar palabras **OOV** como subpalabras, y representar dichas palabras con tokens de las subpalabras.\n",
    "\n",
    "El procedimiento de BPE es como sigue:\n",
    "\n",
    "1. Divide las palabras del corpus en caracteres despu√©s de agregar \\</w\\>.\n",
    "1. Inicializar el vocabulario con caracteres √∫nicos en el corpus.\n",
    "1. Calcule la frecuencia de un par de caracteres o secuencias de caracteres en corpus.\n",
    "1. Combinar el par m√°s frecuente en el corpus.\n",
    "1. Guarda la mejor pareja para el vocabulario.\n",
    "1. Repita los pasos 3 a 5 para un cierto n√∫mero de iteraciones.\n",
    "\n",
    "Entendamos los pasos anteriores con un ejemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0804863-e678-4d71-b9e3-8369885c5833",
   "metadata": {},
   "source": [
    "||Corpus||\n",
    "|-|-|-|\n",
    "|low|lower|newest|\n",
    "|low|lower|newest|\n",
    "|low|widest|newest|\n",
    "|low|widest|newest|\n",
    "|low|widest|newest|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc54959-9440-4cb3-a5cf-5e0ee53a0b13",
   "metadata": {},
   "source": [
    "1. Divide las palabras del corpus en caracteres despu√©s de agregar \\</w\\>:\n",
    "\n",
    "||Corpus||\n",
    "|-|-|-|\n",
    "|low\\</w\\>|lower\\</w\\>|newest\\</w\\>|\n",
    "|low\\</w\\>|lower\\</w\\>|newest\\</w\\>|\n",
    "|low\\</w\\>|widest\\</w\\>|newest\\</w\\>|\n",
    "|low\\</w\\>|widest\\</w\\>|newest\\</w\\>|\n",
    "|low\\</w\\>|widest\\</w\\>|newest\\</w\\>|\n",
    "\n",
    "||Corpus||\n",
    "|-|-|-|\n",
    "|l o w \\</w\\>|l o w e r \\</w\\>|n e w e s t \\</w\\>|\n",
    "|l o w \\</w\\>|l o w e r\\</w\\>|n e w e s t \\</w\\>|\n",
    "|l o w \\</w\\>|w i d e s t \\</w\\>|n e w e s t \\</w\\>|\n",
    "|l o w \\</w\\>|w i d e s t \\</w\\>|n e w e s t \\</w\\>|\n",
    "|l o w \\</w\\>|w i d e s t \\</w\\>|n e w e s t \\</w\\>|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8313c768-60d4-4218-a793-5e91a6300db3",
   "metadata": {},
   "source": [
    "2. Inicializar el vocabulario con caracteres √∫nicos en el corpus.\n",
    "\n",
    "|||||||||||\n",
    "|-|-|-|-|-|-|-|-|-|-|\n",
    "|d|e|i|l|n|o|r|s|t|w|\n",
    "|-|-|-|-|-|-|-|-|-|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd45f172-6b79-4f7d-b23b-152ecaf66213",
   "metadata": {},
   "source": [
    "**Iteraci√≥n 1:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec1851-ecab-470f-a9a4-d161d231beee",
   "metadata": {},
   "source": [
    "3. Calcule la frecuencia de un par de caracteres o secuencias de caracteres en corpus.\n",
    "\n",
    "||||\n",
    "|-|-|-|\n",
    "|d-e (3)|l-o (7)|t-\\</w\\> (8)|\n",
    "|e-r (2)|n-e (5)|w-\\</w\\> (5)|\n",
    "|**e-s (8)**|o-w (7)|w-e (7)|\n",
    "|e-w (5)|r-\\</w\\> (2)|w-i (3)|\n",
    "|i-d (3)|s-t (8)||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a1d234-eb39-4b96-939c-0155732aaf53",
   "metadata": {},
   "source": [
    "4. Combinar el par m√°s frecuente en el corpus.\n",
    "\n",
    "||Corpus||\n",
    "|-|-|-|\n",
    "|l o w \\</w\\>|l o w e r \\</w\\>|n e w **es** t \\</w\\>|\n",
    "|l o w \\</w\\>|l o w e r\\</w\\>|n e w **es** t \\</w\\>|\n",
    "|l o w \\</w\\>|w i d **es** t \\</w\\>|n e w **es** t \\</w\\>|\n",
    "|l o w \\</w\\>|w i d **es** t \\</w\\>|n e w **es** t \\</w\\>|\n",
    "|l o w \\</w\\>|w i d **es** t \\</w\\>|n e w **es** t \\</w\\>|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31435196-91f7-4ac8-a4c2-324a80622f70",
   "metadata": {},
   "source": [
    "5. Guarda la mejor pareja para el vocabulario.\n",
    "\n",
    "|||||||||||\n",
    "|-|-|-|-|-|-|-|-|-|-|\n",
    "|d|e|i|l|n|o|r|s|t|w|\n",
    "|**es**|-|-|-|-|-|-|-|-|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e22fb1-487e-49be-98c7-b133af426dbe",
   "metadata": {},
   "source": [
    "**Iteraci√≥n 2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e45cdd7-7a29-4202-96ef-5ac9a8f2f1ca",
   "metadata": {},
   "source": [
    "3. Calcule la frecuencia de un par de caracteres o secuencias de caracteres en corpus.\n",
    "\n",
    "||||\n",
    "|-|-|-|\n",
    "|d-es (3)|l-o (7)|w-\\</w\\> (5)|\n",
    "|e-r (2)|n-e (5)|w-es (5)|\n",
    "|e-w (5)|o-w (7)|w-e (7)|\n",
    "|**es-t (8)**|r-\\</w\\> (2)|w-i (3)|\n",
    "|i-d (3)|t-\\</w\\> (8)||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c4f17-f30b-404d-a49a-3b9f57771b91",
   "metadata": {},
   "source": [
    "4. Combinar el par m√°s frecuente en el corpus.\n",
    "\n",
    "||Corpus||\n",
    "|-|-|-|\n",
    "|l o w \\</w\\>|l o w e r \\</w\\>|n e w **est**  \\</w\\>|\n",
    "|l o w \\</w\\>|l o w e r\\</w\\>|n e w **est**  \\</w\\>|\n",
    "|l o w \\</w\\>|w i d **est**  \\</w\\>|n e w **est**  \\</w\\>|\n",
    "|l o w \\</w\\>|w i d **est**  \\</w\\>|n e w **est** \\</w\\>|\n",
    "|l o w \\</w\\>|w i d **est**  \\</w\\>|n e w **est**  \\</w\\>|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5114b0a0-beae-4b9a-9885-3d0f5845f28d",
   "metadata": {},
   "source": [
    "5. Guarda la mejor pareja para el vocabulario.\n",
    "\n",
    "|||||||||||\n",
    "|-|-|-|-|-|-|-|-|-|-|\n",
    "|d|e|i|l|n|o|r|s|t|w|\n",
    "|**es**|**est**|-|-|-|-|-|-|-|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f0db72-2e8f-47cd-b977-125d1cfd64ab",
   "metadata": {},
   "source": [
    "**Despu√©s de 10 iteraciones, obtenemos:**\n",
    "\n",
    "|||||||||||\n",
    "|-|-|-|-|-|-|-|-|-|-|\n",
    "|d|e|i|l|n|o|r|s|t|w|\n",
    "|**es**|**est**|**est\\</w\\>**|**lo**|**low**|**low\\</w\\>**|**ne**|**new**|**newest\\</w\\>**|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d17a0e-865c-49bb-be84-871479ee8a74",
   "metadata": {},
   "source": [
    "En el momento de test, las palabras **OOV** se divide en secuencias de caracteres. \n",
    "\n",
    "Luego, las operaciones aprendidas se aplican para fusionar los caracteres en s√≠mbolos conocidos m√°s grandes.\n",
    "\n",
    "Para mayor informaci√≥n, se puede referir a [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf), Senrich R, et al., 2016.\n",
    "\n",
    "Aqu√≠ hay un procedimiento paso a paso para representar palabras OOV:\n",
    "\n",
    "1. Divide la palabra OOV en caracteres despu√©s de agregar \\</w\\>\n",
    "1. Calcular un par de caracteres o secuencias de caracteres en una palabra\n",
    "1. Seleccionar los pares presentes en las operaciones aprendidas\n",
    "1. Combinar el par m√°s frecuente\n",
    "1. Repita los pasos 2 y 3 hasta que sea posible la fusi√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2285e2-57e1-4bbf-aa88-81b3f5928a5e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### <span style=\"color:blue\">Implementaci√≥n</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1434f3a7-f755-4584-a01b-faa91c9b9609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing library\n",
    "import pandas as pd\n",
    "\n",
    "#reading .txt file\n",
    "text = pd.read_csv(\"../Datos/sample.txt\")\n",
    "\n",
    "#converting a dataframe into a single list \n",
    "corpus=[]\n",
    "for row in text.values:\n",
    "    # divide line text by spaces\n",
    "    tokens = row[0].split(\" \")\n",
    "    for token in tokens:\n",
    "        # append each token found\n",
    "        corpus.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b4d35578-8654-4040-b7c5-36c3f2797a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Textos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>low lower newest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>low lower newest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>low widest newest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>low widest newest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>low widest newest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Textos\n",
       "0   low lower newest\n",
       "1   low lower newest\n",
       "2  low widest newest\n",
       "3  low widest newest\n",
       "4  low widest newest"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "11855ed2-86a2-4c5b-bc11-6fb7afd47f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['low',\n",
       " 'lower',\n",
       " 'newest',\n",
       " 'low',\n",
       " 'lower',\n",
       " 'newest',\n",
       " 'low',\n",
       " 'widest',\n",
       " 'newest',\n",
       " 'low',\n",
       " 'widest',\n",
       " 'newest',\n",
       " 'low',\n",
       " 'widest',\n",
       " 'newest']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d51e789b-49d8-429d-b83a-ebea41c9dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initlialize the vocabulary\n",
    "vocab = list(set(\" \".join(corpus)))\n",
    "vocab.remove(' ')\n",
    "\n",
    "#split the word into characters\n",
    "corpus = [\" \".join(token) for token in corpus]\n",
    "\n",
    "#appending </w>\n",
    "corpus=[token+' </w>' for token in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "73512434-09d2-4834-95f9-ae69938bbce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w', 'o', 'e', 'l', 'r', 's', 't', 'i', 'd', 'n']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "75e1e406-0b3c-4040-b462-13d13bb1b98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l o w </w>',\n",
       " 'l o w e r </w>',\n",
       " 'n e w e s t </w>',\n",
       " 'l o w </w>',\n",
       " 'l o w e r </w>',\n",
       " 'n e w e s t </w>',\n",
       " 'l o w </w>',\n",
       " 'w i d e s t </w>',\n",
       " 'n e w e s t </w>',\n",
       " 'l o w </w>',\n",
       " 'w i d e s t </w>',\n",
       " 'n e w e s t </w>',\n",
       " 'l o w </w>',\n",
       " 'w i d e s t </w>',\n",
       " 'n e w e s t </w>']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c7794619-0ec9-43ae-9f3f-8a9477adc2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 5, 'w i d e s t </w>': 3}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "#returns frequency of each word\n",
    "corpus = collections.Counter(corpus)\n",
    "\n",
    "#convert counter object to dictionary\n",
    "corpus = dict(corpus)\n",
    "print(\"Corpus:\",corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af068f9d-8400-4fce-8fa4-7411ec988405",
   "metadata": {},
   "source": [
    "La funci√≥n a continuaci√≥n cuenta la frecuencia de los bigramas de caracteres de cada palabra en el corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7d907f04-5771-489b-9cdf-968a4be98ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computer frequency of a pair of characters or character sequences\n",
    "#accepts corpus and return frequency of each pair\n",
    "def get_stats(corpus):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in corpus.items():\n",
    "        # divide string into characters\n",
    "        symbols = word.split()\n",
    "        # count frequencies of pairs\n",
    "        for i in range(len(symbols)-1):\n",
    "            # look the pairs and count\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b77eaf4-d58c-4764-9b3e-9bb66838fdd7",
   "metadata": {},
   "source": [
    "Finalmente, combinamos los pares m√°s frecuentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b510118b-aeef-4eeb-b8f7-8114f6de497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merges the most frequent pair in the corpus\n",
    "#accepts the corpus and best pair\n",
    "#returns the modified corpus \n",
    "import re\n",
    "def merge_vocab(pair, corpus_in):\n",
    "    corpus_out = {}\n",
    "    # Ignore special characters\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    # Merge the sequence into the corpus\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    for word in corpus_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        corpus_out[w_out] = corpus_in[word]\n",
    "    \n",
    "    return corpus_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2e4df641-0153-4cc3-85aa-dcc1c8f0ccec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('l', 'o'): 7,\n",
       "             ('o', 'w'): 7,\n",
       "             ('w', '</w>'): 5,\n",
       "             ('w', 'e'): 7,\n",
       "             ('e', 'r'): 2,\n",
       "             ('r', '</w>'): 2,\n",
       "             ('n', 'e'): 5,\n",
       "             ('e', 'w'): 5,\n",
       "             ('e', 's'): 8,\n",
       "             ('s', 't'): 8,\n",
       "             ('t', '</w>'): 8,\n",
       "             ('w', 'i'): 3,\n",
       "             ('i', 'd'): 3,\n",
       "             ('d', 'e'): 3})"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute frequency of bigrams in a corpus\n",
    "pairs = get_stats(corpus)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "647a0948-9a37-4479-8742-82bfa7aa0cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Frequent pair: ('e', 's')\n"
     ]
    }
   ],
   "source": [
    "#compute the best pair\n",
    "best = max(pairs, key=pairs.get)\n",
    "print(\"Most Frequent pair:\",best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7938ea30-0e83-4c1f-a3d9-d3a8815519b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Merging: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 5, 'w i d es t </w>': 3}\n"
     ]
    }
   ],
   "source": [
    "#merge the frequent pair in corpus\n",
    "corpus = merge_vocab(best, corpus)\n",
    "print(\"After Merging:\", corpus)\n",
    "\n",
    "#convert a tuple to a string\n",
    "best = \"\".join(list(best))\n",
    "\n",
    "#append to merge list and vocabulary\n",
    "merges = []\n",
    "merges.append(best)\n",
    "vocab.append(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ef67f663-2c61-4888-8f50-2852dfbe9c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Merge Operations: ['es', 'est', 'est</w>', 'lo', 'low', 'low</w>', 'ne', 'new', 'newest</w>', 'wi', 'wid']\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    \n",
    "    #compute frequency of bigrams in a corpus\n",
    "    pairs = get_stats(corpus)\n",
    "    \n",
    "    #compute the best pair\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    \n",
    "    #merge the frequent pair in corpus\n",
    "    corpus = merge_vocab(best, corpus)\n",
    "    \n",
    "    #append to merge list and vocabulary\n",
    "    merges.append(best)\n",
    "    vocab.append(best)\n",
    "\n",
    "#convert a tuple to a string\n",
    "merges_in_string = [\"\".join(list(i)) for i in merges]\n",
    "print(\"BPE Merge Operations:\",merges_in_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c3a51ed1-cfa8-4274-9ab4-b3e1153783e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w',\n",
       " 'o',\n",
       " 'e',\n",
       " 'l',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'i',\n",
       " 'd',\n",
       " 'n',\n",
       " 'es',\n",
       " ('es', 't'),\n",
       " ('est', '</w>'),\n",
       " ('l', 'o'),\n",
       " ('lo', 'w'),\n",
       " ('low', '</w>'),\n",
       " ('n', 'e'),\n",
       " ('ne', 'w'),\n",
       " ('new', 'est</w>'),\n",
       " ('w', 'i'),\n",
       " ('wi', 'd')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9ee7dd7d-2dec-4f2f-a41d-15b7958a567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying BPE to OOV\n",
    "oov ='lowest'\n",
    "\n",
    "#tokenize OOV into characters\n",
    "oov = \" \".join(list(oov))\n",
    "\n",
    "#append </w> \n",
    "oov = oov + ' </w>'\n",
    "\n",
    "#create a dictionary\n",
    "oov = { oov : 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "096e080a-dd3e-480a-8433-e7391be9cb27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l o w e s t </w>': 1}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "31a782c0-8890-4642-9ce8-9333a1ac105a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "Iteration  1 lo w e s t </w>\n",
      "[4]\n",
      "Iteration  2 low e s t </w>\n",
      "[]\n",
      "\n",
      "BPE Completed...\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "while(True):\n",
    "\n",
    "    #compute frequency\n",
    "    pairs = get_stats(oov)\n",
    "\n",
    "    #extract keys\n",
    "    pairs = pairs.keys()\n",
    "    \n",
    "    #find the pairs available in the learned operations\n",
    "    ind=[merges.index(i) for i in pairs if i in merges]\n",
    "    #print(ind)\n",
    "    if(len(ind)==0):\n",
    "        print(\"\\nBPE Completed...\")\n",
    "        break\n",
    "    \n",
    "    #choose the most frequent learned operation\n",
    "    best = merges[min(ind)]\n",
    "    \n",
    "    #merge the best pair\n",
    "    oov = merge_vocab(best, oov)\n",
    "    \n",
    "    print(\"Iteration \",i+1, list(oov.keys())[0])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c142e107-be66-4f60-be7d-e22fd70f333f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([('l', 'o'), ('o', 'w'), ('w', 'e'), ('e', 's'), ('s', 't'), ('t', '</w>')])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = get_stats(oov)\n",
    "pairs = pairs.keys()\n",
    "print(pairs)\n",
    "ind=[merges.index(i) for i in pairs if i in merges]\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e8e02334-77b5-4051-938d-bf50295dfd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['es',\n",
       " ('es', 't'),\n",
       " ('est', '</w>'),\n",
       " ('l', 'o'),\n",
       " ('lo', 'w'),\n",
       " ('low', '</w>'),\n",
       " ('n', 'e'),\n",
       " ('ne', 'w'),\n",
       " ('new', 'est</w>'),\n",
       " ('w', 'i'),\n",
       " ('wi', 'd')]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8f6da-50fe-47be-af14-2a4ebcc2d6e6",
   "metadata": {},
   "source": [
    "Modelos como GPT-2, RoBERTa, XLM y FlauBERT usan BPE para su tokenizaci√≥n.\n",
    "\n",
    "En las siguientes lecciones veremos estos tipos de modelos a profundidad (Decoder-Only)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d823704b-ad48-4b9d-a479-afd214fcfd67",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\">WordPiece</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4f728f-c0ab-4eb8-a43f-902277996f9a",
   "metadata": {},
   "source": [
    "Modelos como BERT, DistilBERT, Electra usan este tipo de tokenizaci√≥n.\n",
    "\n",
    "Actualmente, hay dos implementaciones del algoritmo de WordPiece: de abajo hacia arriba y de arriba hacia abajo. \n",
    "\n",
    "El enfoque ascendente original se basa en BPE. \n",
    "\n",
    "BERT utiliza la implementaci√≥n de arriba hacia abajo de WordPiece.\n",
    "\n",
    "Sus comienzos datan de [Japanese and Korean Voice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf), y gan√≥ popularidad gracias a BERT.\n",
    "\n",
    "Recordemos, BPE toma un par de tokens (bytes), observa la frecuencia de cada par y fusiona el par que tiene la frecuencia combinada m√°s alta. \n",
    "\n",
    "El proceso es codicioso (greedy), ya que busca la frecuencia combinada m√°s alta en cada paso.\n",
    "\n",
    "Entonces, ¬øcu√°l es el problema con BPE? ü§î \n",
    "\n",
    "Puede tener instancias en las que hay m√°s de una forma de codificar una palabra en particular. \n",
    "\n",
    "Entonces se vuelve dif√≠cil para el algoritmo elegir tokens de subpalabras ya que no hay forma de priorizar cu√°l usar primero. \n",
    "\n",
    "Por lo tanto, la misma entrada puede representarse mediante diferentes codificaciones que afectan la precisi√≥n de las representaciones aprendidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b1cf94-0886-4921-b4b6-cf50146705c6",
   "metadata": {},
   "source": [
    "Miremos el siguiente vocabulario para un corpus peque√±o:\n",
    "\n",
    "|N√∫mero|Token|Frecuencia|\n",
    "|-|-|-|\n",
    "|1|li|3|\n",
    "|2|l|5|\n",
    "|3|ea|2|\n",
    "|4|eb|4|\n",
    "|5|near|6|\n",
    "|6|bra|2|\n",
    "|7|al|4|\n",
    "|8|n|5|\n",
    "|9|r|1|\n",
    "|10|ge|11|\n",
    "|11|g|7|\n",
    "|12|e|8|\n",
    "\n",
    "SI queremos tokenizar la frase **linear algebra**, podemos hacer lo siguiente:\n",
    "\n",
    "linear = **li**+**near** √≥ **li** + **n** + **ea** + **r**\n",
    "\n",
    "algebra = **al** + **ge** + **bra** √≥ **al** + **g** + **e** + **bra**\n",
    "\n",
    "Como vemos, tenemos cuatro formas de poder codificar la frase, lo cual representa un problema para los modelos.\n",
    "\n",
    "En la ciencia de datos, cuando avanzamos un paso por delante de la frecuencia o el conteo, buscamos enfoques probabil√≠sticos. \n",
    "\n",
    "En WordPiece, en realidad hacemos lo mismo. La √∫nica diferencia entre WordPiece y BPE es la forma en que se agregan los pares de s√≠mbolos al vocabulario. \n",
    "\n",
    "En cada paso iterativo, WordPiece elige un par de s√≠mbolos que resultar√° en el mayor aumento de probabilidad al fusionarse. \n",
    "\n",
    "Maximizar la probabilidad de los datos de entrenamiento es equivalente a encontrar el par de s√≠mbolos cuya probabilidad dividida por la probabilidad del primero seguido por la probabilidad del segundo s√≠mbolo en el par es mayor que cualquier otro par de s√≠mbolos.\n",
    "\n",
    "Por ejemplo, el algoritmo verificar√° si la probabilidad de ocurrencia de ‚Äúes‚Äù es mayor que la probabilidad de ocurrencia de ‚Äúe‚Äù seguida de ‚Äús‚Äù. \n",
    "\n",
    "La fusi√≥n ocurrir√° solo si la probabilidad de \"es\" dividida por \"e\", \"s\" es mayor que cualquier otro par de s√≠mbolos.\n",
    "\n",
    "As√≠, podemos decir que WordPiece eval√∫a lo que perder√° fusionando los dos s√≠mbolos para asegurarse de que el paso que est√° dando realmente vale la pena o no.\n",
    "\n",
    "Podemos resumir el algoritmo en los siguientes pasos, seg√∫n el paper:\n",
    "\n",
    "1. Inicialice el inventario de unidades de palabras con los caracteres base.\n",
    "1. Construya un modelo de lenguaje sobre los datos de entrenamiento usando la palabra inventario de 1.\n",
    "1. Genere una nueva unidad de palabras combinando dos unidades del inventario de palabras actual. El inventario de unidades de palabras se incrementar√° en 1 despu√©s de agregar esta nueva unidad de palabras. La nueva unidad de palabra se elige entre todas las posibles para que aumente al m√°ximo la probabilidad de que los datos de entrenamiento se agreguen al modelo.\n",
    "1. Vaya a 2 hasta que se alcance un l√≠mite predefinido de unidades de palabras o el aumento de probabilidad caiga por debajo de un cierto umbral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d22e030-97a0-42b2-bcf2-672201ea56cf",
   "metadata": {},
   "source": [
    "Debe estar pensando que el entrenamiento debe ser un procedimiento computacionalmente costoso si se realiza utilizando la fuerza bruta. \n",
    "\n",
    "Si es as√≠, entonces tienes raz√≥n.\n",
    "\n",
    "La complejidad del tiempo es O(K¬≤) donde K es el n√∫mero de unidades de palabras actuales. \n",
    "\n",
    "En cuanto a cada iteraci√≥n, necesitamos probar todas las combinaciones de pares posibles y entrenar un nuevo modelo de lenguaje cada vez. \n",
    "\n",
    "Sin embargo, el algoritmo de entrenamiento puede acelerarse significativamente siguiendo algunos trucos simples, como se explica en el art√≠culo.\n",
    "\n",
    "Podemos probar solo los pares que realmente existen en los datos de entrenamiento, probar solo los pares con una probabilidad significativa de ser los mejores (los que tienen altas prioridades), combinar varios pasos de agrupaci√≥n en una sola iteraci√≥n (posible para un grupo de pares que no se afectan entre s√≠). \n",
    "\n",
    "Seg√∫n el documento, estas aceleraciones codiciosas ayudaron a construir un vocabulario de 200k para los conjuntos de datos de japon√©s y coreano en solo unas pocas horas en una sola m√°quina.\n",
    "\n",
    "La implementaci√≥n de este algoritmo se parece mucho entonces a la de BPE con unos peque√±os cambios de optimizaci√≥n.\n",
    "\n",
    "As√≠ pues, Tensorflow y HuggingFace han creade excelentes API para no hacer el entrenamiento desde cero.\n",
    "\n",
    "Veremos m√°s adelante un ejemplo en concreto.\n",
    "\n",
    "Si tiene curiosidad inmediata, puede mirar en [Subword tokenizers](https://www.tensorflow.org/text/guide/subwords_tokenizer) de Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a084df43-a07c-4cda-846e-93167959f9ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\">Unigram</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c602cbc2-dfae-4014-8870-cc7d11285cc7",
   "metadata": {},
   "source": [
    "El algoritmo Unigram se usa a menudo en SentencePiece, que es el algoritmo de tokenizaci√≥n que usan modelos como AlBERT, T5, mBART, Big Bird y XLNet.\n",
    "\n",
    "En comparaci√≥n con BPE y WordPiece, Unigram funciona en la otra direcci√≥n: comienza con un gran vocabulario y le quita tokens hasta que alcanza el tama√±o de vocabulario deseado. \n",
    "\n",
    "Hay varias opciones para usar para construir ese vocabulario base: podemos tomar las subcadenas m√°s comunes en palabras previamente tokenizadas, por ejemplo, o aplicar BPE en el corpus inicial con un tama√±o de vocabulario grande.\n",
    "\n",
    "En cada paso del entrenamiento, el algoritmo Unigram calcula una p√©rdida sobre el corpus dado el vocabulario actual. Luego, para cada s√≠mbolo en el vocabulario, el algoritmo calcula cu√°nto aumentar√≠a la p√©rdida total si se eliminara el s√≠mbolo, y busca los s√≠mbolos que la aumentar√≠an menos. \n",
    "\n",
    "Esos s√≠mbolos tienen un efecto menor en la p√©rdida general del corpus, por lo que, en cierto sentido, son \"menos necesarios\" y son los mejores candidatos para su eliminaci√≥n.\n",
    "\n",
    "Toda esta es una operaci√≥n muy costosa, por lo que no solo eliminamos el s√≠mbolo √∫nico asociado con el aumento de p√©rdida m√°s bajo, sino el porcentaje p (siendo p un hiperpar√°metro que puede controlar, generalmente 10 o 20) de los s√≠mbolos asociados con el aumento m√°s bajo. aumento de p√©rdidas.\n",
    "\n",
    "Este proceso luego se repite hasta que el vocabulario ha alcanzado el tama√±o deseado.\n",
    "\n",
    "Tenga en cuenta que nunca eliminamos los caracteres base, para asegurarnos de que cualquier palabra pueda tokenizarse.\n",
    "\n",
    "Ahora, esto todav√≠a es un poco vago: la parte principal del algoritmo es calcular una p√©rdida sobre el corpus y ver c√≥mo cambia cuando eliminamos algunos tokens del vocabulario, pero a√∫n no hemos explicado c√≥mo hacerlo. \n",
    "\n",
    "Este paso se basa en el algoritmo de tokenizaci√≥n de un modelo de Unigram, por lo que nos sumergiremos en esto a continuaci√≥n.\n",
    "\n",
    "Para mostrar un ejemplo, vayamos a [Tokenization algorithm](https://huggingface.co/course/chapter6/7?fw=pt#tokenization-algorithm), de HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7bfc0b-4b1a-48b2-afd4-7c6b022de40c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\">SentencePiece</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680eb071-622b-43e8-a4a0-56f287f9d551",
   "metadata": {},
   "source": [
    "Sorprendentemente, en realidad no es un tokenizador.\n",
    "\n",
    "En realidad, es un m√©todo para seleccionar tokens de una lista precompilada, optimizando el proceso de tokenizaci√≥n en funci√≥n de un corpus proporcionado. SentencePiece, es el nombre de un paquete (disponible en [google/sentencepiece](https://github.com/google/sentencepiece/)) que implementa el algoritmo de regularizaci√≥n de subpalabras (todo por el mismo autor, Kudo, Taku). \n",
    "\n",
    "**Puntos fuertes de SentencePiece:**\n",
    "\n",
    "1. Est√° implementado en C++ y es incre√≠blemente r√°pido. Puede entrenar un tokenizador en un corpus de 10‚Åµ caracteres en segundos.\n",
    "1. Tambi√©n es incre√≠blemente r√°pido para tokenizar. Esto significa que puede usarlo directamente en datos de texto sin procesar, sin la necesidad de almacenar sus datos tokenizados en el disco.\n",
    "1. La regularizaci√≥n de subpalabras es como una versi√≥n de texto del aumento de datos y puede mejorar en gran medida la calidad de su modelo.\n",
    "1. Es independiente del espacio en blanco. Puede entrenar idiomas que no est√©n delimitados por espacios en blanco, como el chino y el japon√©s, con la misma facilidad que lo har√≠a con el ingl√©s o el franc√©s.\n",
    "1. Puede funcionar a nivel de byte, por lo que **casi** nunca necesitar√° usar tokens [UNK] o [OOV]. Esto no es espec√≠fico solo de SentencePiece.\n",
    "\n",
    "Est√° fundamentado en [Byte Pair Encoding is Suboptimal for Language Model Pretraining](https://arxiv.org/pdf/2004.03720.pdf).\n",
    "\n",
    "En pocas palabras, SentencePiece usa una estrategia entre BPE y modelos unigram para aprovechar las m√∫ltiples formas que tienen las parejas de unirse como representaci√≥n de una secuencia de s√≠mbolos.\n",
    "\n",
    "A partir de estos m√©todos se optimiza una funci√≥n que maximiza la probabilidad del conjunto de entrenamiento.\n",
    "\n",
    "Para mayores detalles, podemos ir a [SentencePiece Tokenizer Demystified](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15), por Jonathan Kernes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cde0145-e30d-4cea-917d-8dd550912ae1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Datasets</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b914af0-46a5-4fad-bf8f-a857d98c05a8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\">Desde el Hub de HF</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9c08c8-bf69-43ca-9c9f-661abf3bc630",
   "metadata": {},
   "source": [
    "Haremos un ejercicio de tokenizaci√≥n.\n",
    "\n",
    "Usemos el dataset Oscar para nuestros fines y la versi√≥n en lat√≠n, pues los otros idiomas tienen much√≠sima informaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f43b99-0ccd-4b6d-97a7-dfcceab61f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efc6f3c6-21b3-4b1e-9c82-e85f66386d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset oscar/unshuffled_deduplicated_la (download: 3.26 MiB, generated: 8.46 MiB, post-processed: Unknown size, total: 11.72 MiB) to /Users/moury/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_la/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50538fa5e85b45c2b42fffc71911f16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3108dafba447f6990a5aa920c46f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b84aa62f8b48ab8c863fb4dc3f3410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/18808 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset oscar downloaded and prepared to /Users/moury/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_la/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51df06ebfc50428ba5b814adc66b9dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('oscar', 'unshuffled_deduplicated_la')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be8cfd7b-d33b-4616-a848-88d01d260aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text'],\n",
       "        num_rows: 18808\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4db5c60-60f8-4704-b521-c164b4c19ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'text': 'H√¶ sunt generationes No√´: No√´ vir justus atque perfectus fuit in generationibus suis; cum Deo ambulavit.\\nEcce ego adducam aquas diluvii super terram, ut interficiam omnem carnem, in qua spiritus vit√¶ est subter c√¶lum: universa qu√¶ in terra sunt, consumentur.\\nTolles igitur tecum ex omnibus escis, qu√¶ mandi possunt, et comportabis apud te: et erunt tam tibi, quam illis in cibum.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85beb19f-aaa2-49d2-9b14-6e4b755fb950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82114f1094934cfe9414880828101e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18808 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(dataset['train']):\n",
    "    sample = sample['text'].replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "    if len(text_data) == 1_000:\n",
    "        # once we git the 10K mark, save to file\n",
    "        with open(f'./data/text/oscar_la/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "# after saving in 10K chunks, we will have ~2082 leftover samples, we save those now too\n",
    "with open(f'./data/text/oscar_la/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec209133-3da3-45e5-ba84-540510197ae7",
   "metadata": {},
   "source": [
    "## Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36bc5846-f7f1-40dc-918a-4c0b88f05a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/text/oscar_la/text_10.txt', './data/text/oscar_la/text_11.txt', './data/text/oscar_la/text_13.txt', './data/text/oscar_la/text_12.txt', './data/text/oscar_la/text_16.txt', './data/text/oscar_la/text_17.txt', './data/text/oscar_la/text_15.txt', './data/text/oscar_la/text_14.txt', './data/text/oscar_la/text_2.txt', './data/text/oscar_la/text_3.txt', './data/text/oscar_la/text_1.txt', './data/text/oscar_la/text_0.txt', './data/text/oscar_la/text_4.txt', './data/text/oscar_la/text_5.txt', './data/text/oscar_la/text_7.txt', './data/text/oscar_la/text_6.txt', './data/text/oscar_la/text_8.txt', './data/text/oscar_la/text_9.txt', './data/text/oscar_la/text_18.txt']\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "rutas = glob('./data/*/*/*.txt')\n",
    "print(rutas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c03d78-105d-43d2-ab22-fcc7259989bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "898ef424-50a0-447a-9a9c-a118321a0b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351e3d43-e727-4f2f-a1d4-edc8e9ca8502",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files=rutas,vocab_size=30_522,min_frequency=2,\n",
    "                special_tokens=[\n",
    "                    '<s>','<pad>','</s>','<unk>','<mask>'\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ff385327-2e8d-4591-b6cc-cf719a2d4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('latinberto',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9dd4d222-52ba-4c8d-8bce-ca914c990849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latinberto/vocab.json', 'latinberto/merges.txt']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model('latinberto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "364bf184-6058-4412-aeaf-0df124b6a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46201e2-7dd3-46cd-b7e2-b3b17e9c5366",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('./latinberto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7c984827-7bf0-4087-9f37-9269946d35d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['et', 'ƒ†tim', 'enti', 'ƒ†b', 'us']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('et timenti bus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e943655f-f546-43bb-9984-e162dba85d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 304, 1489, 1077, 386, 268, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('et timenti bus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1db7f26d-c477-4ccd-9766-f217f1e19026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'et', 'ƒ†tim', 'enti', 'ƒ†b', 'us', '</s>']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([0, 304, 1489, 1077, 386, 268, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "65287048-903a-4bd1-87d8-42742754424b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2022-04-04 19:01:27--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.24.222\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.24.222|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 191984949 (183M) [application/zip]\n",
      "Saving to: ‚Äòwikitext-103-raw-v1.zip‚Äô\n",
      "\n",
      "wikitext-103-raw-v1 100%[===================>] 183.09M   175KB/s    in 12m 43s \n",
      "\n",
      "2022-04-04 19:14:12 (246 KB/s) - ‚Äòwikitext-103-raw-v1.zip‚Äô saved [191984949/191984949]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "66369cbc-0d78-4867-b3d1-d3185cccd768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Archive:  wikitext-103-raw-v1.zip\n",
      "   creating: wikitext-103-raw/\n",
      "  inflating: wikitext-103-raw/wiki.test.raw  \n",
      "  inflating: wikitext-103-raw/wiki.valid.raw  \n",
      "  inflating: wikitext-103-raw/wiki.train.raw  \n"
     ]
    }
   ],
   "source": [
    "!unzip wikitext-103-raw-v1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "359807ed-b08c-4b63-95ba-715e6f908028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "db7c7310-95c3-42ec-b18a-d1b81b4d4755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1b203468-79f8-4fad-b94e-2c22fdcc565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "957e3b23-7604-4dd6-8cea-bc4a6bc33c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f4ae9bed-5a00-4f9a-91c9-6e410e36efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"data/tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9dec84-6a5d-49b1-9674-f6e1e01abf46",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\">Desde el computador local</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "490907c0-840c-4361-8661-b11ccdf76e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-27 16:39:39--  https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-train.json.gz [following]\n",
      "--2022-04-27 16:39:40--  https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-train.json.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7725286 (7.4M) [application/octet-stream]\n",
      "Saving to: ‚ÄòSQuAD_it-train.json.gz‚Äô\n",
      "\n",
      "SQuAD_it-train.json 100%[===================>]   7.37M  2.39MB/s    in 3.1s    \n",
      "\n",
      "2022-04-27 16:39:44 (2.39 MB/s) - ‚ÄòSQuAD_it-train.json.gz‚Äô saved [7725286/7725286]\n",
      "\n",
      "--2022-04-27 16:39:45--  https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-test.json.gz [following]\n",
      "--2022-04-27 16:39:45--  https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-test.json.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1051245 (1.0M) [application/octet-stream]\n",
      "Saving to: ‚ÄòSQuAD_it-test.json.gz‚Äô\n",
      "\n",
      "SQuAD_it-test.json. 100%[===================>]   1.00M  5.91MB/s    in 0.2s    \n",
      "\n",
      "2022-04-27 16:39:46 (5.91 MB/s) - ‚ÄòSQuAD_it-test.json.gz‚Äô saved [1051245/1051245]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
    "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7acf2d94-d65e-40ea-ad6b-d20ee2b38c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD_it-test.json.gz:\t   87.4% -- replaced with SQuAD_it-test.json\n",
      "SQuAD_it-train.json.gz:\t   82.2% -- replaced with SQuAD_it-train.json\n"
     ]
    }
   ],
   "source": [
    "!gzip -dkv SQuAD_it-*.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b21b426a-bc2f-45fc-ae56-cfe653986bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c7f58bf9a4c25889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /Users/moury/.cache/huggingface/datasets/json/default-c7f58bf9a4c25889/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd3d32b692a49f7b9023dcdb66eb28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8875119bbaac4005be42b8accea5ce37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/moury/.cache/huggingface/datasets/json/default-c7f58bf9a4c25889/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8dc298c64049ccae2048aff0739042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=\"SQuAD_it-train.json\", field=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b02cdf2a-85fa-4b66-b1dc-2ae2a1e58a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 442\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73a7cec-da67-4845-8627-e4ed1a7f5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_it_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "74d98f37-deb4-4b11-a209-e8ba51a65151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-5d1738e3eece01bd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /Users/moury/.cache/huggingface/datasets/json/default-5d1738e3eece01bd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2173ee6ad7b747879d6525661e79691d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1a773f9bfb435f9db7d4d40e4513ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/moury/.cache/huggingface/datasets/json/default-5d1738e3eece01bd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17a33dbb2404329b545570886e68362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 442\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 48\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\"train\": \"SQuAD_it-train.json\", \"test\": \"SQuAD_it-test.json\"}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd3fd93-1bb0-4538-a242-b254ce08ac3d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\">De forma remota</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8421e-b6e9-4e5d-872a-c87a48b9e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322629da-115b-416a-8e8e-418e91d5064f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\">Desde pandas (Back and Forth)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "522242b3-0fb8-46ff-b064-ef49eaba7217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-27 16:49:38--  https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 42989872 (41M) [application/x-httpd-php]\n",
      "Saving to: ‚ÄòdrugsCom_raw.zip‚Äô\n",
      "\n",
      "drugsCom_raw.zip    100%[===================>]  41.00M  44.5KB/s    in 16m 30s \n",
      "\n",
      "2022-04-27 17:06:08 (42.4 KB/s) - ‚ÄòdrugsCom_raw.zip‚Äô saved [42989872/42989872]\n",
      "\n",
      "Archive:  drugsCom_raw.zip\n",
      "  inflating: drugsComTest_raw.tsv    \n",
      "  inflating: drugsComTrain_raw.tsv   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8c0944eec81d0f16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/moury/.cache/huggingface/datasets/csv/default-8c0944eec81d0f16/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627f0f08c6a74e399d6d48b3966941dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427f8b54cbc54bdd834467a589328792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/moury/.cache/huggingface/datasets/csv/default-8c0944eec81d0f16/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fae8a1f734c4cf5b326074b4b31425f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>206461</td>\n",
       "      <td>Valsartan</td>\n",
       "      <td>Left Ventricular Dysfunction</td>\n",
       "      <td>\"It has no side effect, I take it in combinati...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>May 20, 2012</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    drugName                     condition  \\\n",
       "0      206461   Valsartan  Left Ventricular Dysfunction   \n",
       "1       95260  Guanfacine                          ADHD   \n",
       "2       92703      Lybrel                 Birth Control   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"It has no side effect, I take it in combinati...     9.0   \n",
       "1  \"My son is halfway through his fourth week of ...     8.0   \n",
       "2  \"I used to take another oral contraceptive, wh...     5.0   \n",
       "\n",
       "                date  usefulCount  \n",
       "0       May 20, 2012           27  \n",
       "1     April 27, 2010          192  \n",
       "2  December 14, 2009           17  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!unzip drugsCom_raw.zip\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"}\n",
    "# \\t is the tab character in Python\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
    "\n",
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# Peek at the first few examples\n",
    "drug_sample[:3]\n",
    "\n",
    "\n",
    "# CONVERTIR A PANDAS\n",
    "drug_dataset.set_format(\"pandas\")\n",
    "\n",
    "drug_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed6810-0667-44f8-95d1-8229a570aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datarame completo\n",
    "train_df = drug_dataset[\"train\"][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491deb6-89fc-48d1-ac07-5bc440d91f2b",
   "metadata": {},
   "source": [
    "Desde aqu√≠, podemos usar todas las operaciones de pandas que queramos y volver al dataset con las modificaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c8ec8-55a6-45c8-b813-03e3fdce59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = (\n",
    "    train_df[\"condition\"]\n",
    "    .value_counts()\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"condition\", \"condition\": \"frequency\"})\n",
    ")\n",
    "frequencies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a81432d-7f76-462e-a6ea-6e72e643eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "freq_dataset = Dataset.from_pandas(frequencies)\n",
    "freq_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3614f057-76ef-4b9f-80a6-a1ec93cf4d28",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\">Guardar en Disco y volver a cargar</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b4822-c1e0-4abd-9f43-c8341c02cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dataset.save_to_disk(\"freq_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b4c55-1e69-42d8-b046-67e40933b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "drug_dataset_reloaded = load_from_disk(\"freq_dataset\")\n",
    "drug_dataset_reloaded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
