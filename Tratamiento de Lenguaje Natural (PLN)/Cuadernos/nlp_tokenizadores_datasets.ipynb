{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc293c4a-2e1e-496d-89e3-ab2755e7ecfc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5725a1-922a-4698-84aa-bd3acde95846",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Hugging Face: Tokenizadores y datasets</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79df48-d239-44d3-87ad-350a9c22db83",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center>T5, Longformers, ST-Moe32B, PaLM </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ea67e-4059-4263-9839-a1a0af4b8645",
   "metadata": {
    "tags": []
   },
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8535f2c-407f-4469-9865-b5a6f3c68627",
   "metadata": {},
   "source": [
    "### Coordinador\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9299cf-5583-4183-a193-3e2409a8f0ac",
   "metadata": {},
   "source": [
    "- Campo Elías Pardo Turriago, cepardot@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38821aaf-ab62-4122-896f-ec956f83fa90",
   "metadata": {},
   "source": [
    "### Conferencistas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dcb774-31fc-4f60-a706-0589c5e0ce0b",
   "metadata": {},
   "source": [
    "- Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "- Daniel  Montenegro, Msc, dextronomo@gmail.com \n",
    "- Oleg Jarma, Estadístico, ojarmam@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be563094-f120-4c65-8c32-a4d12f139dfb",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asesora Medios y Marketing digital</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a4fd42-e3b7-4d0c-b5a4-1a5f4ef4263d",
   "metadata": {},
   "source": [
    "- Maria del Pilar Montenegro, pmontenegro88@gmail.com \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ba12e5-1d2d-4b6e-818e-ba5f39f70d9c",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b516d987-273d-4208-9a6e-de1659131a21",
   "metadata": {},
   "source": [
    "- Nayibe Yesenia Arias, naariasc@unal.edu.co\n",
    "- Venus Celeste Puertas, vpuertasg@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccfdb82-6552-41d2-b3e7-684588492c2d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5fae6a-e1d4-4f2e-a39f-1dc5092f86dd",
   "metadata": {},
   "source": [
    "1. [What is Tokenization in NLP? Here’s All You Need To Know](https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/#:~:text=Hence%2C%20tokenization%20can%20be%20broadly,n%2Dgram%20characters%20tokenization.)\n",
    "1.[Byte Pair Encoding — The Dark Horse of Modern NLP](https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10)\n",
    "1. [Aprendizaje Profundo-Diplomado](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [WordPiece: Subword-based tokenization algorithm](https://towardsdatascience.com/wordpiece-subword-based-tokenization-algorithm-1fbd14394ed7)\n",
    "1. [SentencePiece Tokenizer Demystified](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15)\n",
    "1. [Aprendizaje Profundo-PLN](https://github.com/AprendizajeProfundo/PLN)\n",
    "1. [Ashish Vaswani et al.,   Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf), diciembre 2017.\n",
    "1. [Dennis Rothman, Transformers for Natural Language processing](http://libgen.rs/search.php?req=Transformers+for+Natural+Language+processing&open=0&res=25&view=simple&phrase=1&column=def), enero 2021.\n",
    "1.[ Varios,  Dive into deep learning](https://d2l.ai/), enero 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafab141-e901-4884-b13b-2abc2370dfe4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0d29b-d230-4b96-8701-3cd537bbceb6",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Tipos de Tokenización Moderna](#Tipos-de-Tokenización-Moderna)\n",
    "    * [BPE](#BPE)\n",
    "    * [WordPiece](#WordPiece)\n",
    "    * [Unigram](#Unigram)\n",
    "    * [SentencePiece](#SentencePiece)\n",
    "* [Datasets](#SUPERGLUE-Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4d687-0d55-4387-981d-c566df2f6297",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51c3de-15d8-44bc-9e88-34f0e8cbab1e",
   "metadata": {},
   "source": [
    "Como hemos visto en lecciones anteriores, la tokenización juega un papel fundamental cuando se trabaja en NLP o NLU.\n",
    "\n",
    "Recordemos que tokenizar signfica segmentar el texto en partes fundamentales (**tokens**).\n",
    "\n",
    "*Los Tokens son los bloques de construcción del lenguaje natural.*\n",
    "\n",
    "**¿Qué signfica una parte fundamental?**\n",
    "\n",
    "La respuesta a esta pregunta puede variar:\n",
    "\n",
    "- Palabras (Word Tokenization)\n",
    "- Caracteres (Character Tokenization)\n",
    "- Subpalabras (Subword Tokenization)\n",
    "\n",
    "Cada una de las tres tiene sus ventajas y sus desventajas.\n",
    "\n",
    "Por un lado, una tokenización por palabras puede guardar información relevante semántica del texto que se está analizando, pero tenemos el problema de tener vocabularios con un grande e imanegable número de tokens.\n",
    "\n",
    "Note por ejemplo, la palabra *jugar*. \n",
    "\n",
    "Algunas variantes son: *jugué, jugaré, jugaremos, juguemos, ...*.\n",
    "\n",
    "Esto hace difícil el manejo de textos, pues si en textos nuevos que vea un modelo de NLP aparecen estas variantes y no se encuentran en el vocabulario con el que fue entrenado el modelo, se considerarán **OOV** (Out Of Vocabulary) y se perderá información relevante al momento de hacer predicciones o clasificación.\n",
    "\n",
    "Como ejemplo, recordemos que tokenizar una frase por palabras puede ser de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab3ff528-6def-4b1c-a399-6e7d3198ed5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Los',\n",
       " 'pajaritos',\n",
       " 'vuelan',\n",
       " ',',\n",
       " 'y',\n",
       " 'las',\n",
       " 'nubes',\n",
       " 'se',\n",
       " 'enlazan',\n",
       " 'con',\n",
       " 'un',\n",
       " 'azul',\n",
       " 'infinito',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "#String\n",
    "texto = 'Los pajaritos vuelan, y las nubes se enlazan con un azul infinito.'\n",
    "# Separar puntos y comas\n",
    "texto = texto.translate(str.maketrans({'.': ' .', ',': ' ,'}))\n",
    "# Dividir por espacios\n",
    "tokens = texto.split()\n",
    "# Ver tokenización\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c056a26-74e0-49e7-ac59-d72bbca4068f",
   "metadata": {},
   "source": [
    "Como vemos, si un modelo se entrenara con esta única frase y de repente le entraramos la palabra *nubes*, el modelo no podría indentificar dicho token como relacionado con *nube* y le daría una etiqueta especial **UNK** (Unknown).\n",
    "\n",
    "Esto no es óptimo. Sin embargo, este tipo de tokenización se utiliza en **Word2Vec** y **GloVe**.\n",
    "\n",
    "Por otro lado, la tokenización por caracteres, si bien resuelve este problema de **OOV**, los tokens individuales carecen de significado por si sólo, lo que haría que el modelo pasara mal rato tratando de hacer tareas complicadas del lenguaje, como por ejemplo ahcer inferencias a partir de una frase. \n",
    "\n",
    "Además, la longitud de las secuencias de input del modelo se volverán rápidamente muy extensas, lo que haría que el modelo tuviese que hacer una gran cantidad (en algunos casos imposible) de cálculos.\n",
    "\n",
    "Recordemos cómo tokenizar por caracteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9431ed-7dc9-45b4-83c5-4b54d57488e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L|o|s| |p|a|j|a|r|i|t|o|s| |v|u|e|l|a|n| |,| |y| |l|a|s| |n|u|b|e|s| |s|e| |e|n|l|a|z|a|n| |c|o|n| |u|n| |a|z|u|l| |i|n|f|i|n|i|t|o| |.|"
     ]
    }
   ],
   "source": [
    "c_tokens = [t for t in texto]\n",
    "for c in c_tokens:\n",
    "    print(c,end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181649e-59d7-43c5-9155-45a4adca8594",
   "metadata": {},
   "source": [
    "Ante este tipo de dificultades, podríamos preguntarnos qué pasaría en un mundo intermedio, donde no nos quedemos con las palabras completas, pero tampoco simplifiquemos hasta el mundo de caracteres.\n",
    "\n",
    "Bienvenidos a la tokenización por subpalabras.\n",
    "\n",
    "En particular, el uso de n-gramas fue potenciado por Mikolov T. en su desarrollo de **FastText** en [Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf).\n",
    "\n",
    "Veamos a profundidad tipos de tokenización utilizados en los modelos modernos (Transformers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb344c0-b72c-4daf-9d65-84cd334ef68a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Tipos de Tokenización Moderna</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e85fe-ee2d-40d9-864d-c8d5720b636a",
   "metadata": {},
   "source": [
    "El fin último de la tokenización es la construcción del vocabulario que usará el modelo para convertir texto en números.\n",
    "\n",
    "Un vocabulario eficiente no preservará toda la información de los tokens, sino los más frecuentes según algún umbral.\n",
    "\n",
    "Una tokenización por subpalabras, se puede entender de la siguiente manera:\n",
    "\n",
    "- Jugaremos -> Jugar-emos\n",
    "- Jugarás   -> Jugar-ás\n",
    "\n",
    "Como podemos notar, esto puede reducir en gran parte el tamaño del vocabulario a la vez que se preservan significados de palabras en su forma básica.\n",
    "\n",
    "Todos los modelos modernos de Transformers utlizan alguna forma de tokenización por subpalabras.\n",
    "\n",
    "Veamos algunos de ellos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9baa571-9c18-4d72-85ad-3904ef3a5a54",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### <span style=\"color:blue\">BPE</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0108b11f-f23a-4923-99fb-032fb6844fd5",
   "metadata": {},
   "source": [
    "**BPE** viene de Byte-Pair Encoding, y se encarga de segmentar palabras **OOV** como subpalabras, y representar dichas palabras con tokens de las subpalabras.\n",
    "\n",
    "El procedimiento de BPE es como sigue:\n",
    "\n",
    "1. Divide las palabras del corpus en caracteres después de agregar \\</w\\>.\n",
    "1. Inicializar el vocabulario con caracteres únicos en el corpus.\n",
    "1. Calcule la frecuencia de un par de caracteres o secuencias de caracteres en corpus.\n",
    "1. Combinar el par más frecuente en el corpus.\n",
    "1. Guarda la mejor pareja para el vocabulario.\n",
    "1. Repita los pasos 3 a 5 para un cierto número de iteraciones.\n",
    "\n",
    "Entendamos los pasos anteriores con un ejemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0804863-e678-4d71-b9e3-8369885c5833",
   "metadata": {},
   "source": [
    "||Corpus||\n",
    "|-|-|-|\n",
    "|low|lower|newest|\n",
    "|low|lower|newest|\n",
    "|low|widest|newest|\n",
    "|low|widest|newest|\n",
    "|low|widest|newest|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc54959-9440-4cb3-a5cf-5e0ee53a0b13",
   "metadata": {},
   "source": [
    "1. Divide las palabras del corpus en caracteres después de agregar \\</w\\>:\n",
    "\n",
    "||Corpus||\n",
    "|-|-|-|\n",
    "|low\\</w\\>|lower\\</w\\>|newest\\</w\\>|\n",
    "|low\\</w\\>|lower\\</w\\>|newest\\</w\\>|\n",
    "|low\\</w\\>|widest\\</w\\>|newest\\</w\\>|\n",
    "|low\\</w\\>|widest\\</w\\>|newest\\</w\\>|\n",
    "|low\\</w\\>|widest\\</w\\>|newest\\</w\\>|\n",
    "\n",
    "||Corpus||\n",
    "|-|-|-|\n",
    "|l o w \\</w\\>|l o w e r \\</w\\>|n e w e s t \\</w\\>|\n",
    "|l o w \\</w\\>|l o w e r\\</w\\>|n e w e s t \\</w\\>|\n",
    "|l o w \\</w\\>|w i d e s t \\</w\\>|n e w e s t \\</w\\>|\n",
    "|l o w \\</w\\>|w i d e s t \\</w\\>|n e w e s t \\</w\\>|\n",
    "|l o w \\</w\\>|w i d e s t \\</w\\>|n e w e s t \\</w\\>|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8313c768-60d4-4218-a793-5e91a6300db3",
   "metadata": {},
   "source": [
    "2. Inicializar el vocabulario con caracteres únicos en el corpus.\n",
    "\n",
    "|||||||||||\n",
    "|-|-|-|-|-|-|-|-|-|-|\n",
    "|d|e|i|l|n|o|r|s|t|w|\n",
    "|-|-|-|-|-|-|-|-|-|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd45f172-6b79-4f7d-b23b-152ecaf66213",
   "metadata": {},
   "source": [
    "**Iteración 1:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec1851-ecab-470f-a9a4-d161d231beee",
   "metadata": {},
   "source": [
    "3. Calcule la frecuencia de un par de caracteres o secuencias de caracteres en corpus.\n",
    "\n",
    "||||\n",
    "|-|-|-|\n",
    "|d-e (3)|l-o (7)|t-\\</w\\> (8)|\n",
    "|e-r (2)|n-e (5)|w-\\</w\\> (5)|\n",
    "|**e-s (8)**|o-w (7)|w-e (7)|\n",
    "|e-w (5)|r-\\</w\\> (2)|w-i (3)|\n",
    "|i-d (3)|s-t (8)||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a1d234-eb39-4b96-939c-0155732aaf53",
   "metadata": {},
   "source": [
    "4. Combinar el par más frecuente en el corpus.\n",
    "\n",
    "||Corpus||\n",
    "|-|-|-|\n",
    "|l o w \\</w\\>|l o w e r \\</w\\>|n e w **es** t \\</w\\>|\n",
    "|l o w \\</w\\>|l o w e r\\</w\\>|n e w **es** t \\</w\\>|\n",
    "|l o w \\</w\\>|w i d **es** t \\</w\\>|n e w **es** t \\</w\\>|\n",
    "|l o w \\</w\\>|w i d **es** t \\</w\\>|n e w **es** t \\</w\\>|\n",
    "|l o w \\</w\\>|w i d **es** t \\</w\\>|n e w **es** t \\</w\\>|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31435196-91f7-4ac8-a4c2-324a80622f70",
   "metadata": {},
   "source": [
    "5. Guarda la mejor pareja para el vocabulario.\n",
    "\n",
    "|||||||||||\n",
    "|-|-|-|-|-|-|-|-|-|-|\n",
    "|d|e|i|l|n|o|r|s|t|w|\n",
    "|**es**|-|-|-|-|-|-|-|-|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e22fb1-487e-49be-98c7-b133af426dbe",
   "metadata": {},
   "source": [
    "**Iteración 2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e45cdd7-7a29-4202-96ef-5ac9a8f2f1ca",
   "metadata": {},
   "source": [
    "3. Calcule la frecuencia de un par de caracteres o secuencias de caracteres en corpus.\n",
    "\n",
    "||||\n",
    "|-|-|-|\n",
    "|d-es (3)|l-o (7)|w-\\</w\\> (5)|\n",
    "|e-r (2)|n-e (5)|w-es (5)|\n",
    "|e-w (5)|o-w (7)|w-e (7)|\n",
    "|**es-t (8)**|r-\\</w\\> (2)|w-i (3)|\n",
    "|i-d (3)|t-\\</w\\> (8)||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c4f17-f30b-404d-a49a-3b9f57771b91",
   "metadata": {},
   "source": [
    "4. Combinar el par más frecuente en el corpus.\n",
    "\n",
    "||Corpus||\n",
    "|-|-|-|\n",
    "|l o w \\</w\\>|l o w e r \\</w\\>|n e w **est**  \\</w\\>|\n",
    "|l o w \\</w\\>|l o w e r\\</w\\>|n e w **est**  \\</w\\>|\n",
    "|l o w \\</w\\>|w i d **est**  \\</w\\>|n e w **est**  \\</w\\>|\n",
    "|l o w \\</w\\>|w i d **est**  \\</w\\>|n e w **est** \\</w\\>|\n",
    "|l o w \\</w\\>|w i d **est**  \\</w\\>|n e w **est**  \\</w\\>|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5114b0a0-beae-4b9a-9885-3d0f5845f28d",
   "metadata": {},
   "source": [
    "5. Guarda la mejor pareja para el vocabulario.\n",
    "\n",
    "|||||||||||\n",
    "|-|-|-|-|-|-|-|-|-|-|\n",
    "|d|e|i|l|n|o|r|s|t|w|\n",
    "|**es**|**est**|-|-|-|-|-|-|-|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f0db72-2e8f-47cd-b977-125d1cfd64ab",
   "metadata": {},
   "source": [
    "**Después de 10 iteraciones, obtenemos:**\n",
    "\n",
    "|||||||||||\n",
    "|-|-|-|-|-|-|-|-|-|-|\n",
    "|d|e|i|l|n|o|r|s|t|w|\n",
    "|**es**|**est**|**est\\</w\\>**|**lo**|**low**|**low\\</w\\>**|**ne**|**new**|**newest\\</w\\>**|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d17a0e-865c-49bb-be84-871479ee8a74",
   "metadata": {},
   "source": [
    "En el momento de test, las palabras **OOV** se divide en secuencias de caracteres. \n",
    "\n",
    "Luego, las operaciones aprendidas se aplican para fusionar los caracteres en símbolos conocidos más grandes.\n",
    "\n",
    "Para mayor información, se puede referir a [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf), Senrich R, et al., 2016.\n",
    "\n",
    "Aquí hay un procedimiento paso a paso para representar palabras OOV:\n",
    "\n",
    "1. Divide la palabra OOV en caracteres después de agregar \\</w\\>\n",
    "1. Calcular un par de caracteres o secuencias de caracteres en una palabra\n",
    "1. Seleccionar los pares presentes en las operaciones aprendidas\n",
    "1. Combinar el par más frecuente\n",
    "1. Repita los pasos 2 y 3 hasta que sea posible la fusión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2285e2-57e1-4bbf-aa88-81b3f5928a5e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### <span style=\"color:blue\">Implementación</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1434f3a7-f755-4584-a01b-faa91c9b9609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing library\n",
    "import pandas as pd\n",
    "\n",
    "#reading .txt file\n",
    "text = pd.read_csv(\"../Datos/sample.txt\")\n",
    "\n",
    "#converting a dataframe into a single list \n",
    "corpus=[]\n",
    "for row in text.values:\n",
    "    # divide line text by spaces\n",
    "    tokens = row[0].split(\" \")\n",
    "    for token in tokens:\n",
    "        # append each token found\n",
    "        corpus.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b4d35578-8654-4040-b7c5-36c3f2797a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Textos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>low lower newest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>low lower newest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>low widest newest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>low widest newest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>low widest newest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Textos\n",
       "0   low lower newest\n",
       "1   low lower newest\n",
       "2  low widest newest\n",
       "3  low widest newest\n",
       "4  low widest newest"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "11855ed2-86a2-4c5b-bc11-6fb7afd47f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['low',\n",
       " 'lower',\n",
       " 'newest',\n",
       " 'low',\n",
       " 'lower',\n",
       " 'newest',\n",
       " 'low',\n",
       " 'widest',\n",
       " 'newest',\n",
       " 'low',\n",
       " 'widest',\n",
       " 'newest',\n",
       " 'low',\n",
       " 'widest',\n",
       " 'newest']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d51e789b-49d8-429d-b83a-ebea41c9dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initlialize the vocabulary\n",
    "vocab = list(set(\" \".join(corpus)))\n",
    "vocab.remove(' ')\n",
    "\n",
    "#split the word into characters\n",
    "corpus = [\" \".join(token) for token in corpus]\n",
    "\n",
    "#appending </w>\n",
    "corpus=[token+' </w>' for token in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "73512434-09d2-4834-95f9-ae69938bbce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w', 'o', 'e', 'l', 'r', 's', 't', 'i', 'd', 'n']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "75e1e406-0b3c-4040-b462-13d13bb1b98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l o w </w>',\n",
       " 'l o w e r </w>',\n",
       " 'n e w e s t </w>',\n",
       " 'l o w </w>',\n",
       " 'l o w e r </w>',\n",
       " 'n e w e s t </w>',\n",
       " 'l o w </w>',\n",
       " 'w i d e s t </w>',\n",
       " 'n e w e s t </w>',\n",
       " 'l o w </w>',\n",
       " 'w i d e s t </w>',\n",
       " 'n e w e s t </w>',\n",
       " 'l o w </w>',\n",
       " 'w i d e s t </w>',\n",
       " 'n e w e s t </w>']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c7794619-0ec9-43ae-9f3f-8a9477adc2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 5, 'w i d e s t </w>': 3}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "#returns frequency of each word\n",
    "corpus = collections.Counter(corpus)\n",
    "\n",
    "#convert counter object to dictionary\n",
    "corpus = dict(corpus)\n",
    "print(\"Corpus:\",corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af068f9d-8400-4fce-8fa4-7411ec988405",
   "metadata": {},
   "source": [
    "La función a continuación cuenta la frecuencia de los bigramas de caracteres de cada palabra en el corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7d907f04-5771-489b-9cdf-968a4be98ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computer frequency of a pair of characters or character sequences\n",
    "#accepts corpus and return frequency of each pair\n",
    "def get_stats(corpus):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in corpus.items():\n",
    "        # divide string into characters\n",
    "        symbols = word.split()\n",
    "        # count frequencies of pairs\n",
    "        for i in range(len(symbols)-1):\n",
    "            # look the pairs and count\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b77eaf4-d58c-4764-9b3e-9bb66838fdd7",
   "metadata": {},
   "source": [
    "Finalmente, combinamos los pares más frecuentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b510118b-aeef-4eeb-b8f7-8114f6de497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merges the most frequent pair in the corpus\n",
    "#accepts the corpus and best pair\n",
    "#returns the modified corpus \n",
    "import re\n",
    "def merge_vocab(pair, corpus_in):\n",
    "    corpus_out = {}\n",
    "    # Ignore special characters\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    # Merge the sequence into the corpus\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    for word in corpus_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        corpus_out[w_out] = corpus_in[word]\n",
    "    \n",
    "    return corpus_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2e4df641-0153-4cc3-85aa-dcc1c8f0ccec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('l', 'o'): 7,\n",
       "             ('o', 'w'): 7,\n",
       "             ('w', '</w>'): 5,\n",
       "             ('w', 'e'): 7,\n",
       "             ('e', 'r'): 2,\n",
       "             ('r', '</w>'): 2,\n",
       "             ('n', 'e'): 5,\n",
       "             ('e', 'w'): 5,\n",
       "             ('e', 's'): 8,\n",
       "             ('s', 't'): 8,\n",
       "             ('t', '</w>'): 8,\n",
       "             ('w', 'i'): 3,\n",
       "             ('i', 'd'): 3,\n",
       "             ('d', 'e'): 3})"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute frequency of bigrams in a corpus\n",
    "pairs = get_stats(corpus)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "647a0948-9a37-4479-8742-82bfa7aa0cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Frequent pair: ('e', 's')\n"
     ]
    }
   ],
   "source": [
    "#compute the best pair\n",
    "best = max(pairs, key=pairs.get)\n",
    "print(\"Most Frequent pair:\",best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7938ea30-0e83-4c1f-a3d9-d3a8815519b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Merging: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 5, 'w i d es t </w>': 3}\n"
     ]
    }
   ],
   "source": [
    "#merge the frequent pair in corpus\n",
    "corpus = merge_vocab(best, corpus)\n",
    "print(\"After Merging:\", corpus)\n",
    "\n",
    "#convert a tuple to a string\n",
    "best = \"\".join(list(best))\n",
    "\n",
    "#append to merge list and vocabulary\n",
    "merges = []\n",
    "merges.append(best)\n",
    "vocab.append(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ef67f663-2c61-4888-8f50-2852dfbe9c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Merge Operations: ['es', 'est', 'est</w>', 'lo', 'low', 'low</w>', 'ne', 'new', 'newest</w>', 'wi', 'wid']\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    \n",
    "    #compute frequency of bigrams in a corpus\n",
    "    pairs = get_stats(corpus)\n",
    "    \n",
    "    #compute the best pair\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    \n",
    "    #merge the frequent pair in corpus\n",
    "    corpus = merge_vocab(best, corpus)\n",
    "    \n",
    "    #append to merge list and vocabulary\n",
    "    merges.append(best)\n",
    "    vocab.append(best)\n",
    "\n",
    "#convert a tuple to a string\n",
    "merges_in_string = [\"\".join(list(i)) for i in merges]\n",
    "print(\"BPE Merge Operations:\",merges_in_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c3a51ed1-cfa8-4274-9ab4-b3e1153783e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w',\n",
       " 'o',\n",
       " 'e',\n",
       " 'l',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'i',\n",
       " 'd',\n",
       " 'n',\n",
       " 'es',\n",
       " ('es', 't'),\n",
       " ('est', '</w>'),\n",
       " ('l', 'o'),\n",
       " ('lo', 'w'),\n",
       " ('low', '</w>'),\n",
       " ('n', 'e'),\n",
       " ('ne', 'w'),\n",
       " ('new', 'est</w>'),\n",
       " ('w', 'i'),\n",
       " ('wi', 'd')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9ee7dd7d-2dec-4f2f-a41d-15b7958a567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying BPE to OOV\n",
    "oov ='lowest'\n",
    "\n",
    "#tokenize OOV into characters\n",
    "oov = \" \".join(list(oov))\n",
    "\n",
    "#append </w> \n",
    "oov = oov + ' </w>'\n",
    "\n",
    "#create a dictionary\n",
    "oov = { oov : 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "096e080a-dd3e-480a-8433-e7391be9cb27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l o w e s t </w>': 1}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "31a782c0-8890-4642-9ce8-9333a1ac105a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "Iteration  1 lo w e s t </w>\n",
      "[4]\n",
      "Iteration  2 low e s t </w>\n",
      "[]\n",
      "\n",
      "BPE Completed...\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "while(True):\n",
    "\n",
    "    #compute frequency\n",
    "    pairs = get_stats(oov)\n",
    "\n",
    "    #extract keys\n",
    "    pairs = pairs.keys()\n",
    "    \n",
    "    #find the pairs available in the learned operations\n",
    "    ind=[merges.index(i) for i in pairs if i in merges]\n",
    "    #print(ind)\n",
    "    if(len(ind)==0):\n",
    "        print(\"\\nBPE Completed...\")\n",
    "        break\n",
    "    \n",
    "    #choose the most frequent learned operation\n",
    "    best = merges[min(ind)]\n",
    "    \n",
    "    #merge the best pair\n",
    "    oov = merge_vocab(best, oov)\n",
    "    \n",
    "    print(\"Iteration \",i+1, list(oov.keys())[0])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c142e107-be66-4f60-be7d-e22fd70f333f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([('l', 'o'), ('o', 'w'), ('w', 'e'), ('e', 's'), ('s', 't'), ('t', '</w>')])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = get_stats(oov)\n",
    "pairs = pairs.keys()\n",
    "print(pairs)\n",
    "ind=[merges.index(i) for i in pairs if i in merges]\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e8e02334-77b5-4051-938d-bf50295dfd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['es',\n",
       " ('es', 't'),\n",
       " ('est', '</w>'),\n",
       " ('l', 'o'),\n",
       " ('lo', 'w'),\n",
       " ('low', '</w>'),\n",
       " ('n', 'e'),\n",
       " ('ne', 'w'),\n",
       " ('new', 'est</w>'),\n",
       " ('w', 'i'),\n",
       " ('wi', 'd')]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8f6da-50fe-47be-af14-2a4ebcc2d6e6",
   "metadata": {},
   "source": [
    "Modelos como GPT-2, RoBERTa, XLM y FlauBERT usan BPE para su tokenización.\n",
    "\n",
    "En las siguientes lecciones veremos estos tipos de modelos a profundidad (Decoder-Only)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d823704b-ad48-4b9d-a479-afd214fcfd67",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\">WordPiece</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4f728f-c0ab-4eb8-a43f-902277996f9a",
   "metadata": {},
   "source": [
    "Modelos como BERT, DistilBERT, Electra usan este tipo de tokenización.\n",
    "\n",
    "Actualmente, hay dos implementaciones del algoritmo de WordPiece: de abajo hacia arriba y de arriba hacia abajo. \n",
    "\n",
    "El enfoque ascendente original se basa en BPE. \n",
    "\n",
    "BERT utiliza la implementación de arriba hacia abajo de WordPiece.\n",
    "\n",
    "Sus comienzos datan de [Japanese and Korean Voice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf), y ganó popularidad gracias a BERT.\n",
    "\n",
    "Recordemos, BPE toma un par de tokens (bytes), observa la frecuencia de cada par y fusiona el par que tiene la frecuencia combinada más alta. \n",
    "\n",
    "El proceso es codicioso (greedy), ya que busca la frecuencia combinada más alta en cada paso.\n",
    "\n",
    "Entonces, ¿cuál es el problema con BPE? 🤔 \n",
    "\n",
    "Puede tener instancias en las que hay más de una forma de codificar una palabra en particular. \n",
    "\n",
    "Entonces se vuelve difícil para el algoritmo elegir tokens de subpalabras ya que no hay forma de priorizar cuál usar primero. \n",
    "\n",
    "Por lo tanto, la misma entrada puede representarse mediante diferentes codificaciones que afectan la precisión de las representaciones aprendidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b1cf94-0886-4921-b4b6-cf50146705c6",
   "metadata": {},
   "source": [
    "Miremos el siguiente vocabulario para un corpus pequeño:\n",
    "\n",
    "|Número|Token|Frecuencia|\n",
    "|-|-|-|\n",
    "|1|li|3|\n",
    "|2|l|5|\n",
    "|3|ea|2|\n",
    "|4|eb|4|\n",
    "|5|near|6|\n",
    "|6|bra|2|\n",
    "|7|al|4|\n",
    "|8|n|5|\n",
    "|9|r|1|\n",
    "|10|ge|11|\n",
    "|11|g|7|\n",
    "|12|e|8|\n",
    "\n",
    "SI queremos tokenizar la frase **linear algebra**, podemos hacer lo siguiente:\n",
    "\n",
    "linear = **li**+**near** ó **li** + **n** + **ea** + **r**\n",
    "\n",
    "algebra = **al** + **ge** + **bra** ó **al** + **g** + **e** + **bra**\n",
    "\n",
    "Como vemos, tenemos cuatro formas de poder codificar la frase, lo cual representa un problema para los modelos.\n",
    "\n",
    "En la ciencia de datos, cuando avanzamos un paso por delante de la frecuencia o el conteo, buscamos enfoques probabilísticos. \n",
    "\n",
    "En WordPiece, en realidad hacemos lo mismo. La única diferencia entre WordPiece y BPE es la forma en que se agregan los pares de símbolos al vocabulario. \n",
    "\n",
    "En cada paso iterativo, WordPiece elige un par de símbolos que resultará en el mayor aumento de probabilidad al fusionarse. \n",
    "\n",
    "Maximizar la probabilidad de los datos de entrenamiento es equivalente a encontrar el par de símbolos cuya probabilidad dividida por la probabilidad del primero seguido por la probabilidad del segundo símbolo en el par es mayor que cualquier otro par de símbolos.\n",
    "\n",
    "Por ejemplo, el algoritmo verificará si la probabilidad de ocurrencia de “es” es mayor que la probabilidad de ocurrencia de “e” seguida de “s”. \n",
    "\n",
    "La fusión ocurrirá solo si la probabilidad de \"es\" dividida por \"e\", \"s\" es mayor que cualquier otro par de símbolos.\n",
    "\n",
    "Así, podemos decir que WordPiece evalúa lo que perderá fusionando los dos símbolos para asegurarse de que el paso que está dando realmente vale la pena o no.\n",
    "\n",
    "Podemos resumir el algoritmo en los siguientes pasos, según el paper:\n",
    "\n",
    "1. Inicialice el inventario de unidades de palabras con los caracteres base.\n",
    "1. Construya un modelo de lenguaje sobre los datos de entrenamiento usando la palabra inventario de 1.\n",
    "1. Genere una nueva unidad de palabras combinando dos unidades del inventario de palabras actual. El inventario de unidades de palabras se incrementará en 1 después de agregar esta nueva unidad de palabras. La nueva unidad de palabra se elige entre todas las posibles para que aumente al máximo la probabilidad de que los datos de entrenamiento se agreguen al modelo.\n",
    "1. Vaya a 2 hasta que se alcance un límite predefinido de unidades de palabras o el aumento de probabilidad caiga por debajo de un cierto umbral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d22e030-97a0-42b2-bcf2-672201ea56cf",
   "metadata": {},
   "source": [
    "Debe estar pensando que el entrenamiento debe ser un procedimiento computacionalmente costoso si se realiza utilizando la fuerza bruta. \n",
    "\n",
    "Si es así, entonces tienes razón.\n",
    "\n",
    "La complejidad del tiempo es O(K²) donde K es el número de unidades de palabras actuales. \n",
    "\n",
    "En cuanto a cada iteración, necesitamos probar todas las combinaciones de pares posibles y entrenar un nuevo modelo de lenguaje cada vez. \n",
    "\n",
    "Sin embargo, el algoritmo de entrenamiento puede acelerarse significativamente siguiendo algunos trucos simples, como se explica en el artículo.\n",
    "\n",
    "Podemos probar solo los pares que realmente existen en los datos de entrenamiento, probar solo los pares con una probabilidad significativa de ser los mejores (los que tienen altas prioridades), combinar varios pasos de agrupación en una sola iteración (posible para un grupo de pares que no se afectan entre sí). \n",
    "\n",
    "Según el documento, estas aceleraciones codiciosas ayudaron a construir un vocabulario de 200k para los conjuntos de datos de japonés y coreano en solo unas pocas horas en una sola máquina.\n",
    "\n",
    "La implementación de este algoritmo se parece mucho entonces a la de BPE con unos pequeños cambios de optimización.\n",
    "\n",
    "Así pues, Tensorflow y HuggingFace han creade excelentes API para no hacer el entrenamiento desde cero.\n",
    "\n",
    "Veremos más adelante un ejemplo en concreto.\n",
    "\n",
    "Si tiene curiosidad inmediata, puede mirar en [Subword tokenizers](https://www.tensorflow.org/text/guide/subwords_tokenizer) de Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a084df43-a07c-4cda-846e-93167959f9ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\">Unigram</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c602cbc2-dfae-4014-8870-cc7d11285cc7",
   "metadata": {},
   "source": [
    "El algoritmo Unigram se usa a menudo en SentencePiece, que es el algoritmo de tokenización que usan modelos como AlBERT, T5, mBART, Big Bird y XLNet.\n",
    "\n",
    "En comparación con BPE y WordPiece, Unigram funciona en la otra dirección: comienza con un gran vocabulario y le quita tokens hasta que alcanza el tamaño de vocabulario deseado. \n",
    "\n",
    "Hay varias opciones para usar para construir ese vocabulario base: podemos tomar las subcadenas más comunes en palabras previamente tokenizadas, por ejemplo, o aplicar BPE en el corpus inicial con un tamaño de vocabulario grande.\n",
    "\n",
    "En cada paso del entrenamiento, el algoritmo Unigram calcula una pérdida sobre el corpus dado el vocabulario actual. Luego, para cada símbolo en el vocabulario, el algoritmo calcula cuánto aumentaría la pérdida total si se eliminara el símbolo, y busca los símbolos que la aumentarían menos. \n",
    "\n",
    "Esos símbolos tienen un efecto menor en la pérdida general del corpus, por lo que, en cierto sentido, son \"menos necesarios\" y son los mejores candidatos para su eliminación.\n",
    "\n",
    "Toda esta es una operación muy costosa, por lo que no solo eliminamos el símbolo único asociado con el aumento de pérdida más bajo, sino el porcentaje p (siendo p un hiperparámetro que puede controlar, generalmente 10 o 20) de los símbolos asociados con el aumento más bajo. aumento de pérdidas.\n",
    "\n",
    "Este proceso luego se repite hasta que el vocabulario ha alcanzado el tamaño deseado.\n",
    "\n",
    "Tenga en cuenta que nunca eliminamos los caracteres base, para asegurarnos de que cualquier palabra pueda tokenizarse.\n",
    "\n",
    "Ahora, esto todavía es un poco vago: la parte principal del algoritmo es calcular una pérdida sobre el corpus y ver cómo cambia cuando eliminamos algunos tokens del vocabulario, pero aún no hemos explicado cómo hacerlo. \n",
    "\n",
    "Este paso se basa en el algoritmo de tokenización de un modelo de Unigram, por lo que nos sumergiremos en esto a continuación.\n",
    "\n",
    "Para mostrar un ejemplo, vayamos a [Tokenization algorithm](https://huggingface.co/course/chapter6/7?fw=pt#tokenization-algorithm), de HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7bfc0b-4b1a-48b2-afd4-7c6b022de40c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\">SentencePiece</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680eb071-622b-43e8-a4a0-56f287f9d551",
   "metadata": {},
   "source": [
    "Sorprendentemente, en realidad no es un tokenizador.\n",
    "\n",
    "En realidad, es un método para seleccionar tokens de una lista precompilada, optimizando el proceso de tokenización en función de un corpus proporcionado. SentencePiece, es el nombre de un paquete (disponible en [google/sentencepiece](https://github.com/google/sentencepiece/)) que implementa el algoritmo de regularización de subpalabras (todo por el mismo autor, Kudo, Taku). \n",
    "\n",
    "**Puntos fuertes de SentencePiece:**\n",
    "\n",
    "1. Está implementado en C++ y es increíblemente rápido. Puede entrenar un tokenizador en un corpus de 10⁵ caracteres en segundos.\n",
    "1. También es increíblemente rápido para tokenizar. Esto significa que puede usarlo directamente en datos de texto sin procesar, sin la necesidad de almacenar sus datos tokenizados en el disco.\n",
    "1. La regularización de subpalabras es como una versión de texto del aumento de datos y puede mejorar en gran medida la calidad de su modelo.\n",
    "1. Es independiente del espacio en blanco. Puede entrenar idiomas que no estén delimitados por espacios en blanco, como el chino y el japonés, con la misma facilidad que lo haría con el inglés o el francés.\n",
    "1. Puede funcionar a nivel de byte, por lo que **casi** nunca necesitará usar tokens [UNK] o [OOV]. Esto no es específico solo de SentencePiece.\n",
    "\n",
    "Está fundamentado en [Byte Pair Encoding is Suboptimal for Language Model Pretraining](https://arxiv.org/pdf/2004.03720.pdf).\n",
    "\n",
    "En pocas palabras, SentencePiece usa una estrategia entre BPE y modelos unigram para aprovechar las múltiples formas que tienen las parejas de unirse como representación de una secuencia de símbolos.\n",
    "\n",
    "A partir de estos métodos se optimiza una función que maximiza la probabilidad del conjunto de entrenamiento.\n",
    "\n",
    "Para mayores detalles, podemos ir a [SentencePiece Tokenizer Demystified](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15), por Jonathan Kernes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cde0145-e30d-4cea-917d-8dd550912ae1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Datasets</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b914af0-46a5-4fad-bf8f-a857d98c05a8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\">Desde el Hub de HF</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9c08c8-bf69-43ca-9c9f-661abf3bc630",
   "metadata": {},
   "source": [
    "Haremos un ejercicio de tokenización.\n",
    "\n",
    "Usemos el dataset Oscar para nuestros fines y la versión en latín, pues los otros idiomas tienen muchísima información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f43b99-0ccd-4b6d-97a7-dfcceab61f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efc6f3c6-21b3-4b1e-9c82-e85f66386d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset oscar/unshuffled_deduplicated_la (download: 3.26 MiB, generated: 8.46 MiB, post-processed: Unknown size, total: 11.72 MiB) to /Users/moury/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_la/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50538fa5e85b45c2b42fffc71911f16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3108dafba447f6990a5aa920c46f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b84aa62f8b48ab8c863fb4dc3f3410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/18808 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset oscar downloaded and prepared to /Users/moury/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_la/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51df06ebfc50428ba5b814adc66b9dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('oscar', 'unshuffled_deduplicated_la')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be8cfd7b-d33b-4616-a848-88d01d260aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text'],\n",
       "        num_rows: 18808\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4db5c60-60f8-4704-b521-c164b4c19ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'text': 'Hæ sunt generationes Noë: Noë vir justus atque perfectus fuit in generationibus suis; cum Deo ambulavit.\\nEcce ego adducam aquas diluvii super terram, ut interficiam omnem carnem, in qua spiritus vitæ est subter cælum: universa quæ in terra sunt, consumentur.\\nTolles igitur tecum ex omnibus escis, quæ mandi possunt, et comportabis apud te: et erunt tam tibi, quam illis in cibum.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85beb19f-aaa2-49d2-9b14-6e4b755fb950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82114f1094934cfe9414880828101e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18808 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(dataset['train']):\n",
    "    sample = sample['text'].replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "    if len(text_data) == 1_000:\n",
    "        # once we git the 10K mark, save to file\n",
    "        with open(f'./data/text/oscar_la/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "# after saving in 10K chunks, we will have ~2082 leftover samples, we save those now too\n",
    "with open(f'./data/text/oscar_la/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec209133-3da3-45e5-ba84-540510197ae7",
   "metadata": {},
   "source": [
    "## Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36bc5846-f7f1-40dc-918a-4c0b88f05a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/text/oscar_la/text_10.txt', './data/text/oscar_la/text_11.txt', './data/text/oscar_la/text_13.txt', './data/text/oscar_la/text_12.txt', './data/text/oscar_la/text_16.txt', './data/text/oscar_la/text_17.txt', './data/text/oscar_la/text_15.txt', './data/text/oscar_la/text_14.txt', './data/text/oscar_la/text_2.txt', './data/text/oscar_la/text_3.txt', './data/text/oscar_la/text_1.txt', './data/text/oscar_la/text_0.txt', './data/text/oscar_la/text_4.txt', './data/text/oscar_la/text_5.txt', './data/text/oscar_la/text_7.txt', './data/text/oscar_la/text_6.txt', './data/text/oscar_la/text_8.txt', './data/text/oscar_la/text_9.txt', './data/text/oscar_la/text_18.txt']\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "rutas = glob('./data/*/*/*.txt')\n",
    "print(rutas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c03d78-105d-43d2-ab22-fcc7259989bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "898ef424-50a0-447a-9a9c-a118321a0b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351e3d43-e727-4f2f-a1d4-edc8e9ca8502",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files=rutas,vocab_size=30_522,min_frequency=2,\n",
    "                special_tokens=[\n",
    "                    '<s>','<pad>','</s>','<unk>','<mask>'\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ff385327-2e8d-4591-b6cc-cf719a2d4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('latinberto',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9dd4d222-52ba-4c8d-8bce-ca914c990849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latinberto/vocab.json', 'latinberto/merges.txt']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model('latinberto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "364bf184-6058-4412-aeaf-0df124b6a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46201e2-7dd3-46cd-b7e2-b3b17e9c5366",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('./latinberto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7c984827-7bf0-4087-9f37-9269946d35d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['et', 'Ġtim', 'enti', 'Ġb', 'us']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('et timenti bus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e943655f-f546-43bb-9984-e162dba85d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 304, 1489, 1077, 386, 268, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('et timenti bus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1db7f26d-c477-4ccd-9766-f217f1e19026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'et', 'Ġtim', 'enti', 'Ġb', 'us', '</s>']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([0, 304, 1489, 1077, 386, 268, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "65287048-903a-4bd1-87d8-42742754424b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2022-04-04 19:01:27--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.24.222\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.24.222|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 191984949 (183M) [application/zip]\n",
      "Saving to: ‘wikitext-103-raw-v1.zip’\n",
      "\n",
      "wikitext-103-raw-v1 100%[===================>] 183.09M   175KB/s    in 12m 43s \n",
      "\n",
      "2022-04-04 19:14:12 (246 KB/s) - ‘wikitext-103-raw-v1.zip’ saved [191984949/191984949]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "66369cbc-0d78-4867-b3d1-d3185cccd768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Archive:  wikitext-103-raw-v1.zip\n",
      "   creating: wikitext-103-raw/\n",
      "  inflating: wikitext-103-raw/wiki.test.raw  \n",
      "  inflating: wikitext-103-raw/wiki.valid.raw  \n",
      "  inflating: wikitext-103-raw/wiki.train.raw  \n"
     ]
    }
   ],
   "source": [
    "!unzip wikitext-103-raw-v1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "359807ed-b08c-4b63-95ba-715e6f908028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "db7c7310-95c3-42ec-b18a-d1b81b4d4755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1b203468-79f8-4fad-b94e-2c22fdcc565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "957e3b23-7604-4dd6-8cea-bc4a6bc33c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f4ae9bed-5a00-4f9a-91c9-6e410e36efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"data/tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9dec84-6a5d-49b1-9674-f6e1e01abf46",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\">Desde el computador local</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "490907c0-840c-4361-8661-b11ccdf76e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-27 16:39:39--  https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-train.json.gz [following]\n",
      "--2022-04-27 16:39:40--  https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-train.json.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7725286 (7.4M) [application/octet-stream]\n",
      "Saving to: ‘SQuAD_it-train.json.gz’\n",
      "\n",
      "SQuAD_it-train.json 100%[===================>]   7.37M  2.39MB/s    in 3.1s    \n",
      "\n",
      "2022-04-27 16:39:44 (2.39 MB/s) - ‘SQuAD_it-train.json.gz’ saved [7725286/7725286]\n",
      "\n",
      "--2022-04-27 16:39:45--  https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-test.json.gz [following]\n",
      "--2022-04-27 16:39:45--  https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-test.json.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1051245 (1.0M) [application/octet-stream]\n",
      "Saving to: ‘SQuAD_it-test.json.gz’\n",
      "\n",
      "SQuAD_it-test.json. 100%[===================>]   1.00M  5.91MB/s    in 0.2s    \n",
      "\n",
      "2022-04-27 16:39:46 (5.91 MB/s) - ‘SQuAD_it-test.json.gz’ saved [1051245/1051245]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
    "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7acf2d94-d65e-40ea-ad6b-d20ee2b38c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD_it-test.json.gz:\t   87.4% -- replaced with SQuAD_it-test.json\n",
      "SQuAD_it-train.json.gz:\t   82.2% -- replaced with SQuAD_it-train.json\n"
     ]
    }
   ],
   "source": [
    "!gzip -dkv SQuAD_it-*.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b21b426a-bc2f-45fc-ae56-cfe653986bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c7f58bf9a4c25889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /Users/moury/.cache/huggingface/datasets/json/default-c7f58bf9a4c25889/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd3d32b692a49f7b9023dcdb66eb28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8875119bbaac4005be42b8accea5ce37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/moury/.cache/huggingface/datasets/json/default-c7f58bf9a4c25889/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8dc298c64049ccae2048aff0739042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=\"SQuAD_it-train.json\", field=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b02cdf2a-85fa-4b66-b1dc-2ae2a1e58a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 442\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73a7cec-da67-4845-8627-e4ed1a7f5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_it_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "74d98f37-deb4-4b11-a209-e8ba51a65151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-5d1738e3eece01bd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /Users/moury/.cache/huggingface/datasets/json/default-5d1738e3eece01bd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2173ee6ad7b747879d6525661e79691d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1a773f9bfb435f9db7d4d40e4513ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/moury/.cache/huggingface/datasets/json/default-5d1738e3eece01bd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17a33dbb2404329b545570886e68362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 442\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 48\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\"train\": \"SQuAD_it-train.json\", \"test\": \"SQuAD_it-test.json\"}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd3fd93-1bb0-4538-a242-b254ce08ac3d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\">De forma remota</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8421e-b6e9-4e5d-872a-c87a48b9e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322629da-115b-416a-8e8e-418e91d5064f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\">Desde pandas (Back and Forth)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "522242b3-0fb8-46ff-b064-ef49eaba7217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-27 16:49:38--  https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 42989872 (41M) [application/x-httpd-php]\n",
      "Saving to: ‘drugsCom_raw.zip’\n",
      "\n",
      "drugsCom_raw.zip    100%[===================>]  41.00M  44.5KB/s    in 16m 30s \n",
      "\n",
      "2022-04-27 17:06:08 (42.4 KB/s) - ‘drugsCom_raw.zip’ saved [42989872/42989872]\n",
      "\n",
      "Archive:  drugsCom_raw.zip\n",
      "  inflating: drugsComTest_raw.tsv    \n",
      "  inflating: drugsComTrain_raw.tsv   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8c0944eec81d0f16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/moury/.cache/huggingface/datasets/csv/default-8c0944eec81d0f16/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627f0f08c6a74e399d6d48b3966941dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427f8b54cbc54bdd834467a589328792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/moury/.cache/huggingface/datasets/csv/default-8c0944eec81d0f16/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fae8a1f734c4cf5b326074b4b31425f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>206461</td>\n",
       "      <td>Valsartan</td>\n",
       "      <td>Left Ventricular Dysfunction</td>\n",
       "      <td>\"It has no side effect, I take it in combinati...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>May 20, 2012</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    drugName                     condition  \\\n",
       "0      206461   Valsartan  Left Ventricular Dysfunction   \n",
       "1       95260  Guanfacine                          ADHD   \n",
       "2       92703      Lybrel                 Birth Control   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"It has no side effect, I take it in combinati...     9.0   \n",
       "1  \"My son is halfway through his fourth week of ...     8.0   \n",
       "2  \"I used to take another oral contraceptive, wh...     5.0   \n",
       "\n",
       "                date  usefulCount  \n",
       "0       May 20, 2012           27  \n",
       "1     April 27, 2010          192  \n",
       "2  December 14, 2009           17  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!unzip drugsCom_raw.zip\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"}\n",
    "# \\t is the tab character in Python\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
    "\n",
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# Peek at the first few examples\n",
    "drug_sample[:3]\n",
    "\n",
    "\n",
    "# CONVERTIR A PANDAS\n",
    "drug_dataset.set_format(\"pandas\")\n",
    "\n",
    "drug_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed6810-0667-44f8-95d1-8229a570aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datarame completo\n",
    "train_df = drug_dataset[\"train\"][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491deb6-89fc-48d1-ac07-5bc440d91f2b",
   "metadata": {},
   "source": [
    "Desde aquí, podemos usar todas las operaciones de pandas que queramos y volver al dataset con las modificaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c8ec8-55a6-45c8-b813-03e3fdce59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = (\n",
    "    train_df[\"condition\"]\n",
    "    .value_counts()\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"condition\", \"condition\": \"frequency\"})\n",
    ")\n",
    "frequencies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a81432d-7f76-462e-a6ea-6e72e643eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "freq_dataset = Dataset.from_pandas(frequencies)\n",
    "freq_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3614f057-76ef-4b9f-80a6-a1ec93cf4d28",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\">Guardar en Disco y volver a cargar</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b4822-c1e0-4abd-9f43-c8341c02cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dataset.save_to_disk(\"freq_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b4c55-1e69-42d8-b046-67e40933b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "drug_dataset_reloaded = load_from_disk(\"freq_dataset\")\n",
    "drug_dataset_reloaded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
