{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Generación de texto usando caracteres y redes recurrentes </center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Introducción</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/text_generation_sampling.png\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Muestreo  de palabras</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Tensorflow, Text generation with a RNN](https://www.tensorflow.org/tutorials/text/text_generation)\n",
    "1. [Ralf C. Staudemeyer and Eric Rothstein Morris,Understanding LSTM a tutorial into Long Short-Term Memory Recurrent Neural Networks*, arxiv, September 2019](https://arxiv.org/pdf/1909.09586.pdf)\n",
    "1. [Karpathy, The Unreasonable Effectiveness of Recurrent Neural Networks](  http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "1. [Proyecto Gutemberg](https://www.gutenberg.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Importa módulos requeridos](#Importa-módulos-requeridos)\n",
    "* [Lee los datos](#Descarga-los-datos)\n",
    "* [Crea  el alfabeto](#Crea-el-alfabeto)\n",
    "* [Crea los diccionarios ](#Crea-los-diccionarios )\n",
    "* [Transforma el texto en un arreglo de caracteres](#Transforma-el-texto-en-un-arreglo-de-caracteres)\n",
    "* [Creación de datos de entrenamiento y etiquetas](#Creación-de-datos-de-entrenamiento-y-etiquetas)\n",
    "* [Construcción del Modelo](#Construcción-del-Modelo)\n",
    "* [Prueba del modelo ](#Prueba-del-modelo)\n",
    "* [Entrenamiento](#Entrenamiento)\n",
    "* [Generación de texto](#Generación-de-texto)\n",
    "* [Entrenamiento personalizado](#Entrenamiento-personalizado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este cuaderno es una adaptación del tutorial de tensorflow [Text generation with a RNN](https://www.tensorflow.org/tutorials/text/text_generation).\n",
    "\n",
    "Se muestra cómo generar texto usando un RNR basado en caracteres. Trabajaremos con un conjunto de datos de los algunos documentos de cuentos dsiponibles en la biblioteca libre Gutemberg, con los códigos:  \n",
    "\n",
    "+ 55514-0.txt, \n",
    "+ 61244-0.txt, \n",
    "+ pg36805.txt, \n",
    "+ pg45438.txt, \n",
    "+ pg46000.txt, \n",
    "+ 30053-0.txt,\n",
    "\n",
    "Y los poemas del Daniel Montenegro, uno de los autores\n",
    "\n",
    "+ Poemas_Output.txt\n",
    "+ Poemas_Todo.txt\n",
    "\n",
    "Los datos fueron preparados por Alvaro Montenegro, uno de los autores.\n",
    "\n",
    "\n",
    "Dada una secuencia de caracteres a partir de estos datos  se entrena un modelo para predecir el siguiente carácter en la secuencia. \n",
    "\n",
    "\n",
    "Se pueden generar secuencias de texto más largas llamando al modelo repetidamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Importa módulos requeridos</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de Tensorflow:  2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "\n",
    "print(\"Versión de Tensorflow: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Lee los datos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtiene el path en donde están los datos y los lee en una única lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Datos/55514-0.txt',\n",
       " '../Datos/Poemas_Output.txt',\n",
       " '../Datos/pg46000.txt',\n",
       " '../Datos/Poemas_Todo.txt',\n",
       " '../Datos/pg45438.txt',\n",
       " '../Datos/pg36805.txt',\n",
       " '../Datos/30053-0.txt',\n",
       " '../Datos/61244-0.txt']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../Datos/'\n",
    "files = glob.glob(path+ '*.txt')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for file in files:\n",
    "    t = open(files[6], 'rb').read().decode(encoding='utf-8')\n",
    "    text.extend(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "686344"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)#691080\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Una mirada a los primeros 250  caracters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\r', '\\n', 'E', 'L', ' ', 'P', 'Á', 'J', 'A', 'R', 'O', ' ', 'E', 'N', ' ', 'L', 'A', ' ', 'N', 'I', 'E', 'V', 'E', '\\r', '\\n', '\\r', '\\n', '\\r', '\\n', 'E', 'r', 'a', ' ', 'c', 'i', 'e', 'g', 'o', ' ', 'd', 'e', ' ', 'n', 'a', 'c', 'i', 'm', 'i', 'e', 'n', 't', 'o', '.', ' ', 'L', 'e', ' ', 'h', 'a', 'b', 'í', 'a', 'n', ' ', 'e', 'n', 's', 'e', 'ñ', 'a', 'd', 'o', ' ', 'l', 'o', ' ', 'ú', 'n', 'i', 'c', 'o', ' ', 'q', 'u', 'e', ' ', 'l', 'o', 's', ' ', 'c', 'i', 'e', 'g', 'o', 's', '\\r', '\\n', 's', 'u', 'e', 'l', 'e', 'n', ' ', 'a', 'p', 'r', 'e', 'n', 'd', 'e', 'r', ',', ' ', 'l', 'a', ' ', 'm', 'ú', 's', 'i', 'c', 'a', ';', ' ', 'y', ' ', 'f', 'u', 'e', ' ', 'e', 'n', ' ', 'e', 's', 't', 'e', ' ', 'a', 'r', 't', 'e', ' ', 'm', 'u', 'y', ' ', 'a', 'v', 'e', 'n', 't', 'a', 'j', 'a', 'd', 'o', '.', ' ', 'S', 'u', ' ', 'm', 'a', 'd', 'r', 'e', '\\r', '\\n', 'm', 'u', 'r', 'i', 'ó', ' ', 'p', 'o', 'c', 'o', 's', ' ', 'a', 'ñ', 'o', 's', ' ', 'd', 'e', 's', 'p', 'u', 'é', 's', ' ', 'd', 'e', ' ', 'd', 'a', 'r', 'l', 'e', ' ', 'l', 'a', ' ', 'v', 'i', 'd', 'a', ';', ' ', 's', 'u', ' ', 'p', 'a', 'd', 'r', 'e', ',', ' ', 'm', 'ú', 's', 'i', 'c', 'o', ' ', 'm', 'a', 'y', 'o', 'r', ' ', 'd', 'e', ' ', 'u', 'n', '\\r', '\\n', 'r', 'e', 'g', 'i', 'm', 'i']\n"
     ]
    }
   ],
   "source": [
    "# Echa un vistazo a los primeros 250 caracteres del texto\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\r',\n",
       " '\\n',\n",
       " 'E',\n",
       " 'L',\n",
       " ' ',\n",
       " 'P',\n",
       " 'Á',\n",
       " 'J',\n",
       " 'A',\n",
       " 'R',\n",
       " 'O',\n",
       " ' ',\n",
       " 'E',\n",
       " 'N',\n",
       " ' ',\n",
       " 'L',\n",
       " 'A',\n",
       " ' ',\n",
       " 'N',\n",
       " 'I',\n",
       " 'E',\n",
       " 'V',\n",
       " 'E',\n",
       " '\\r',\n",
       " '\\n',\n",
       " '\\r',\n",
       " '\\n',\n",
       " '\\r',\n",
       " '\\n',\n",
       " 'E',\n",
       " 'r',\n",
       " 'a',\n",
       " ' ',\n",
       " 'c',\n",
       " 'i',\n",
       " 'e',\n",
       " 'g',\n",
       " 'o',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " ' ',\n",
       " 'n',\n",
       " 'a',\n",
       " 'c',\n",
       " 'i',\n",
       " 'm',\n",
       " 'i',\n",
       " 'e',\n",
       " 'n']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Crea  el alfabeto</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este alfabeto es una lista que contiene las letras mayúscula y munúsculas, y algunos caracteres especiales, incluyendo el salto de línea '\\n'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 caracteres únicos\n"
     ]
    }
   ],
   "source": [
    "# Los caracteres únicos en el archivo.\n",
    "vocab = sorted(set(text))\n",
    "print ('{} caracteres únicos'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '\\r',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'Y',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '¡',\n",
       " '´',\n",
       " '¸',\n",
       " 'Á',\n",
       " 'Ñ',\n",
       " 'á',\n",
       " 'é',\n",
       " 'í',\n",
       " 'ñ',\n",
       " 'ó',\n",
       " 'ú']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Crea los diccionarios </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- caracter a índice\n",
    "- índice a caracter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de un mapeo de caracteres únicos a índices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " '\\r': 1,\n",
       " ' ': 2,\n",
       " '!': 3,\n",
       " '\"': 4,\n",
       " '(': 5,\n",
       " ')': 6,\n",
       " '*': 7,\n",
       " ',': 8,\n",
       " '-': 9,\n",
       " '.': 10,\n",
       " ':': 11,\n",
       " ';': 12,\n",
       " '?': 13,\n",
       " 'A': 14,\n",
       " 'B': 15,\n",
       " 'C': 16,\n",
       " 'D': 17,\n",
       " 'E': 18,\n",
       " 'F': 19,\n",
       " 'G': 20,\n",
       " 'H': 21,\n",
       " 'I': 22,\n",
       " 'J': 23,\n",
       " 'L': 24,\n",
       " 'M': 25,\n",
       " 'N': 26,\n",
       " 'O': 27,\n",
       " 'P': 28,\n",
       " 'Q': 29,\n",
       " 'R': 30,\n",
       " 'S': 31,\n",
       " 'T': 32,\n",
       " 'U': 33,\n",
       " 'V': 34,\n",
       " 'Y': 35,\n",
       " 'a': 36,\n",
       " 'b': 37,\n",
       " 'c': 38,\n",
       " 'd': 39,\n",
       " 'e': 40,\n",
       " 'f': 41,\n",
       " 'g': 42,\n",
       " 'h': 43,\n",
       " 'i': 44,\n",
       " 'j': 45,\n",
       " 'k': 46,\n",
       " 'l': 47,\n",
       " 'm': 48,\n",
       " 'n': 49,\n",
       " 'o': 50,\n",
       " 'p': 51,\n",
       " 'q': 52,\n",
       " 'r': 53,\n",
       " 's': 54,\n",
       " 't': 55,\n",
       " 'u': 56,\n",
       " 'v': 57,\n",
       " 'x': 58,\n",
       " 'y': 59,\n",
       " 'z': 60,\n",
       " '¡': 61,\n",
       " '´': 62,\n",
       " '¸': 63,\n",
       " 'Á': 64,\n",
       " 'Ñ': 65,\n",
       " 'á': 66,\n",
       " 'é': 67,\n",
       " 'í': 68,\n",
       " 'ñ': 69,\n",
       " 'ó': 70,\n",
       " 'ú': 71}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n', '\\r', ' ', '!', '\"', '(', ')', '*', ',', '-', '.', '0', '1',\n",
       "       '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B',\n",
       "       'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'O', 'P',\n",
       "       'Q', 'R', 'S', 'T', 'U', 'V', 'Y', '[', ']', 'a', 'b', 'c', 'd',\n",
       "       'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q',\n",
       "       'r', 's', 't', 'u', 'v', 'x', 'y', 'z', '¡', '´', '¸', 'Á', 'Ñ',\n",
       "       'á', 'é', 'í', 'ñ', 'ó', 'ú'], dtype='<U1')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Transforma el texto en un arreglo de caracteres </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  0, 18, ...,  0,  1,  0])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\r':   1,\n",
      "  ' ' :   2,\n",
      "  '!' :   3,\n",
      "  '\"' :   4,\n",
      "  '(' :   5,\n",
      "  ')' :   6,\n",
      "  '*' :   7,\n",
      "  ',' :   8,\n",
      "  '-' :   9,\n",
      "  '.' :  10,\n",
      "  '0' :  11,\n",
      "  '1' :  12,\n",
      "  '2' :  13,\n",
      "  '3' :  14,\n",
      "  '4' :  15,\n",
      "  '5' :  16,\n",
      "  '6' :  17,\n",
      "  '7' :  18,\n",
      "  '8' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demostración del uso de los diccionarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función *repr* convierte el argumento en una expresión imprimible(cuando es posible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\r', '\\n', 'E', 'L', ' ', 'P', 'Á', 'J', 'A', 'R', 'O', ' ', 'E'] ---- caracteres mapeados a int ---- > [ 1  0 18 24  2 28 64 23 14 30 27  2 18]\n"
     ]
    }
   ],
   "source": [
    "# Muestre cómo los primeros 13 caracteres del texto se asignan a números enteros\n",
    "print ('{} ---- caracteres mapeados a int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La tarea de predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dado un caracter, o una secuencia de caracteres, ¿cuál es el próximo caracter más probable? Esta es la tarea en la que vamos a entrenar al modelo. \n",
    "\n",
    "\n",
    "La entrada al modelo será una secuencia de caracteres, y entrenamos al modelo para predecir la salida, el siguiente carácter en cada paso de tiempo.\n",
    "\n",
    "Dado que los RNR mantienen un estado interno que depende de los elementos vistos anteriormente, dados todos los caracteres calculados hasta este momento, ¿cuál es el siguiente carácter?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Creación de datos de entrenamiento y etiquetas  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El texto se divide  en secuencias de entrenamiento. Cada secuencia de entrada contendrá  una longitud *seq_length* de caracteres del texto.\n",
    "\n",
    "Para cada secuencia de entrada, las etiquetas correspondientes contienen la misma longitud de texto, excepto que se desplaza un carácter a la derecha.\n",
    "\n",
    "Así que primero se divide el texto en trozos de *seq_length + 1*. Por ejemplo, digamos que *seq_length* es 3 y nuestro texto es \"Hola\". La secuencia de entrada sería \"Hol\" y la secuencia de destino \"ola\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La oración de longitud máxima que queremos para una sola entrada en caracteres\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego se usa la función *tf.data.Dataset.from_tensor_slices* para convertir el vector de texto en una secuencia de índices de caracteres. Un tensor de enteros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "E\n",
      "L\n",
      " \n",
      "P\n",
      "Á\n",
      "J\n",
      "A\n",
      "R\n"
     ]
    }
   ],
   "source": [
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(10):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int64>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método por lotes nos permite convertir fácilmente estos caracteres individuales en secuencias del tamaño deseado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\r\\nEL PÁJARO EN LA NIEVE\\r\\n\\r\\n\\r\\nEra ciego de nacimiento. Le habían enseñado lo único que los ciegos\\r\\nsue'\n",
      "'len aprender, la música; y fue en este arte muy aventajado. Su madre\\r\\nmurió pocos años después de dar'\n",
      "'le la vida; su padre, músico mayor de un\\r\\nregimiento, hacía un año solamente. Tenía un hermano en Amé'\n",
      "'rica que no\\r\\ndaba cuenta de sí; sin embargo, sabía por referencias que estaba casado,\\r\\nque tenía dos '\n",
      "'niños muy hermosos y ocupaba buena posición. El padre\\r\\nindignado, mientras vivió, de la ingratitud de'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (101,), types: tf.int64>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(['\\r', '\\n', 'E', 'L', ' ', 'P', 'Á', 'J', 'A', 'R', 'O', ' ', 'E',\n",
      "       'N', ' ', 'L', 'A', ' ', 'N', 'I', 'E', 'V', 'E', '\\r', '\\n', '\\r',\n",
      "       '\\n', '\\r', '\\n', 'E', 'r', 'a', ' ', 'c', 'i', 'e', 'g', 'o', ' ',\n",
      "       'd', 'e', ' ', 'n', 'a', 'c', 'i', 'm', 'i', 'e', 'n', 't', 'o',\n",
      "       '.', ' ', 'L', 'e', ' ', 'h', 'a', 'b', 'í', 'a', 'n', ' ', 'e',\n",
      "       'n', 's', 'e', 'ñ', 'a', 'd', 'o', ' ', 'l', 'o', ' ', 'ú', 'n',\n",
      "       'i', 'c', 'o', ' ', 'q', 'u', 'e', ' ', 'l', 'o', 's', ' ', 'c',\n",
      "       'i', 'e', 'g', 'o', 's', '\\r', '\\n', 's', 'u', 'e'], dtype='<U1')\n",
      "array(['l', 'e', 'n', ' ', 'a', 'p', 'r', 'e', 'n', 'd', 'e', 'r', ',',\n",
      "       ' ', 'l', 'a', ' ', 'm', 'ú', 's', 'i', 'c', 'a', ';', ' ', 'y',\n",
      "       ' ', 'f', 'u', 'e', ' ', 'e', 'n', ' ', 'e', 's', 't', 'e', ' ',\n",
      "       'a', 'r', 't', 'e', ' ', 'm', 'u', 'y', ' ', 'a', 'v', 'e', 'n',\n",
      "       't', 'a', 'j', 'a', 'd', 'o', '.', ' ', 'S', 'u', ' ', 'm', 'a',\n",
      "       'd', 'r', 'e', '\\r', '\\n', 'm', 'u', 'r', 'i', 'ó', ' ', 'p', 'o',\n",
      "       'c', 'o', 's', ' ', 'a', 'ñ', 'o', 's', ' ', 'd', 'e', 's', 'p',\n",
      "       'u', 'é', 's', ' ', 'd', 'e', ' ', 'd', 'a', 'r'], dtype='<U1')\n",
      "array(['l', 'e', ' ', 'l', 'a', ' ', 'v', 'i', 'd', 'a', ';', ' ', 's',\n",
      "       'u', ' ', 'p', 'a', 'd', 'r', 'e', ',', ' ', 'm', 'ú', 's', 'i',\n",
      "       'c', 'o', ' ', 'm', 'a', 'y', 'o', 'r', ' ', 'd', 'e', ' ', 'u',\n",
      "       'n', '\\r', '\\n', 'r', 'e', 'g', 'i', 'm', 'i', 'e', 'n', 't', 'o',\n",
      "       ',', ' ', 'h', 'a', 'c', 'í', 'a', ' ', 'u', 'n', ' ', 'a', 'ñ',\n",
      "       'o', ' ', 's', 'o', 'l', 'a', 'm', 'e', 'n', 't', 'e', '.', ' ',\n",
      "       'T', 'e', 'n', 'í', 'a', ' ', 'u', 'n', ' ', 'h', 'e', 'r', 'm',\n",
      "       'a', 'n', 'o', ' ', 'e', 'n', ' ', 'A', 'm', 'é'], dtype='<U1')\n",
      "array(['r', 'i', 'c', 'a', ' ', 'q', 'u', 'e', ' ', 'n', 'o', '\\r', '\\n',\n",
      "       'd', 'a', 'b', 'a', ' ', 'c', 'u', 'e', 'n', 't', 'a', ' ', 'd',\n",
      "       'e', ' ', 's', 'í', ';', ' ', 's', 'i', 'n', ' ', 'e', 'm', 'b',\n",
      "       'a', 'r', 'g', 'o', ',', ' ', 's', 'a', 'b', 'í', 'a', ' ', 'p',\n",
      "       'o', 'r', ' ', 'r', 'e', 'f', 'e', 'r', 'e', 'n', 'c', 'i', 'a',\n",
      "       's', ' ', 'q', 'u', 'e', ' ', 'e', 's', 't', 'a', 'b', 'a', ' ',\n",
      "       'c', 'a', 's', 'a', 'd', 'o', ',', '\\r', '\\n', 'q', 'u', 'e', ' ',\n",
      "       't', 'e', 'n', 'í', 'a', ' ', 'd', 'o', 's', ' '], dtype='<U1')\n",
      "array(['n', 'i', 'ñ', 'o', 's', ' ', 'm', 'u', 'y', ' ', 'h', 'e', 'r',\n",
      "       'm', 'o', 's', 'o', 's', ' ', 'y', ' ', 'o', 'c', 'u', 'p', 'a',\n",
      "       'b', 'a', ' ', 'b', 'u', 'e', 'n', 'a', ' ', 'p', 'o', 's', 'i',\n",
      "       'c', 'i', 'ó', 'n', '.', ' ', 'E', 'l', ' ', 'p', 'a', 'd', 'r',\n",
      "       'e', '\\r', '\\n', 'i', 'n', 'd', 'i', 'g', 'n', 'a', 'd', 'o', ',',\n",
      "       ' ', 'm', 'i', 'e', 'n', 't', 'r', 'a', 's', ' ', 'v', 'i', 'v',\n",
      "       'i', 'ó', ',', ' ', 'd', 'e', ' ', 'l', 'a', ' ', 'i', 'n', 'g',\n",
      "       'r', 'a', 't', 'i', 't', 'u', 'd', ' ', 'd', 'e'], dtype='<U1')\n"
     ]
    }
   ],
   "source": [
    "for item in sequences.take(5):\n",
    "  print(repr(idx2char[item.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para cada secuencia, duplíquela y cámbiela para formar el texto de entrada y de destino utilizando el método  map para aplicar una función simple a cada lote. Es  similar a la función apply de R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function split_input_target at 0x7f9aa5fe89d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function split_input_target at 0x7f9aa5fe89d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
      "array([ 1,  0, 18, 24,  2, 28, 64, 23, 14, 30, 27,  2, 18, 26,  2, 24, 14,\n",
      "        2, 26, 22, 18, 34, 18,  1,  0,  1,  0,  1,  0, 18, 53, 36,  2, 38,\n",
      "       44, 40, 42, 50,  2, 39, 40,  2, 49, 36, 38, 44, 48, 44, 40, 49, 55,\n",
      "       50, 10,  2, 24, 40,  2, 43, 36, 37, 68, 36, 49,  2, 40, 49, 54, 40,\n",
      "       69, 36, 39, 50,  2, 47, 50,  2, 71, 49, 44, 38, 50,  2, 52, 56, 40,\n",
      "        2, 47, 50, 54,  2, 38, 44, 40, 42, 50, 54,  1,  0, 54, 56])>, <tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
      "array([ 0, 18, 24,  2, 28, 64, 23, 14, 30, 27,  2, 18, 26,  2, 24, 14,  2,\n",
      "       26, 22, 18, 34, 18,  1,  0,  1,  0,  1,  0, 18, 53, 36,  2, 38, 44,\n",
      "       40, 42, 50,  2, 39, 40,  2, 49, 36, 38, 44, 48, 44, 40, 49, 55, 50,\n",
      "       10,  2, 24, 40,  2, 43, 36, 37, 68, 36, 49,  2, 40, 49, 54, 40, 69,\n",
      "       36, 39, 50,  2, 47, 50,  2, 71, 49, 44, 38, 50,  2, 52, 56, 40,  2,\n",
      "       47, 50, 54,  2, 38, 44, 40, 42, 50, 54,  1,  0, 54, 56, 40])>)\n",
      "(<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
      "array([47, 40, 49,  2, 36, 51, 53, 40, 49, 39, 40, 53,  8,  2, 47, 36,  2,\n",
      "       48, 71, 54, 44, 38, 36, 12,  2, 59,  2, 41, 56, 40,  2, 40, 49,  2,\n",
      "       40, 54, 55, 40,  2, 36, 53, 55, 40,  2, 48, 56, 59,  2, 36, 57, 40,\n",
      "       49, 55, 36, 45, 36, 39, 50, 10,  2, 31, 56,  2, 48, 36, 39, 53, 40,\n",
      "        1,  0, 48, 56, 53, 44, 70,  2, 51, 50, 38, 50, 54,  2, 36, 69, 50,\n",
      "       54,  2, 39, 40, 54, 51, 56, 67, 54,  2, 39, 40,  2, 39, 36])>, <tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
      "array([40, 49,  2, 36, 51, 53, 40, 49, 39, 40, 53,  8,  2, 47, 36,  2, 48,\n",
      "       71, 54, 44, 38, 36, 12,  2, 59,  2, 41, 56, 40,  2, 40, 49,  2, 40,\n",
      "       54, 55, 40,  2, 36, 53, 55, 40,  2, 48, 56, 59,  2, 36, 57, 40, 49,\n",
      "       55, 36, 45, 36, 39, 50, 10,  2, 31, 56,  2, 48, 36, 39, 53, 40,  1,\n",
      "        0, 48, 56, 53, 44, 70,  2, 51, 50, 38, 50, 54,  2, 36, 69, 50, 54,\n",
      "        2, 39, 40, 54, 51, 56, 67, 54,  2, 39, 40,  2, 39, 36, 53])>)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(2):\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '\\r\\nEL PÁJARO EN LA NIEVE\\r\\n\\r\\n\\r\\nEra ciego de nacimiento. Le habían enseñado lo único que los ciegos\\r\\nsu'\n",
      "Target data: '\\nEL PÁJARO EN LA NIEVE\\r\\n\\r\\n\\r\\nEra ciego de nacimiento. Le habían enseñado lo único que los ciegos\\r\\nsue'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 1 ('\\r')\n",
      "  expected output: 0 ('\\n')\n",
      "Step    1\n",
      "  input: 0 ('\\n')\n",
      "  expected output: 18 ('E')\n",
      "Step    2\n",
      "  input: 18 ('E')\n",
      "  expected output: 24 ('L')\n",
      "Step    3\n",
      "  input: 24 ('L')\n",
      "  expected output: 2 (' ')\n",
      "Step    4\n",
      "  input: 2 (' ')\n",
      "  expected output: 28 ('P')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de lotes de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos `tf.data` para dividir el texto en secuencias manejables. Pero antes de introducir estos datos en el modelo, necesitamos mezclar los datos y empaquetarlos en lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Construcción del Modelo  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use `tf.keras.Sequential` para definir el modelo. Para este sencillo ejemplo, se utilizan tres capas para definir nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/text_generation_training.png\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Modelo de generación de texto</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Prueba del modelo  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ahora ejecute el modelo para ver que se comporta como se esperaba.\n",
    "\n",
    "Primero verifique la forma de la salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 72) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo anterior, la longitud de secuencia de la entrada es `100` pero el modelo puede ejecutarse en entradas de cualquier longitud.\n",
    "\n",
    "El dataset de entrenamiento que tiene lotes de tamaño 64 * 100, los cuales tiene orden aleatorio. El primer lote actual y se pasa por el modelo.\n",
    "\n",
    "El modelo tiene de momento los pesos iniciales. El lote de datos pasa por el modelo y devuelve un tensor de tamaño (64, 100, 84).\n",
    "\n",
    "Como el vocabulario tiene 84 caracteres y cada sequencia tiene tamaño 100, el modelo predice el siguiente caracterer para cada caracter en la entrada.\n",
    "\n",
    "La predicción funciona así:\n",
    "\n",
    "Por cada caracter de entrada, se predice el siguiente caracter así: el modelo asigna un valor numérico a cada elemento en el vocabulario. Si se eligiera el máximo el caracter seleccionado sería simplemente el carater con el valor mas grande.\n",
    "\n",
    "Ya por fuera del modelo.\n",
    "\n",
    "El siguiente es el vector de predicciones del siguiente caracter para todos los caracteres de la primera secuencia en el primer bloque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (64, None, 256)           18432     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (64, None, 72)            73800     \n",
      "=================================================================\n",
      "Total params: 4,030,536\n",
      "Trainable params: 4,030,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener predicciones reales del modelo, necesitamos tomar muestras de la distribución de salida, para obtener índices de caracteres reales. Esta distribución está definida por los logits sobre el vocabulario de los caracteres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es mejor muestrear  esta distribución,  que tomar el *argmax* para evitar  que el modelo quede atascado en un bucle. Probemos para el primer ejemplo en el lote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 72), dtype=float32, numpy=\n",
       "array([[ 6.25738129e-03,  5.26489178e-03, -3.67547339e-03, ...,\n",
       "        -6.28688326e-03,  9.09127994e-04, -6.80384971e-03],\n",
       "       [-1.13937622e-02,  4.77731228e-04, -2.16814363e-03, ...,\n",
       "         1.32553745e-02,  1.34873260e-02, -7.78970961e-03],\n",
       "       [-8.15382041e-03,  8.70817155e-03, -1.33496802e-03, ...,\n",
       "         1.41692981e-02,  1.76555272e-02, -2.85920594e-03],\n",
       "       ...,\n",
       "       [-6.19465671e-03,  2.93969573e-03,  5.07240836e-03, ...,\n",
       "         6.39514066e-03, -8.57854262e-04,  1.86331011e-02],\n",
       "       [-2.42418610e-05, -9.15902667e-04,  7.22392369e-03, ...,\n",
       "        -4.23473399e-03, -2.86005484e-03,  1.04329251e-02],\n",
       "       [-2.64084293e-03, -2.22867588e-03,  9.56252776e-03, ...,\n",
       "        -3.35635175e-03,  9.54606012e-03,  3.16632190e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos valores son logits para obtener probabilidades. Entonces se normaliza a la una distribución categórica, es decir se usa softmax. Con esto, se genera una muestra de la variable categórica para seleccionar el caracter.\n",
    "\n",
    "El siguiente es el vector de predicciones del siguiente caracter para todos los caracteres de  la primera secuencia en el primer bloque.\n",
    "\n",
    "Vamos a predecir los caracteres como indicamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 1), dtype=int64, numpy=\n",
       "array([[23],\n",
       "       [58],\n",
       "       [17],\n",
       "       [ 6],\n",
       "       [12],\n",
       "       [17],\n",
       "       [63],\n",
       "       [60],\n",
       "       [21],\n",
       "       [27],\n",
       "       [31],\n",
       "       [70],\n",
       "       [ 5],\n",
       "       [53],\n",
       "       [52],\n",
       "       [26],\n",
       "       [50],\n",
       "       [61],\n",
       "       [39],\n",
       "       [43],\n",
       "       [ 6],\n",
       "       [13],\n",
       "       [16],\n",
       "       [41],\n",
       "       [54],\n",
       "       [ 7],\n",
       "       [64],\n",
       "       [23],\n",
       "       [22],\n",
       "       [70],\n",
       "       [44],\n",
       "       [70],\n",
       "       [17],\n",
       "       [30],\n",
       "       [32],\n",
       "       [35],\n",
       "       [62],\n",
       "       [68],\n",
       "       [52],\n",
       "       [ 1],\n",
       "       [29],\n",
       "       [31],\n",
       "       [71],\n",
       "       [14],\n",
       "       [22],\n",
       "       [32],\n",
       "       [63],\n",
       "       [ 2],\n",
       "       [18],\n",
       "       [32],\n",
       "       [18],\n",
       "       [48],\n",
       "       [ 2],\n",
       "       [52],\n",
       "       [ 5],\n",
       "       [22],\n",
       "       [16],\n",
       "       [55],\n",
       "       [11],\n",
       "       [69],\n",
       "       [60],\n",
       "       [59],\n",
       "       [55],\n",
       "       [11],\n",
       "       [37],\n",
       "       [29],\n",
       "       [55],\n",
       "       [ 5],\n",
       "       [38],\n",
       "       [29],\n",
       "       [60],\n",
       "       [17],\n",
       "       [44],\n",
       "       [15],\n",
       "       [59],\n",
       "       [64],\n",
       "       [54],\n",
       "       [ 3],\n",
       "       [14],\n",
       "       [63],\n",
       "       [25],\n",
       "       [14],\n",
       "       [63],\n",
       "       [ 0],\n",
       "       [63],\n",
       "       [30],\n",
       "       [36],\n",
       "       [20],\n",
       "       [23],\n",
       "       [63],\n",
       "       [43],\n",
       "       [23],\n",
       "       [59],\n",
       "       [49],\n",
       "       [29],\n",
       "       [23],\n",
       "       [32],\n",
       "       [37],\n",
       "       [68],\n",
       "       [51]])>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 58, 17,  6, 12, 17, 63, 60, 21, 27, 31, 70,  5, 53, 52, 26, 50,\n",
       "       61, 39, 43,  6, 13, 16, 41, 54,  7, 64, 23, 22, 70, 44, 70, 17, 30,\n",
       "       32, 35, 62, 68, 52,  1, 29, 31, 71, 14, 22, 32, 63,  2, 18, 32, 18,\n",
       "       48,  2, 52,  5, 22, 16, 55, 11, 69, 60, 59, 55, 11, 37, 29, 55,  5,\n",
       "       38, 29, 60, 17, 44, 15, 59, 64, 54,  3, 14, 63, 25, 14, 63,  0, 63,\n",
       "       30, 36, 20, 23, 63, 43, 23, 59, 49, 29, 23, 32, 37, 68, 51])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto nos da, en cada paso de tiempo, una predicción del siguiente índice de caracteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 58, 17,  6, 12, 17, 63, 60, 21, 27, 31, 70,  5, 53, 52, 26, 50,\n",
       "       61, 39, 43,  6, 13, 16, 41, 54,  7, 64, 23, 22, 70, 44, 70, 17, 30,\n",
       "       32, 35, 62, 68, 52,  1, 29, 31, 71, 14, 22, 32, 63,  2, 18, 32, 18,\n",
       "       48,  2, 52,  5, 22, 16, 55, 11, 69, 60, 59, 55, 11, 37, 29, 55,  5,\n",
       "       38, 29, 60, 17, 44, 15, 59, 64, 54,  3, 14, 63, 25, 14, 63,  0, 63,\n",
       "       30, 36, 20, 23, 63, 43, 23, 59, 49, 29, 23, 32, 37, 68, 51])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta salida significa que el caracter predicho como el siguiente para el primer caracter es el caracter 58 y asi...\n",
    "\n",
    "Los textos lucen así actualmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decodifíquelos para ver el texto predicho por este modelo no entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " ' su seno hacia el firmamento\\r\\nun número infinito de pequeñas columnas de humo, las cuales al\\r\\nextend'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'JxD);D¸zHOSó(rqNo¡dh)?Cfs*ÁJIóióDRTY´íq\\rQSúAIT¸ ETEm q(ICt:ñzyt:bQt(cQzDiByÁs!A¸MA¸\\n¸RaGJ¸hJynQJTbíp'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Entrenamiento </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, el problema puede tratarse como un problema de clasificación estándar. Dado el estado RNN anterior, y la entrada en este paso de tiempo, predice la clase del siguiente carácter.\n",
    "\n",
    "Se define una función de pérdida adecuada para este caso. Esta función recibe los targets (labels) y las predicciones La función calcula la distribución a partir de las predicciones y genera para cada caracter un valor predicho. Esto es lo que se necesita para calcular la entropía cruzada en este caso (promedio). Este es el valor retornado. En el ejemplo, como entra una secuencia y se predicen 100 caracteres hay 100 distribuciones para el siguiente caracter. Una por cada caracter en la entrada. En total hay un lote de 64 secuencias. Luego la función de pérdida se basa en el promedio de  64∗100=6400  entropías cruzadas dispersas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjuntar un optimizador y una función de pérdida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de pérdida estándar `tf.keras.losses.sparse_categorical_crossentropy` funciona en este caso porque se aplica en la última dimensión de las predicciones.\n",
    "\n",
    "Debido a que nuestro modelo devuelve logits, debemos establecer el indicador `from_logits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 72)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.2749677\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Configure el procedimiento de entrenamiento utilizando el método `tf.keras.Model.compile`. Usaremos `tf.keras.optimizers.Adam` con argumentos predeterminados y la función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración de checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilice un `tf.keras.callbacks.ModelCheckpoint` para asegurarse de que los puntos de control se guarden durante el entrenamiento. Se crea un directorio en el cual se guardará checkpoints con el estado del modelos. Los pesos son guardados allí para uso posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio en donde se guardarán los checkpoints\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Nombre de los archivos de los  checkpoint \n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecuta el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En history queda al información de loss, y acurracy para revisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Generación de texto </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaura el último  checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección vamos a tomar un pre-entranamiento. \n",
    "\n",
    "Usaremos los pesos almacenados, pero ahora vamos a cambiar el modelo para recibir lotes de tamaño 1. Es decir vamos a recibir una secuencia para hacer la predicción a partir de esa secuencia.\n",
    "\n",
    "\n",
    "Debido a la forma en que se pasa el estado RNN de un paso a otro, el modelo solo acepta un tamaño de lote fijo una vez construido.\n",
    "\n",
    "Para ejecutar el modelo con un tamaño de lote diferente, necesitamos reconstruir el modelo y restaurar los pesos desde el punto de control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El bucle de predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente bloque de código genera el texto:\n",
    "\n",
    "* Comienza eligiendo una cadena de inicio, inicializando el estado de la  RNR y configurando el número de caracteres a generar.\n",
    "\n",
    "* Obtenga la distribución de predicción del siguiente carácter utilizando la cadena de inicio y el estado de la  RNR.\n",
    "\n",
    "* Luego, use una distribución categórica para calcular el índice del carácter predicho. Use este caracter predicho como nuestra próxima entrada al modelo.\n",
    "\n",
    "* El estado RNN devuelto por el modelo se retroalimenta al modelo para que ahora tenga más contexto, en lugar de una sola palabra. Después de predecir la siguiente palabra, los estados RNN modificados se retroalimentan nuevamente en el modelo, que es cómo aprende a medida que obtiene más contexto de las palabras predichas previamente.\n",
    "\n",
    "La siguiente es la linea que define la capa GRU arriba en la definición del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.layers.GRU(rnn_units,  return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro *stateful* determina si los valores de estado recurrente deben matenerse (True) o no, cuando se pasa al siguiente caracter. En este caso, al mantenerse esos valores, se mantiene la memoria de la secuencia inicial y de los caracteres que van siendo generados. \n",
    "\n",
    "Por eso, solamente se pasa en el primer paso la secuencia de entrada completa. Desde el segundo paso solamente se pasa el nuevo caracter (que es el predicho). Con esta predicción y la memoria del resto de la secuencia anterior mantenida en estado recurrente se hace la siguiente predicción y así sucesivamente tomado la última secuencia pasada por la capa.\n",
    "\n",
    "Para pasar la última secuencia tratada en la capa GRU, se usa el parámetro *return_sequences*. Por defecto es falso, es decir se omite en la salida la última secuencia pasada por la capa GRU. Aquí hemos colocado *return_sequences=True*, para pasar únicamente el último carater predicho.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Al observar el texto generado, verá que el modelo sabe cuándo colocar mayúsculas, hacer párrafos e imita un vocabulario de escritura similar a Shakespeare. Con el pequeño número de épocas de entrenamiento, aún no ha aprendido a formar oraciones coherentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/text_generation_sampling.png\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Muestreo  de palabras</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función implementa el ciclo de predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Paso de evaluación (generación de texto usando el modelo aprendido)\n",
    "\n",
    "    # Número de caracteres a generar\n",
    "    num_generate = 1000\n",
    "\n",
    "    # Convertir nuestra cadena de inicio en números (vectorizar)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Cadena vacía para almacenar nuestros resultados\n",
    "    text_generated = []\n",
    "\n",
    "    # Las bajas temperaturas dan como resultado un texto más predecible.\n",
    "    # Las temperaturas más altas dan como resultado un texto más sorprendente.\n",
    "    # Experimente para encontrar la mejor configuración.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Aquí batch size == 1\n",
    "    model.reset_states() # borra las unidades recurrentes\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # eliminar la dimensión del lote\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # usar una distribución categórica para predecir la palabra devuelta por el modelo\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Pasamos la palabra predicha como siguiente entrada al modelo\n",
    "        # junto con el estado oculto anterior\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. *reset_states* borra solo los estados recurrentes de la red. Vale la pena mencionar que dependiendo de si la opción *stateful = True* se configuró en la red, el comportamiento de esta función podría ser diferente. Si no está configurado, todos los estados se restablecen automáticamente después de cada cálculo por lotes en su red (por ejemplo, después de llamar a *fit, predict y evaluate* también). Si no es así, debe llamar a reset_states cada vez que desee que las llamadas de modelo consecutivas sean independientes.\n",
    "2. *tf.expand_dims* devuelve un tensor con una dimensión adicional insertada en el eje de índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo más fácil que puedes hacer para mejorar los resultados es entrenarlo por más tiempo (prueba `EPOCHS = 30`).\n",
    "\n",
    "También puede experimentar con una cadena de inicio diferente, o intentar agregar otra capa RNN para mejorar la precisión del modelo, o ajustar el parámetro de temperatura para generar predicciones más o menos aleatorias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span style=\"color:blue\">Entrenamiento personalizado </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "El procedimiento de entrenamiento anterior es simple, pero no le da mucho control.\n",
    "\n",
    "Entonces, ahora que ha visto cómo ejecutar el modelo manualmente, descomprimimos el ciclo de entrenamiento e implementémoslo nosotros mismos. Esto proporciona un punto de partida si, por ejemplo, se implementa _ aprendizaje curricular_ para ayudar a estabilizar la salida de bucle abierto del modelo.\n",
    "\n",
    "Usaremos `tf.GradientTape` para rastrear los gradientes. Puede obtener más información sobre este enfoque leyendo el [eager execution guide](https://www.tensorflow.org/guide/eager).\n",
    "\n",
    "El procedimiento funciona de la siguiente manera:\n",
    "\n",
    "* Primero, inicialice el estado RNN. Hacemos esto llamando al método `tf.keras.Model.reset_states`.\n",
    "\n",
    "* Luego, repita el conjunto de datos (lote por lote) y calcule las * predicciones * asociadas con cada una.\n",
    "\n",
    "* Abra un `tf.GradientTape`, y calcule las predicciones y pérdidas en ese contexto.\n",
    "\n",
    "* Calcular los gradientes de la pérdida con respecto a las variables del modelo utilizando el método `tf.GradientTape.grads`.\n",
    "\n",
    "* Finalmente, dé un paso hacia abajo utilizando el método `tf.train.Optimizer.apply_gradients` del optimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inp)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.keras.losses.sparse_categorical_crossentropy(\n",
    "                target, predictions, from_logits=True))\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso de entrenamiento\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # inicializando el estado oculto al comienzo de cada época\n",
    "    # inicialmente oculto es Ninguno\n",
    "    hidden = model.reset_states()\n",
    "\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        loss = train_step(inp, target)\n",
    "\n",
    "    if batch_n % 100 == 0:\n",
    "        template = 'Epoch {} Batch {} Loss {}'\n",
    "        print(template.format(epoch+1, batch_n, loss))\n",
    " \n",
    "    # guardar (punto de control) el modelo cada 5 épocas\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Regresar al inicio](Contenido)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
