{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Visión por Computadora</center></span>\n",
    "## <span style=\"color:red\"><center>GANs Condicionales</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinador\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Campo Elías Pardo Turriago, cepardot@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conferencistas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "- Daniel  Montenegro, Msc, dextronomo@gmail.com \n",
    "- Oleg Jarma, Estadístico, ojarmam@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asesora Medios y Marketing digital</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Maria del Pilar Montenegro, pmontenegro88@gmail.com \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nayibe Yesenia Arias, naariasc@unal.edu.co\n",
    "- Venus Celeste Puertas, vpuertasg@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se habló anteriormente cuando se mostró [StyleGAN](../Cuadernos/redes_generativas_adversarias.ipynb#StyleGAN), y como indirectamente se demostró cuando generamos [Rostros artificialmente](../Cuadernos/DCGAN.ipynb), en el modelo clásico de Redes Generativas Adversarias tenemos un control muy limitado sobre lo que queremos que se genere. Podemos crear un modelo que genere números a mano, pero luego no podemos decirle que queremos que nos genere un 8. \n",
    "\n",
    "Así que, si se busca que la red genere resultados específicos se podría Alimentar datos especializados, lo cual le quita robustez al modelo, o podríamos hacer que la red generadora sea capaz de diferenciar diferentes clases. Logrando así, la capacidad de elegir el tipo de output que queremos obtener."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Con un vector y con una clase.....</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea principal de las GANs condicionales es el agregar una nuevo tipo de información tanto en los datos reales, como en los vectores aleatorios de entrada para la generación: Clases. Aunque esto parezca una respuesta obvia, no se usan de la forma que se espera.\n",
    "\n",
    "A nivel de la base de datos real, la estructura del dataset vuelve a ser más clásica. Hacemos la separación clásica entre los labels de interés, mientras se agrega la clase \"verdadero\" a cada uno. En la generación de bases de datos, además del vector aleatorio que se convertirá en nuestra imagen, le entregamos una de estas clases, dándole una \"condición\" sobre el tipo de dato que debería estar generando para obtener mejores resultados. De forma resumida, tomamos las clases, usualmente tomadas como la variable respuesta, como una nueva variable de observación\n",
    "\n",
    "¿Cómo cambian nuestras redes en este caso?\n",
    "\n",
    "- La red discriminadora gana una buena ventaja, ya que puede discernir la falsedad de una imagen si su clase no coincide. Así que, el discriminador debe ser capaz de:\n",
    "    - Aceptar el par imagen-clase reales\n",
    "    - discriminar el par imagen-clase falsos, con clase correcta\n",
    "    - discriminar el par imagen-clase falsos, con clase incorrecta\n",
    "\n",
    "- La red generadora, en este nuevo caso, ya no es suficiente el hacer la imagen más realista posible. ahora tiene que ser consciente del tipo de imagen que le estan pidiendo. \n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/cgan.png\" width=\"500\" height=\"450\" align=\"center\"/> \n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Como podemos apreciar, este cambio es uno bastante sencillo de aplicar, que mejora la convergencia de la red, y le da más opciones al usuario. Razón por la cual se ha tomado como un estándar o una recomendación a la hora de desarrollar GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Generación de Piedra, Papel o Tijeras</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomaremos el dataset RPM, el cual ya hemos trabajado en [Transfer Learning](../Cuadernos/taller_transfer_learning.ipynb), usaremos la arquitectura profunda de [DCGANs](../Cuadernos/DCGAN.ipynb), y la acondicionaremos a un mejor resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip \\\n",
    "    -O /mnt/storage/Datasets/rps/rps.zip\n",
    "\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip \\\n",
    "    -O /mnt/storage/Datasets/rps/rps-test-set.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "local_zip = '/mnt/storage/Datasets/rps/rps.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/mnt/storage/Datasets/rps')\n",
    "zip_ref.close()\n",
    "local_zip = '/mnt/storage/Datasets/rps/rps-test-set.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip)\n",
    "zip_ref.extractall('/mnt/storage/Datasets/rps')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('torch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a35811627f2d849a94a681c597b35b034065b47c984a0e4ef87a04e993e3cd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
