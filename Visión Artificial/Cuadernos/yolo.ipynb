{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Visión por Computadora</center></span>\n",
    "## <span style=\"color:red\"><center>You Only Look Once</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinador\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Campo Elías Pardo Turriago, cepardot@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conferencistas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "- Daniel  Montenegro, Msc, dextronomo@gmail.com \n",
    "- Oleg Jarma, Estadístico, ojarmam@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asesora Medios y Marketing digital</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Maria del Pilar Montenegro, pmontenegro88@gmail.com \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nayibe Yesenia Arias, naariasc@unal.edu.co\n",
    "- Venus Celeste Puertas, vpuertasg@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya hemos hablado de la tarea de Detección de objetos, sus diferencias entre modelos de clasificación, sus necesidades y sus falencias, entre otras cosas. El primer acercamiento a estas redes se hizo con la familia de Redes [R-CNN](../Cuadernos/deteccion_objetos_rcnn.ipynb) las cuales funcionan bien, pero en situaciones donde no necesitamos respuestas inmediatas o \"en tiempo real\". el problema es que en la actualidad, la mayoría de problemas importantes o más llamativas, se requiere trabajar con modelos en tiempo real. Es aquí donde entra la familia \"You Only Look Once\", con una mezcla de velocidad y precisión que lo ha puesto como el principal estado del arte "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">fotogramas: ¿Qué son y por qué nos interesan?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el procesamiento de imágenes, no se encuentra mucha diferencia en trabajar con R-CNN y arquitecturas más avanzadas. Esto debido a que, si queremos una sola foto, no es tan importante si el resultado de la detección sale en un segundo, medio segundo o 0.1 segundos. La charla sobre la velocidad de procesamiento cobra mucho más sentido cuando estamos hablando en el reino de los videos.  Y ¿Qué es un video? una serie de imágenes en secuencia. Estas imágenes ahora tomarán el nombre de \"fotograma\". \n",
    "\n",
    "Dependiendo al número de fotogramas que pongamos en un tiempo determinado, nuestro video podrías verse más o menos fluido. Esto es lo que llamamos la \"rata de fotogramas\"(framerate) el cual se define en \"Fotogramas por segundo\" o FPS.\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/fps_comparison.gif\" width=\"500\" height=\"250\" align=\"center\"/> \n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "los FPS, además de ser una herramienta publicitaria, juega un papel importante en el procesamiento de video ya que es lo que nos indica la cantidad de información que va a pasar por el modelo y, al mismo tiempo, nos da una especie de indicador para la velocidad con la que puede trabajar este.\n",
    "\n",
    "en nuestro ejemplo de Faster R-CNN se mencionó que esta red es capaz de hacer detección en una imágen en 0.2 segundos. Si lo pasamos a términos de FPS, estos serían al rededor de 5 fotogramas en un segundo. ¿Cómo se ve eso?\n",
    "\n",
    "\n",
    "[![5-FPS](https://img.youtube.com/vi/yOqNAgBHMzE/0.jpg)](https://www.youtube.com/watch?v=yOqNAgBHMzE)\n",
    "\n",
    "Aunque ciertamente no es extremo, se puede sentir muy lento. Aquí entramos un poco a la idea de la percepción visual de los humanos. Para que un video se considere \"en tiempo real\" debe correr por lo menos a 20 FPS, o mejor dicho, que sea capaz de procesar una sola imagen cada 0.05 segundos. Necesitamos entonces un modelo que sea capaz de procesar imágenes 4 veces más rápido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Darknet y YOLO</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Darknet es un framework completo de Redes neuronales desarrollado por Joseph Redmon, cuyo desarrollo empezó en 2013. Este tiene la capacidad de entrenar y hacer deployment de varias aplicaciones de Redes neuronales de una forma sencilla. El trabajo original puede verse [en la página oficial del proyecto](https://pjreddie.com/darknet/)\n",
    "\n",
    "Sin duda el modelo que más llamó la atención del mundo fue la arquitectura conocida como \"You Only Look Once\" (YOLO). siendo este un modelo de detección de objetos que, cuando salió, causó furor por su poder y velocidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Solo se mira una vez.....</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El nombre no es únicamente un nombre gracioso. tiene que ver con la forma en la que se aplicaba la detección de objetos hasta esos días. En R-CNN, por ejemplo, incialmente se proponen unas regiones donde se cree que hay objetos para luego \"cortar la imagen\" , y cada una de estas regiones recibía el tratamiendo de regresión y clasificación(en los casos inciiales, cada región entraba en la red convolucional de manera individual). Este estilo de detección luego recibiría el nombre de \"detección a dos etapas\", cosa que YOLO no hace. Estos mismos procesos suceden en un solo paso o momento.\n",
    "\n",
    "antes de continuar con la explicación es necesario dejar unos cuantos terminos explicados\n",
    "\n",
    "#### <span style=\"color:blue\">Intersección sobre Unión (IOU)</span>\n",
    "\n",
    "El IOU(por sus siglas en inglés Intersection over union) es el valor con el que observamos qué tan bien las cajas propuestas por el modelo encapsulan el area real que ocupa \n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/iou.png\" width=\"700\" height=\"350\" align=\"center\"/> \n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Este es un valor entre 0 y 1. Entre más cercano esté a 1, decimos que las areas entre el área propuesta y el área real son el mismo.\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/iou_example.png\" width=\"600\" height=\"250\" align=\"center\"/> \n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "#### <span style=\"color:blue\">Cajas Ancla</span>\n",
    "\n",
    "Anteriormente hablamos de las \"Cajas delimitadoras\" y la forma en las que calculábamos su forma: Hacíamos una regresión con respecto a las coordenadas de sus extremos. El problema de esto es que este método no tiene suficiente presición, así que se usa una nueva forma de definir las cajas a utilizar: Cajas Ancla.\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/anchor_box.png\" width=\"300\" height=\"300\" align=\"center\"/> \n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Se definen unas \"formas generales\" de cajas para cada clase, y a la hora de generar estas en la imagen, en lugar de hacer una regresión de cajas, se toma en cuenta la clase encontrada y se le da la coordenada del centro de la caja, su altura y su anchura. Haciendo mucho más rápido este proceso.\n",
    "\n",
    "Dentro de estas cajas ancla es de hecho donde definimos los parámetros que van a entrar a YOLO\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/anchor_box_2.png\" width=\"600\" height=\"600\" align=\"center\"/> \n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">El algoritmo YOLOv1</span>\n",
    "\n",
    "Estudiemos entonces el algoritmo de la primera versión de YOLO\n",
    "\n",
    "- Creamos una malla de tamaño $S\\times S$\n",
    "\n",
    "- Cada celda de la malla detecta una clase, si el centro del objeto está en una celda, se genera un número límite de cajas ancla alrededor de este\n",
    "\n",
    "- Se aplica supresión no máxima sobre las celdas vecinas que tienen la misma clase, para dejar solo una caja\n",
    "\n",
    "- Cada celda hace la predicción sobre la probabilidad del objeto que entiende que está en el.\n",
    "\n",
    "\n",
    "La red que utiliza YOLO es una red con 24 capas convolucionales y 2 capas completamente conectadas. La salida de esta será el número de clases que se van a trabajar. Pero este no es lla histora completa. El proceso de entrenamiento involucra el hacer un pre entrenamiento de clasificación con las redes convolucionales, pero con una única capa conectada con 1000 neuronas de salida. Luego hacen transfer learning con las capas conectadas finales.\n",
    "\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/convnet_yolo.png\" width=\"678\" height=\"300\" align=\"center\"/> \n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "con esta idea, YOLOv1 fue capaz de procesar video a 45 FPS, o procesa una imagen a 0.02 segundos. Esencialmente, YOLOv1 es 10 veces más rápido que Faster R-CNN. Esto con resultados increibles.\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/yolov1_example.png\" width=\"700\" height=\"470\" align=\"center\"/> \n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Claro, como primera versión, esta tuvo ciertos problemas:\n",
    "\n",
    "- La cantidad de objetos a detectar va a depender de la forma de la malla. Y ell tamaño de la malla va a implicar poder computacional\n",
    "- por la naturaleza del algoritmo, se podría detectar el mismo objeto varias veces(casi completamente mítigado por la supresión no máxima)\n",
    "- Ya que cada celda solo es capaz de detectar un objeto, si más de una clase se encuentra en una celda, una de las dos no será detectada.\n",
    "- No trabaja muy bien con objetos pequeños.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Las mejoras de sus siguientes versiones</span>\n",
    "\n",
    "En siguientes papers, se buscó diferentes mejoras al modelo, principalmente en el deployment\n",
    "\n",
    "- YoloV2\n",
    "    - Crearon su propio modelo de clasificación: Darknet 19, que obtuvo un 91% de precisión en ImageNet\n",
    "    - Se cambió el método de entrenamiento\n",
    "    - Agregaron capas de normalización por lotes para mejorar la velocidad\n",
    "- YoloV3\n",
    "    - Se usa el modelo DarkNet 53, que usa conexiones residuales\n",
    "    - Se agrega un paso de regresión logística sobre las predicciónes de cada celda.\n",
    "    - Se reemplazó la función de activación softmax por funciones de clasificación logísticas independientes, permitiendo clasificación multi clase.\n",
    "    - Se agregaron salidas \"Preliminares\" de detecciones tempranas.\n",
    "- YoloV4\n",
    "    - Se usa el modelo  CSPDarknet 53\n",
    "- YoloV5\n",
    "    - ?????????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
